{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e651c99a-fc6c-4b3e-927e-5b7dae9813fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial for setting up a small RAG system using Faiss \n",
    "# and evaluating it using the Gemini Flash 1.5 LLM and the DeepEval library\n",
    "# Google Gemini: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# DeepEval: https://docs.confident-ai.com/docs/guides-rag-evaluation\n",
    "\n",
    "# DeepEval v1.1.6 was fairly compatible with Google Gemini by creating a new LLM class that inherited from DeepEvalBaseLLM\n",
    "# and adding methods that called Gemini's generation functions; it was a similar setup for the Embeddings, inheriting from DeepEvalBaseEmbeddingModel\n",
    "# The only trick is that the LLM output needs to be in JSON format\n",
    "# I used the pydantic and instructor libraries for this; the following gives good examples of how to use them\n",
    "# Tutorial on using custom LLMs with DeepEval: https://docs.confident-ai.com/docs/guides-using-custom-llms\n",
    "\n",
    "# Metrics available in DeepEval:\n",
    "# Contextual Precision: Evaluates whether the reranker in your retriever ranks more relevant nodes in your retrieval context higher than irrelevant ones.\n",
    "# Contextual Recall: Evaluates whether the embedding model in your retriever is able to accurately capture and retrieve relevant information based on the context of the input.\n",
    "# Contextual Relevance: Evaluates whether the text chunk size and top-K of your retriever is able to retrieve information without much irrelevancies.\n",
    "# Answer Relevancy: Evaluates whether the prompt template in your generator is able to instruct your LLM to output relevant and helpful outputs based on the retrieval_context.\n",
    "# Faithfulness: Evaluates whether the LLM used in your generator can output information that does not hallucinate AND contradict any factual information presented in the retrieval_context.\n",
    "# Other metrics are available for non-RAG systems; custom metrics can also be created (I did not test this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dafebe7-76cf-41d8-8734-0f06d15b9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdda733-b425-42f1-831a-6d71952ac5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pydantic import BaseModel, ConfigDict # for JSON output from DeepEval\n",
    "import instructor # for JSON output from DeepEval\n",
    "\n",
    "# Replace these two Google Gemini imports with imports for your LLM\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "import deepeval\n",
    "from deepeval.models import DeepEvalBaseLLM, DeepEvalBaseEmbeddingModel\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval import evaluate\n",
    "from deepeval.evaluate import TestResult, print_test_result\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    FaithfulnessMetric\n",
    ")\n",
    "from deepeval.metrics.ragas import (\n",
    "    RagasMetric,\n",
    "    RAGASAnswerRelevancyMetric,\n",
    "    RAGASFaithfulnessMetric, \n",
    "    RAGASContextualRecallMetric,\n",
    "    RAGASContextualPrecisionMetric,\n",
    "    RAGASContextualRelevancyMetric\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8297c3a4-1366-4e38-9a4c-10f22905e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental variable to opt out of DeepEval tracking telemetry data\n",
    "os.environ[\"DEEPEVAL_TELEMETRY_OPT_OUT\"] = \"YES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03796ba-74cc-4bad-99a2-a594b3898399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepeval.telemetry_opt_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90ecc13-1002-4dad-82e5-cf42b656718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc3144c-2ac2-4448-a86a-a514ee26b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up local API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3cd9fd-5949-4b1e-9894-c26618049ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish RAG pipeline with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eab73af-ed30-4840-be7b-20eb1c66807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Faiss vector store for RAG\n",
    "# # If you already have an index created, skip a few coding cells to the LLM / embeddings setup\n",
    "\n",
    "# # Example of creating a small vector store\n",
    "# # Using 4 State of the Union speeches, all text from whitehouse.gov briefing room speeches posted online, edited to include a title with the date of the speech\n",
    "# # Example from 2024:\n",
    "# # https://www.whitehouse.gov/briefing-room/speeches-remarks/2024/03/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery-2/\n",
    "\n",
    "# # load and parse files\n",
    "# sotu = []\n",
    "# newfiles = [\"./Speeches/titleedits/state_of_the_union_042921.txt\", \"./Speeches/titleedits/state_of_the_union_030122.txt\", \"./Speeches/titleedits/state_of_the_union_020723.txt\", \"./Speeches/titleedits/state_of_the_union_030724.txt\"]\n",
    "# for i in newfiles:\n",
    "#     with open(i) as file:\n",
    "#         for line in file:\n",
    "#             nl = line.rstrip()\n",
    "#             if nl != '':\n",
    "#                 sotu.append(nl)\n",
    "\n",
    "# # convert into Document format\n",
    "# documents = [Document(text=line) for line in sotu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab816039-1c34-4073-84a6-e57ad84832e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='833ea164-b547-46ec-8854-cefdc83fbb10', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='May God protect our troops.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Example of a loaded Document line\n",
    "# documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41cce1c9-8df4-492b-9d15-18dd67cf3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# # Set up the faiss index\n",
    "# d = 768 # dimensions of the input vector of the embedding model that we're going to use; in this case, the google embedding model\n",
    "# faiss_index = faiss.IndexFlatL2(d)\n",
    "# print(faiss_index.is_trained) # double check that the training worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48c5f84-8fc4-4970-9505-3c67705b151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the llm, embeddings, and Settings for Faiss \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\") # Can substitute any LangChain Chat Model\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # Can substitute any LangChain embedding model \n",
    "Settings.embed_model = doc_embeddings # used for LlamaIndex FaissVectorStore\n",
    "Settings.llm = llm # used for LlamaIndex FaissVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82ce1780-061d-4d6d-bf3c-6853db1eb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment for when you need to re-embed and vectorize documents\n",
    "\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, show_progress=True\n",
    "# )\n",
    "\n",
    "# # Save index to disk\n",
    "# index.storage_context.persist()\n",
    "\n",
    "# # Save/remember index id for loading next time\n",
    "# index.index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d046f3-293f-4bac-be0c-bedc5d88012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have a saved index, load that index for RAG answer generation:\n",
    "\n",
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "# My local index id '3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5' uses the 4 speeches including a title that includes the date it was given\n",
    "index = load_index_from_storage(storage_context=storage_context, index_id='3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1ba96f-37e3-4160-bf13-5536c2cbaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional- if you'd like to query your index\n",
    "# # Set up query and chat engines with the index\n",
    "# query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "# chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48698c67-efba-4fe3-a2e1-864a3f43131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The President has highlighted several key achievements in improving the economy during his time in office. These include:\n",
      "\n",
      "* **Job creation:** He boasts of creating over 1.3 million jobs in his first 100 days, surpassing any previous president.  He also mentions creating a record 12 million new jobs in his first two years, exceeding the total job creation of any president in a four-year term.\n",
      "* **Economic growth:** He points to a 5.7% economic growth rate last year, the strongest in nearly 40 years.  He also highlights the International Monetary Fund's prediction of over 6% growth this year, which would be the fastest pace in decades.\n",
      "* **Shifting economic focus:** He emphasizes a move away from \"trickle-down economics\" and towards growing the economy from \"the bottom and the middle out.\"\n",
      "\n",
      "These achievements are presented as evidence of a significant turnaround from the economic crisis he inherited. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Example query and response\n",
    "# query = \"In detail, what has the President done to improve the economy over the four years of his speeches?\"\n",
    "# response = query_engine.query(query) \n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff5c6ba9-c23c-4adb-ad14-f612ce6ed45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of setting up a chat engine with our index\n",
    "# chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')\n",
    "# query = \"You are an expert on analyzing Presidential State of the Union speeches. Could you please analyze the speeches and generate 2 questions and answers from each speech, providing the document filename of each speech that relates to each question?\"\n",
    "# response = chat_engine.chat(query) \n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce50c0-fb99-421f-a198-e397f5411d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05980859-d7e3-44e8-bc4f-10a41fdd111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for ARES RAG evaluation library to work with Gemini and our local RAG setup\n",
    "\n",
    "# ARES has options to run a traditional metrics evaluation (their UES/IDP function for context relevance, answer relevance, and answer faithfulness)\n",
    "# or a Prediction Powered Inference evaluation to generate a confidence interval for a given metric\n",
    "# ARES can also synthetically generate data (Queries/Answers/Context) from specified documents \n",
    "\n",
    "# Code for DeepEval RAG evaluation library to work with Gemini and our local RAG setup\n",
    "# In DeepEval v1.1.6, an example Input/Output/Context is a 'LLMTestCase'\n",
    "# You can evaluate LLMTestCases individually or in a large batch with the evaluate function\n",
    "# Metric scores are produced as well as an LLM generated explanation for a given score \n",
    "# DeepEval can also synthetically generate data from documents - see Synthesizer and generate_goldens_from_docs, below. \n",
    "# Of the 3 libraries I tested, this function produced the most human-realistic queries.\n",
    "# DeepEval also offers other functionality besides RAG evaluation, including \"red teaming LLM applications for security vulnerabilities\"\n",
    "\n",
    "# https://docs.confident-ai.com/docs/guides-rag-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "002bf206-84b1-4bae-ad0c-d23dad165360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval requires a json response. In practice, this has led to malformed json returned from the llm, even with as simple of a schema as this, \n",
    "# but this and the LLM class can likely be refined to improve responses\n",
    "class Response(BaseModel):\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad054ba9-9fb0-4e3d-9f48-7e60eb379ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Open-AI usage requires a custom LLM class for using DeepEval\n",
    "# Tutorial with example code on using custom LLMs with DeepEval: https://docs.confident-ai.com/docs/guides-using-custom-llms\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "        model_config  = ConfigDict(protected_namespaces=())\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel: \n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Gemini 1.5 Flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b903c42-fc2d-4fe2-bb1e-faaa7de1f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, a custom embedding model class is required for non Open-AI embeddings\n",
    "# Tutorial on using custom embeddings with DeepEval: https://docs.confident-ai.com/docs/guides-using-custom-embedding-models \n",
    "class CustomGeminiEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self):\n",
    "        model_config  = ConfigDict(protected_namespaces=())\n",
    "\n",
    "    def load_model(self):\n",
    "        return GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\"\n",
    "        )\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_query(text)\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_query(text)\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_documents(texts)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        \"Custom Gemini Embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e11dcd46-4410-4ae9-bb68-3a386d6484c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom llm and embeddings\n",
    "custom_geminiflash = CustomGeminiFlash()\n",
    "custom_geminiembeddings = CustomGeminiEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1da71503-d604-4936-a243-c3d30b6461f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of evaluating one test case\n",
    "contextual_precision = ContextualPrecisionMetric(model=custom_geminiflash)\n",
    "contextual_recall = ContextualRecallMetric(model=custom_geminiflash)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=custom_geminiflash)\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What measures does the speaker propose to lower prescription drug costs in America?\",\n",
    "    actual_output=\"The speaker proposes giving Medicare the power to negotiate lower prescription drug prices, capping prescription drug costs at $2,000 a year for everyone, and allowing Medicare to negotiate lower prices for 500 drugs over the next decade.s\",\n",
    "    expected_output=\"The speaker proposes that Medicare should be given the power to negotiate lower drug prescription prices. They argue that this would save hundreds of billions of dollars and lower prescription drug costs for everyone. The speaker also states that the money saved could be used to strengthen the Affordable Care Act and expand Medicare coverage benefits without costing taxpayers an additional penny.\",\n",
    "    retrieval_context=['Let’s do what we’ve always talked about for all the years I was down here in this — in this body — in Congress.  Let’s give Medicare the power to save hundreds of billions of dollars by negotiating lower drug prescription prices.  (Applause.)', 'In fact, we pay the highest prescription drug prices of anywhere in the world right here in America — nearly three times — for the same drug, nearly three times what other countries pay.  We have to change that, and we can.', 'And we’re finally giving Medicare the power to negotiate drug prices. Bringing down prescription drug costs doesn’t just save seniors money.', 'For years people have talked about it but I finally got it done and gave Medicare the power to negotiate lower prices for prescription drugs just like the VA does for our veterans.', 'And, by the way, that won’t just — that won’t just help people on Medicare; it will lower prescription drug costs for everyone.', 'Now I want to cap prescription drug costs at $2,000 a year for everyone!', 'We know how to do this.  The last President had that as an objective.  We all know how outrageously expensive drugs are in America.', 'Make no mistake, if you try to do anything to raise the cost of prescription drugs, I will veto it.', 'Now it’s time to go further and give Medicare the power to negotiate lower prices for 500 drugs over the next decade.', 'It will cut the federal deficit, saving tax payers hundreds of billions of dollars on the prescription drugs the government buys for Medicare.']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ef1ea0f-8367-4747-84eb-f16f79895d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Precision Score:  0.9095238095238096\n",
      "Contextual Precision Reason:  The score is 0.91 because the first five nodes are relevant and directly address the speaker's proposal to lower drug prices through Medicare negotiation.  However, the sixth node, focusing on capping prescription drug costs,  is a separate proposal and doesn't directly align with the initial proposal, making it a relevant node ranked lower. The seventh and eighth nodes don't mention specific measures, making them less relevant compared to the first five nodes that directly discuss the speaker's proposed measure.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Recall Score:  1.0\n",
      "Contextual Recall Reason:  The score is 1.00 because the speaker proposes that Medicare should be given the power to negotiate lower drug prescription prices, and the node(s) in retrieval context) support this with phrases like 'Let’s give Medicare the power to save hundreds of billions of dollars by negotiating lower drug prescription prices.'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Relevancy Score:  0.5\n",
      "Contextual Relevancy Reason:  The score is 0.50 because the reasons for irrelevancy indicate that the context doesn't specifically outline measures to lower prescription drug costs, but it does discuss the speaker's position and impact on costs. \"The context discusses the high prescription drug prices in America and suggests changing them, but it doesn't mention any specific measures the speaker proposes to lower costs.\" and \"The context only states the speaker's position on raising the cost of prescription drugs, not any measures to lower it.\" highlight this, suggesting a partial relevance as the context doesn't ignore the topic entirely.\n"
     ]
    }
   ],
   "source": [
    "# For RAG systems, DeepEval recommends the following Retrieval and Generation metrics:\n",
    "# Retrieval metrics:\n",
    "contextual_precision.measure(test_case)\n",
    "print(\"Contextual Precision Score: \", contextual_precision.score)\n",
    "print(\"Contextual Precision Reason: \", contextual_precision.reason)\n",
    "\n",
    "contextual_recall.measure(test_case)\n",
    "print(\"Contextual Recall Score: \", contextual_recall.score)\n",
    "print(\"Contextual Recall Reason: \", contextual_recall.reason)\n",
    "\n",
    "contextual_relevancy.measure(test_case)\n",
    "print(\"Contextual Relevancy Score: \", contextual_relevancy.score)\n",
    "print(\"Contextual Relevancy Reason: \", contextual_relevancy.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cd1ac8d-1f6f-47bb-87df-87528474486a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Relevancy Score:  1.0\n",
      "Answer Relevancy Reason:  The score is 1.00 because the input asks for measures to lower prescription drug costs and the provided JSON correctly represents that information. Great job!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score:  1.0\n",
      "Faithfulness Reason:  The score is 1.00 because there are no contradictions, this is great!\n"
     ]
    }
   ],
   "source": [
    "# Generation metrics:\n",
    "answer_relevancy = AnswerRelevancyMetric(model=custom_geminiflash)\n",
    "faithfulness = FaithfulnessMetric(model=custom_geminiflash)\n",
    "                                 \n",
    "answer_relevancy.measure(test_case)\n",
    "print(\"Answer Relevancy Score: \", answer_relevancy.score)\n",
    "print(\"Answer Relevancy Reason: \", answer_relevancy.reason)\n",
    "\n",
    "faithfulness.measure(test_case)\n",
    "print(\"Faithfulness Score: \", faithfulness.score)\n",
    "print(\"Faithfulness Reason: \", faithfulness.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c32b6c3f-c08a-439f-9996-d33d20ca6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of measuring metrics for multiple test cases / a full dataset\n",
    "\n",
    "# Load manually curated dataset\n",
    "evaldataset = EvaluationDataset()\n",
    "evaldataset.add_test_cases_from_csv_file(\n",
    "    file_path=\"datasets/unlabeled_dataset/unlabeled_dataset_deepeval.csv\",\n",
    "    input_col_name=\"Input\",\n",
    "    actual_output_col_name=\"Actual_Output\",\n",
    "    expected_output_col_name=\"Expected_Output\",\n",
    "#    context_col_name=\"context\",\n",
    "#    context_col_delimiter= \",\",\n",
    "    retrieval_context_col_name=\"Retrieval_Context\",\n",
    "    retrieval_context_col_delimiter= \",\"\n",
    "#    additional_metadata_col_name=\"source_file\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7d54857-001b-403c-9838-01082c365a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever metrics:\n",
    "contextual_precision = ContextualPrecisionMetric(model=custom_geminiflash)\n",
    "contextual_recall = ContextualRecallMetric(model=custom_geminiflash)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=custom_geminiflash) # Note: this was the only metric that would not finish execution for the manually curated dataset (too many 429 errors)\n",
    "\n",
    "# Generation metrics:\n",
    "answer_relevancy = AnswerRelevancyMetric(model=custom_geminiflash)\n",
    "faithfulness = FaithfulnessMetric(model=custom_geminiflash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84960d-d52d-4178-8ef0-088e818b71db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 Options for Metrics Evaluation: \n",
    "\n",
    "# 1) Iterating through test cases seems to work better than bulk evaluation with evaluate,\n",
    "# as errors encountered with evaluate(...) cause no results to be returned\n",
    "# Looping at least saves partial results until an error occurs\n",
    "# I encountered this sometimes with contextual_relevancy and contextual_precision on the test dataset (429 errors or Invalid JSON errors),\n",
    "# yet typically I got results if I iterated through individual test_cases\n",
    "# For future: https://github.com/confident-ai/deepeval/issues/964 may assist with incorrect json errors like what was being returned\n",
    "\n",
    "# Example for evaluating one test case at a time\n",
    "contextprecision_results = []\n",
    "for i in range(0, len(evaldataset.test_cases)):\n",
    "    eval_contextprecision = evaluate(test_cases=[evaldataset.test_cases[i]], metrics=[contextual_precision], throttle_value=10) #throttle_value is for rate limiting, in seconds between queries\n",
    "    contextprecision_results.append(eval_contextprecision[0])\n",
    "\n",
    "# 2) Evaluate through test_cases in bulk\n",
    "# In testing, at least faithfulness, contextual_precision metrics worked this way with a small, manually curated dataset\n",
    "\n",
    "# Throttle_value is for rate limiting- in seconds between queries\n",
    "# test_precision = evaluate(test_cases=evaldataset.test_cases, metrics=[contextual_precision], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd280f4f-5ec1-405b-9e3f-062f1908a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick print for results after evaluation of single test cases, as needed\n",
    "# Get scores to calculate average score\n",
    "scores = []\n",
    "for i in [contextprecision_results]:\n",
    "    for j in i:\n",
    "        if type(j) == TestResult:\n",
    "            scores.append(j.metrics_data[0].score)\n",
    "            print_test_result(j)\n",
    "        else:\n",
    "            print_test_result(j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "615b35bb-0064-4fac-acac-eb8d7c5134e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.833162\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average for the metric\n",
    "scoredata = pd.DataFrame(scores, index=None)\n",
    "scoredata.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a6e436d-fb76-4609-b4fc-ed7b6c992b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scores\n",
    "scoredata.to_csv(\"results/deepeval_contextprecision_unlabeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f9fe2118-2b17-443a-b0ef-3e3e0290adae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 0.8928571428571428, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 0.89 because the first three nodes in the retrieval context strongly support the input by mentioning key details like the Selma march, Edmund Pettus Bridge, voting rights, and the Voting Rights Act.  However, the fourth node, mentioning John Lewis, is only loosely relevant; the fifth node is irrelevant as it discusses President Roosevelt; and the sixth node about January 6th is entirely unrelated.  While the seventh node again strengthens relevance by mentioning John Lewis, the presence of these three irrelevant nodes, especially the completely unrelated node about January 6th ranked lower than most relevant nodes, lowers the contextual precision score., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What was the significance of the Selma March in Alabama, where hundreds of civil rights activists marched across the Edmund Pettus Bridge in 1965, demanding their right to vote, and what role did the march play in leading to the passage of the Voting Rights Act five months later?\n",
      "  - actual output: The march in Selma, Alabama, was a pivotal moment in the fight for voting rights. It brought national attention to the ongoing struggle for equality and served as a catalyst for the passage of the Voting Rights Act. \n",
      "\n",
      "  - expected output: The Selma to Montgomery marches, including Bloody Sunday on March 7, 1965, were pivotal in the fight for voting rights. The brutal attack on peaceful protesters by state troopers on the Edmund Pettus Bridge shocked the nation and brought the issue of voter suppression to the forefront of public consciousness. This event, along with subsequent marches, galvanized public support for the Voting Rights Act, which was signed into law five months later, guaranteeing the right to vote for all Americans regardless of race.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['A transformational moment in our history happened 59 years ago today in Selma\", \" Alabama.'\", \" 'to help shake the nation’s conscience. Five months later\", \" the Voting Rights Act was signed into law.'\", \" 'Hundreds of foot soldiers for justice marched across the Edmund Pettus Bridge\", ' named after a Grand Dragon of the KKK', \" to claim their fundamental right to vote.'\", \" 'Joining us tonight are other marchers who were there including Betty May Fikes\", \" known as the “Voice of Selma”.'\", \" 'Pass and send me the Freedom to Vote Act and the John Lewis Voting Rights Act!'\", \" 'John Lewis was a great friend to many of us here. But if you truly want to honor him and all the heroes who marched with him\", \" then it’s time for more than just talk.'\", \" 'But they failed. America stood strong and democracy prevailed.'\", \" 'President Roosevelt’s purpose was to wake up the Congress and alert the American people that this was no ordinary moment.'\", \" 'History is watching\", \" just like history watched three years ago on January 6th.'\", \" 'Our late friend and former colleague John Lewis was at the march.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 0.8631519274376417, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 0.86 because the first three nodes in the retrieval context strongly support the input query by directly mentioning the Selma march and its connection to the Voting Rights Act, while subsequent relevant nodes further elaborate on its lasting impact on the ongoing struggle for voting rights.  However, the presence of irrelevant nodes, such as the fourth node which discusses Betty May Fikes without addressing the core question, and the tenth node that contradicts the question's premise about the ongoing struggle, slightly lowers the score.  The sixth node, mentioning recent legislation without explaining the Selma March's contribution, also pulls the score down.  Despite these irrelevant nodes, the strong relevance of the initial 'yes' verdicts, especially the first three, maintains a high contextual precision score. These initial nodes correctly address the timeframe, historical context, and the causal link between the march and the Voting Rights Act, while the later 'no' verdicts discuss unrelated information or contradicting viewpoints. For example, the fourth node, ranked lower than the highly relevant top three, is less relevant than those higher ranked nodes because it focuses on Betty May Fikes and doesn't directly address the input question's focus on the march's contribution to the current voting rights struggle. Similarly, the tenth node is also correctly placed lower than others because it presents a counter-narrative to the input query, whereas nodes one through three directly establish the context and causal link., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: How did the Selma March, marked by violence and the subsequent passage of the Voting Rights Act, contribute to the ongoing struggle for voting rights 59 years later?\n",
      "  - actual output: The Selma March was a pivotal moment in the fight for voting rights, demonstrating the ongoing struggle for equality and the need for legislation to protect it.  Despite the progress made with the Voting Rights Act, the context highlights the continued need for action to ensure fair and accessible voting. \n",
      "\n",
      "  - expected output: The Selma March, a pivotal moment in the fight for voting rights, highlighted the brutal reality of racial discrimination and the need for federal protection. The subsequent passage of the Voting Rights Act in 1965 was a monumental achievement, but the fight for voting rights continues as voter suppression and other tactics threaten access to the ballot box today.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['to help shake the nation’s conscience. Five months later\", \" the Voting Rights Act was signed into law.'\", \" 'A transformational moment in our history happened 59 years ago today in Selma\", \" Alabama.'\", \" 'Joining us tonight are other marchers who were there including Betty May Fikes\", \" known as the “Voice of Selma”.'\", \" 'Hundreds of foot soldiers for justice marched across the Edmund Pettus Bridge\", ' named after a Grand Dragon of the KKK', \" to claim their fundamental right to vote.'\", \" 'Pass and send me the Freedom to Vote Act and the John Lewis Voting Rights Act!'\", \" 'Respect free and fair elections! Restore trust in our institutions! And make clear –political violence'\", \" 'But 59 years later\", \" there are forces taking us back in time.'\", \" 'Not since President Lincoln and the Civil War have freedom and democracy been under assault here at home as they are today.'\", \" 'John Lewis was a great friend to many of us here. But if you truly want to honor him and all the heroes who marched with him\", \" then it’s time for more than just talk.'\", \" 'But they failed. America stood strong and democracy prevailed.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 0.7174482924482926, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 0.72 because the first irrelevant node, ranked second, is grammatically incomplete and provides no context for analysis, as stated in its 'grammatically incomplete and provides no context for rhetorical analysis', while relevant nodes such as the first node, ranked first, directly appeals to national unity with 'My Fellow Americans', and other relevant nodes, like the third and fourth, highlight shared historical experience and governance, further strengthening the sense of unity.  The fifth irrelevant node, ranked fifth, is also incomplete and lacks information. The sixth irrelevant node, ranked sixth, mentions actions that don't inherently build national unity, as stated in its 'these actions don't inherently build national unity'. However, subsequent highly ranked relevant nodes effectively showcase rhetorical devices evoking national unity, such as the use of 'we' to foster shared experience and the description of America's shared heritage. The overall ranking shows a trend of relevant nodes preceding irrelevant ones, though some irrelevant nodes are ranked higher than expected, leading to a score below 1., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Analyze the speaker's use of rhetorical devices to evoke a sense of national unity.\n",
      "  - actual output: The speaker uses several rhetorical devices to evoke a sense of national unity.  They address the audience directly with phrases like \"My Fellow Americans\" and \"We are the only nation in the world...\" This creates a sense of shared experience and purpose.  The repetition of \"unprecedented moment\" emphasizes the gravity of the situation and the need for collective action.  The speaker also appeals to shared values and history, reminding the audience of their nation's unique character and strength.  Finally, by contrasting \"old and new\" ideas, the speaker suggests a path forward that embraces both tradition and progress, further unifying the audience around a shared vision for the future. \n",
      "\n",
      "  - expected output: The speaker uses several rhetorical devices to evoke a sense of national unity. They frequently use the phrase \"all Americans\" to emphasize their desire for inclusion and shared purpose. This is further reinforced by the repeated use of \"we\" and \"our\" to create a sense of collective identity and responsibility. The speaker also employs parallelism, stating \"I see a country for all Americans! ... And I will always be a president for all Americans!\" to create a memorable rhythm and highlight the shared aspirations of the nation. They further evoke a sense of national unity by referencing shared values like \"believing in America\" and \"believing in the American people.\" This appeal to common ground is intended to unite the audience in their shared love for the country. Finally, the speaker uses the phrase \"Let's build that future together!\" to call for collective action and emphasize the importance of working together for a shared goal. The overall message is designed to inspire hope and unity, reminding the audience of their shared identity and purpose as Americans.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['Tonight I come to the same chamber to address the nation.'\", \" 'He said\", \" “I address you at a moment unprecedented in the history of the Union.”'\", \" 'Mr. Speaker. Madam Vice President. Members of Congress. My Fellow Americans.'\", \" 'And yes\", ' my purpose tonight is to both wake up this Congress', \" and alert the American people that this is no ordinary moment either.'\", \" 'Now it is we who face an unprecedented moment in the history of the Union.'\", \" 'We are the only nation in the world with a heart and soul that draws from old and new.'\", \" 'And it’s because of you that tonight we can proudly say the State of our Union is strong and getting stronger!'\", \" 'President Roosevelt’s purpose was to wake up the Congress and alert the American people that this was no ordinary moment.'\", \" 'My fellow Americans the issue facing our nation isn’t how old we are it’s how old our ideas are?'\", \" 'America cannot go back. I am here tonight to show the way forward. Because I know how far we’ve come.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 0.7232993197278912, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 0.72 because the top-ranked nodes in the retrieval context, such as the first node with the quote '\"Mr. Speaker. Madam Vice President. Members of Congress. My Fellow Americans.\" ', effectively use direct address and shared responsibility to foster unity, as explained in its reasons.  Similarly, the second node, with the quote '\"And yes, my purpose tonight is to both wake up this Congress, and alert the American people\"', uses a call to action for unity and shared purpose.  However, lower-ranked nodes, like the third node with the quote '\"Tonight I come to the same chamber to address the nation.\" ', and the fourth node with '\"this is no ordinary moment either\"', only set the stage and do not directly demonstrate rhetorical devices for inspiring patriotism and unity as described in their reasons.  These irrelevant nodes, while present, are ranked lower than the relevant ones, resulting in a score of 0.72.  The presence of these less relevant nodes, especially in the middle ranks, prevents a higher score, although most of the higher ranked nodes are well-aligned with the input. , error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Analyze the speaker's use of rhetorical devices to inspire patriotism and unity.\n",
      "  - actual output: The speaker uses powerful imagery and appeals to shared values to inspire patriotism and unity. They highlight the historical significance of the moment, drawing parallels to past challenges faced by the nation. By invoking the legacy of past leaders like President Roosevelt, the speaker emphasizes the importance of collective action and resilience. The use of phrases like \"the State of our Union is strong\" and \"the only nation in the world with a heart and soul\" evokes a sense of national pride and purpose. Additionally, the speaker emphasizes the need to move forward, highlighting the shared responsibility of all Americans to protect democracy and ensure a brighter future. \n",
      "\n",
      "  - expected output: The speaker employs several rhetorical devices to inspire patriotism and unity. Repetition of phrases like \"for all Americans!\" and \"I believe in America!\" emphasizes inclusivity and shared values.  The direct address to \"the American people\" creates a personal connection, fostering a sense of shared responsibility.  Phrases like \"we are the United States of America\" and \"there is nothing beyond our capacity when we act together!\" evoke a powerful sense of national identity and collective strength.  The call to \"build that future together!\" and the closing blessing invoke a sense of shared destiny and divine protection, further uniting the audience.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['Tonight I come to the same chamber to address the nation.'\", \" 'And yes\", ' my purpose tonight is to both wake up this Congress', \" and alert the American people that this is no ordinary moment either.'\", \" 'Mr. Speaker. Madam Vice President. Members of Congress. My Fellow Americans.'\", \" 'He said\", \" “I address you at a moment unprecedented in the history of the Union.”'\", \" 'My fellow Americans the issue facing our nation isn’t how old we are it’s how old our ideas are?'\", \" 'President Roosevelt’s purpose was to wake up the Congress and alert the American people that this was no ordinary moment.'\", \" 'And it’s because of you that tonight we can proudly say the State of our Union is strong and getting stronger!'\", \" 'We are the only nation in the world with a heart and soul that draws from old and new.'\", \" 'America cannot go back. I am here tonight to show the way forward. Because I know how far we’ve come.'\", \" 'Insurrectionists stormed this very Capitol and placed a dagger at the throat of American democracy.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 1.00 because all relevant nodes in the retrieval context are ranked higher than the irrelevant ones.  The first six nodes perfectly reflect the input query's focus on threats to American democracy, both domestically and internationally, as shown in their descriptions:  'The phrase \"January 6th and the lies about the 2020 election\" directly relates to the domestic threat...', 'The quote \"What makes our moment rare is that freedom and democracy are under attack both at home and overseas at the very same time\" accurately reflects...', 'The statement \"Not since President Lincoln and the Civil War have freedom and democracy been under assault here at home as they are today\" emphasizes...', 'The mention of \"Insurrectionists stormed this very Capitol and placed a dagger at the throat of American democracy\" vividly describes...', 'The phrase \"But we must be honest the threat remains and democracy must be defended\" reflects...', and 'The statement \"Freedom and democracy were under assault in the world\" directly addresses...'.  The last three nodes, however, are irrelevant, focusing on positive statements or the speaker's intent ('The sentence \"And it’s because of you that tonight we can proudly say the State of our Union is strong and getting stronger!\" is a positive statement...', 'The statement \"But they failed. America stood strong and democracy prevailed.\" describes a past success...', and 'The phrase \"my purpose tonight is to both wake up this Congress and alert the American people that this is no ordinary moment either\" is about the intent...'), and are correctly placed after the relevant ones., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What was the key message conveyed in President Biden's State of the Union address in 2024, highlighting the threats to American democracy both domestically and internationally?\n",
      "  - actual output: Empty Response\n",
      "  - expected output: President Biden's 2024 State of the Union address emphasized the multifaceted threats to American democracy, both domestically and internationally. He highlighted Putin's aggression in Ukraine as a threat to global stability and compared it to the January 6th attack on the Capitol, underscoring the ongoing danger to American democracy from within. He called for unity and urged Congress to support Ukraine and defend democratic institutions against both foreign and domestic threats.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['State of the Union Address given by President Biden on March 7\", \" 2024'\", \" 'January 6th and the lies about the 2020 election\", ' and the plots to steal the election', \" posed the gravest threat to our democracy since the Civil War.'\", \" 'What makes our moment rare is that freedom and democracy are under attack\", ' both at home and overseas', \" at the very same time.'\", \" 'Not since President Lincoln and the Civil War have freedom and democracy been under assault here at home as they are today.'\", \" 'Insurrectionists stormed this very Capitol and placed a dagger at the throat of American democracy.'\", \" 'But we must be honest the threat remains and democracy must be defended.'\", \" 'Freedom and democracy were under assault in the world.'\", \" 'And it’s because of you that tonight we can proudly say the State of our Union is strong and getting stronger!'\", \" 'But they failed. America stood strong and democracy prevailed.'\", \" 'And yes\", ' my purpose tonight is to both wake up this Congress', \" and alert the American people that this is no ordinary moment either.']\"]\n"
     ]
    }
   ],
   "source": [
    "# Example output from DeepEval\n",
    "for i in [contextprecision_results]:\n",
    "    for j in i[0:5]:\n",
    "        print_test_result(j)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63c2fc3a-e543-4abe-9a17-5bf68f2933c0",
   "metadata": {},
   "source": [
    "Results/Notes on DeepEval metrics:\n",
    "\n",
    "- Contextual Precision: Average: 0.776\n",
    "  - Calculated fairly similarly (if not the same) as RAGAS's contextual precision; would need to look into code further\n",
    "  - Manual review: Decent overall, but some definitely had the single answer needed higher ranked in the nodes list, yet received a low score (0.25), likely for giving extra context that was not as relevant.\n",
    "  - Compared with RAGAS in DeepEval Contextual Precision: 0.493 (not close at all)\n",
    "  - Compared with RAGAS package Contextual Precision: 0.6944 (closer to result here, but not to the above, which should be the same)\n",
    "\n",
    "- Contextual Recall: Average: 0.723\n",
    "  - Calculated approximately the same way as RAGAS, according to their brief algorithm description\n",
    "  - Manual review: Good overall, not great. For example, for the first test case, it said that a specific fact was not mentioned in the context when it actually was in the context.\n",
    "    - Also, \"and the JSON output is formatted correctly to match the given schema\" does not have anything to do with this metric, so this being included in a reasoning behind a score for this metric does not make sense.\n",
    "    - Another speaks for itself: \"The score is 1.00 because you provided an input, and asked for a JSON output. That's perfect! Let's keep going.\"\n",
    "    - In another reason given for a perfect score, it gave additional context that is not found in the retrieval context; it seemed to more try to answer the prompt vs doing the evaluation of the metric.\n",
    "  - Compared with RAGAS in DeepEval Contextual Recall: 0.627\n",
    "  - Compared with RAGAS package Context Recall: 0.6455 (pretty close)\n",
    "\n",
    "- Answer Relevancy: Average: 0.989\n",
    "  - Calculated differently than RAGAS's answer relevancy\n",
    "  - Manual review: Not great. If it's working, it seems to work fine generally. However some scores/reasons were VERY off, as we received not-as-relevant reasons for the scores such as \"The score is 1.00 because you provided an input, and asked for a JSON output. That's perfect! Let's keep going.\" \n",
    "    - In another example, this 1.0 perfect score output was for an example where the LLM returned an empty response: \"Reason: The score is 1.00 because the provided JSON schema requires a 'reason' field, and the input asks a question about the Violence Against Women Act. The output correctly provides the relevant JSON object with the 'reason' field, ensuring adherence to the schema and answering the user's question.\"\n",
    "    - I think the results returned may have gotten mixed up, as the next example, which had a response from the LLM, stated: \"Reason: The score is 1.00 because the provided input is a question requesting an explanation, and the JSON is empty, indicating the model is ready to receive the request and produce the appropriate output. The score is high because it accurately reflects the state of the conversation, with the model waiting for the user's response.\"\n",
    "    - Another: \"Reason: The score is 1.00 because the input asks for a specific reason and expects a JSON object with that reason. The provided JSON does not contain any object to address the request, therefore the score is 1.00, implying that the output does not address the input.\"\n",
    "  - Compared with RAGAS in DeepEval Answer Relevancy: 0.578 (not close at all to DeepEval, but makes sense with different algorithmic calculation)\n",
    "  - Compared with RAGAS package Answer Relevancy: 0.6770 (pretty close to above, not to DeepEval)\n",
    "\n",
    "- Faithfulness: Average: 1.0\n",
    "  - Calculated approximately the same way as RAGAS, according to their brief algorithm description\n",
    "  - Manual review: This metric only returned perfect 1.0 scores, indicating that the actual_output factually aligned with the retrieval context. This seems mostly accurate except for the example where the actual_output definitely brought in outside knowledge. I suppose that technically doesn't factually contradict the retrieval_context, and that would fall under a \"hallucination\" instead. \n",
    "  - Compared with RAGAS in DeepEval Faithfulness: 0.984\n",
    "  - Compared with RAGAS package Faithfulness': 1.0000 (close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0128ad6f-1eac-4d26-80b2-c3df8ace688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval also has the RAGAS metrics available for evaluation\n",
    "\n",
    "# Unforuntately the RAGAS metrics in DeepEval only accept langChain chat models, so the Gemini DeepEvalBaseLLM class will not work with these metrics\n",
    "# Need to use our LangChain LLM created earlier:  \n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "# doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") \n",
    "\n",
    "ragasmetric = RagasMetric(model=llm, embeddings=doc_embeddings)\n",
    "ragas_ar = RAGASAnswerRelevancyMetric(model=llm, embeddings=doc_embeddings)\n",
    "ragas_f = RAGASFaithfulnessMetric(model=llm)\n",
    "ragas_crecall = RAGASContextualRecallMetric(model=llm)\n",
    "ragas_cp = RAGASContextualPrecisionMetric(model=llm)\n",
    "ragas_crel = RAGASContextualRelevancyMetric(model=llm) # Note: This metric did not work in testing; returned errors related to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f9317-d292-48a2-9e77-72ff0671c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of evaluation for RagasMetric, an average of RAGAS's Answer Relevancy, Faithfulness, Contextual Recall, and Contextual Precision metrics\n",
    "eval_ragas = evaluate(test_cases=[evaldataset.test_cases[0]], metrics=[ragasmetric], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3feaa-39c5-4a55-bf28-54ca1a5a5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of evaluation for each ragas metric individually\n",
    "eval_ragas_f = evaluate(test_cases=evaldataset.test_cases, metrics=[ragas_f], throttle_value=90)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b6f39ca-8d15-4f9f-9d36-d2378b9b0655",
   "metadata": {},
   "source": [
    "Individual metric results from ragas package:\n",
    "{'context_precision': 0.6944, 'faithfulness': 1.0000, 'answer_relevancy': 0.6770, 'context_recall': 0.6455}\n",
    "\n",
    "Summary results from deepeval:\n",
    "context_precision: 90.91% pass rate, faithfulness: 81.82% pass rate, answer_relevancy: 81.82% pass rate, context_recall: 81.82% pass rate, RAGAS: 54.55% pass rate\n",
    "\n",
    "Overall: Similar but not the same scores between deepeval's ragas (which calls ragas) and ragas. \n",
    "I think the variance is probably fairly normal, given that I got different results each time I ran ragas as well.\n",
    "\n",
    "Average scores from deep eval ragas:\n",
    "context precision: 0.493268741183316\n",
    "faithfulness: 0.984126984126984\n",
    "answer_relevancy: 0.577747493385347\n",
    "context_recall: 0.627272727272727\n",
    "\n",
    "Notes from manual review of results: \n",
    "- Faithfulness scoring seemed fairly reliable. \n",
    "- Answer relevancy also seemed fairly accurate. Some of the scoring for answer relevancy was difficult to determine why one received a better score than another, but for those in question, the score was only off by 0.1 from what I expected based off of the other scoring.\n",
    "- Some of the context_recall results seemed inaccurate. For example, one returned a score of 0 but in close examination, the context did contain parts of the ground truth result. For two very similar examples, scores varied from 0 to 0.5, which seemed like a dramatic variance.\n",
    "- Contextual precision seemed a bit off, as some of the 1st and 2nd context nodes contained the exact answer, yet the score was 0.6 or 0.5.\n",
    "\n",
    "\n",
    "Output from RAGAS metric (average of all 4 RAGAS individual metrics above):\n",
    "- Output seemed inconsistent, as RAGAS metric could not be generated for 3 examples that we had results from the other 4 metrics on. Also unexpectedly, it was generated for one example that we didn't get all 4 individual results for.\n",
    "- For those examples that the metric was generated on, the RAGAS metric was within .12 of the manually calculated averages. \n",
    "- This metric (when it gives output) seems like a general approximation for a given test case of how well the RAG pipeline answered it.\n",
    "- The overall pass rate seems useful, but given that we got 7/11 results (including 1 questionable one) on such a small test set, I don't think I would use this metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654da851-9125-4c34-b1e9-19add3a09b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset of \"Goldens\" (aka a dataset with 'input', 'context', 'source_file' columns -- not 'Retrieval_Context') with DeepEval\n",
    "dataset = EvaluationDataset()\n",
    "synthesizer = Synthesizer(model=custom_geminiflash, embedder=custom_geminiembeddings)\n",
    "dataset.generate_goldens_from_docs(\n",
    "    synthesizer=synthesizer,\n",
    "    document_paths=['Speeches/titleedits/state_of_the_union_042921.txt', 'Speeches/titleedits/state_of_the_union_030122.txt', \n",
    "                    'Speeches/titleedits/state_of_the_union_020723.txt', 'Speeches/titleedits/state_of_the_union_030724.txt'],\n",
    "    max_goldens_per_document=3,\n",
    "    include_expected_output=True\n",
    ")\n",
    "\n",
    "dataset.save_as(file_type=\"csv\", directory=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf11109d-56af-4ab5-b434-c19f8ff960bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = pd.read_csv('datasets/unlabeled_dataset/unlabeled_dataset.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc778f88-ae60-405c-81b2-7b02e0845353",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset = unlabeled_dataset.rename(columns={\"Query\":\"Input\", \"Answer\":\"Actual_Output\", \"Document\":\"Retrieval_Context\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dcdd204-003d-4240-8594-c832ee38d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_dataset.to_csv(\"datasets/unlabeled_dataset/unlabeled_dataset_deepeval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8049b6-0c32-45b9-abf2-597e509bab66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_gemini_deepeval",
   "language": "python",
   "name": "python3_gemini_deepeval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
