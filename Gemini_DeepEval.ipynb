{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e651c99a-fc6c-4b3e-927e-5b7dae9813fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG with Gemini Flash 1.5 LLM and DeepEval evaluation\n",
    "# Google Gemini: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# DeepEval: https://docs.confident-ai.com/docs/guides-rag-evaluation\n",
    "# faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dafebe7-76cf-41d8-8734-0f06d15b9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish RAG pipeline with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbdda733-b425-42f1-831a-6d71952ac5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import instructor\n",
    "import deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8297c3a4-1366-4e38-9a4c-10f22905e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental variable to opt out of DeepEval tracking telemetry data\n",
    "os.environ[\"DEEPEVAL_TELEMETRY_OPT_OUT\"] = \"YES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03796ba-74cc-4bad-99a2-a594b3898399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepeval.telemetry_opt_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90ecc13-1002-4dad-82e5-cf42b656718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc3144c-2ac2-4448-a86a-a514ee26b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up local API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c3cd9fd-5949-4b1e-9894-c26618049ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eab73af-ed30-4840-be7b-20eb1c66807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document database\n",
    "# using 4 State of the Union speeches, all text from whitehouse.gov briefing room speeches posted online\n",
    "# Example from 2024:\n",
    "# https://www.whitehouse.gov/briefing-room/speeches-remarks/2024/03/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery-2/\n",
    "sotu = []\n",
    "newfiles = [\"./Speeches/titleedits/state_of_the_union_042921.txt\", \"./Speeches/titleedits/state_of_the_union_030122.txt\", \"./Speeches/titleedits/state_of_the_union_020723.txt\", \"./Speeches/titleedits/state_of_the_union_030724.txt\"]\n",
    "for i in newfiles:\n",
    "    with open(i) as file:\n",
    "        for line in file:\n",
    "            nl = line.rstrip()\n",
    "            if nl != '':\n",
    "                sotu.append(nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fcdf1fa-c3c4-4f71-9585-a96bc9c02c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(text=line) for line in sotu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab816039-1c34-4073-84a6-e57ad84832e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='12476fb7-47ee-41a0-a4da-e40dbfcfea37', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='May God protect our troops.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a loaded Document line\n",
    "documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41cce1c9-8df4-492b-9d15-18dd67cf3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Set up the faiss index\n",
    "d = 768 # dimensions of ___, the embedding model that we're going to use\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "print(faiss_index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e48c5f84-8fc4-4970-9505-3c67705b151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the embeddings\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # optional: task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "Settings.embed_model = doc_embeddings\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82ce1780-061d-4d6d-bf3c-6853db1eb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment for when you need to re-embed and vectorize documents\n",
    "## otherwise, doing local loading below\n",
    "#vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "#storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#index = VectorStoreIndex.from_documents(\n",
    "#    documents, storage_context=storage_context, show_progress=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "433fb37c-fb44-409c-8b42-066cf7a8e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "#index.storage_context.persist()\n",
    "#index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d046f3-293f-4bac-be0c-bedc5d88012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "# index id 'cef7ae30-ff1e-404a-bce6-85d59ca4b376' uses the speeches with a title that includes the date it was given\n",
    "index = load_index_from_storage(storage_context=storage_context, index_id='cef7ae30-ff1e-404a-bce6-85d59ca4b376')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a1ba96f-37e3-4160-bf13-5536c2cbaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query and chat engines\n",
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb6cee44-9396-41b0-b9f8-e6fd633fef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query and response\n",
    "query = \"In detail, what has the President done to improve the economy over the four years of his speeches?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48698c67-efba-4fe3-a2e1-864a3f43131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The President highlights the creation of over 1.3 million jobs in the first 100 days of his term, a record 12 million jobs created in two years, and a strong economic growth rate of 5.7% in the previous year. He also emphasizes the International Monetary Fund's prediction of an economic growth rate exceeding 6% for the current year. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05980859-d7e3-44e8-bc4f-10a41fdd111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of DeepEval implementation, following their guide for RAG\n",
    "# https://docs.confident-ai.com/docs/guides-rag-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfebf282-9965-49bd-9c7f-d0e75e669b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from deepeval.models import DeepEvalBaseLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "002bf206-84b1-4bae-ad0c-d23dad165360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval requires a json response... In practice, this has led to errors, even with as simple of a schema as this\n",
    "class Response(BaseModel):\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad054ba9-9fb0-4e3d-9f48-7e60eb379ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Open-AI requieres a custom LLM class for using DeepEval\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel: \n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Gemini 1.5 Flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b903c42-fc2d-4fe2-bb1e-faaa7de1f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly, a custom embedding model class is required for non Open-AI embeddings\n",
    "from typing import List, Optional\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from deepeval.models import DeepEvalBaseEmbeddingModel\n",
    "\n",
    "class CustomGeminiEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_model(self):\n",
    "        return GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\"\n",
    "        )\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_query(text)\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_query(text)\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_documents(texts)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        \"Custom Gemini Embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e11dcd46-4410-4ae9-bb68-3a386d6484c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_geminiflash = CustomGeminiFlash()\n",
    "custom_geminiembeddings = CustomGeminiEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3568b677-18b8-4cc7-818f-8728b1c6fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test example for using the custom LLM model class\n",
    "# test = custom_geminiflash.generate(prompt=\"How many different types of clouds are there?\", schema=Response)\n",
    "# test.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeae71ae-b309-4c3a-9949-5566626067d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1da71503-d604-4936-a243-c3d30b6461f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric(model=custom_geminiflash)\n",
    "contextual_recall = ContextualRecallMetric(model=custom_geminiflash)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=custom_geminiflash)\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What measures does the speaker propose to lower prescription drug costs in America?\",\n",
    "    actual_output=\"The speaker proposes giving Medicare the power to negotiate lower prescription drug prices, capping prescription drug costs at $2,000 a year for everyone, and allowing Medicare to negotiate lower prices for 500 drugs over the next decade.s\",\n",
    "    expected_output=\"The speaker proposes that Medicare should be given the power to negotiate lower drug prescription prices. They argue that this would save hundreds of billions of dollars and lower prescription drug costs for everyone. The speaker also states that the money saved could be used to strengthen the Affordable Care Act and expand Medicare coverage benefits without costing taxpayers an additional penny.\",\n",
    "    retrieval_context=['Let’s do what we’ve always talked about for all the years I was down here in this — in this body — in Congress.  Let’s give Medicare the power to save hundreds of billions of dollars by negotiating lower drug prescription prices.  (Applause.)', 'In fact, we pay the highest prescription drug prices of anywhere in the world right here in America — nearly three times — for the same drug, nearly three times what other countries pay.  We have to change that, and we can.', 'And we’re finally giving Medicare the power to negotiate drug prices. Bringing down prescription drug costs doesn’t just save seniors money.', 'For years people have talked about it but I finally got it done and gave Medicare the power to negotiate lower prices for prescription drugs just like the VA does for our veterans.', 'And, by the way, that won’t just — that won’t just help people on Medicare; it will lower prescription drug costs for everyone.', 'Now I want to cap prescription drug costs at $2,000 a year for everyone!', 'We know how to do this.  The last President had that as an objective.  We all know how outrageously expensive drugs are in America.', 'Make no mistake, if you try to do anything to raise the cost of prescription drugs, I will veto it.', 'Now it’s time to go further and give Medicare the power to negotiate lower prices for 500 drugs over the next decade.', 'It will cut the federal deficit, saving tax payers hundreds of billions of dollars on the prescription drugs the government buys for Medicare.']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ef1ea0f-8367-4747-84eb-f16f79895d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Precision Score:  0.9095238095238096\n",
      "Contextual Precision Reason:  The score is 0.91 because the first five nodes in the retrieval context are relevant, while the last four are not. The 'no' verdicts should be ranked lower as they do not explicitly propose measures to lower prescription drug costs, with the sixth node focusing on a consequence instead of a measure, the seventh node mentioning the high cost but not offering a solution, and the eighth and ninth nodes mentioning opposition to raising costs and the potential savings but not a specific measure.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Recall Score:  1.0\n",
      "Contextual Recall Reason:  The score is 1.00 because the speaker proposes Medicare to negotiate lower drug prices and this is supported by the first node in the retrieval context.  This node directly discusses Medicare's power to negotiate lower drug prices and the potential savings.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Relevancy Score:  0.4\n",
      "Contextual Relevancy Reason:  The score is 0.4 because the context mentions lowering drug costs but doesn't mention any \"specific measures\" to achieve that, as per the reasons for irrelevancy. For example, \"The context discusses the high prescription drug prices in America compared to other countries, but it doesn't mention any specific measures to lower costs.\"\n"
     ]
    }
   ],
   "source": [
    "# Example of measuring metrics individually for one test_case\n",
    "\n",
    "# Retrieval metrics:\n",
    "contextual_precision.measure(test_case)\n",
    "print(\"Contextual Precision Score: \", contextual_precision.score)\n",
    "print(\"Contextual Precision Reason: \", contextual_precision.reason)\n",
    "\n",
    "contextual_recall.measure(test_case)\n",
    "print(\"Contextual Recall Score: \", contextual_recall.score)\n",
    "print(\"Contextual Recall Reason: \", contextual_recall.reason)\n",
    "\n",
    "contextual_relevancy.measure(test_case)\n",
    "print(\"Contextual Relevancy Score: \", contextual_relevancy.score)\n",
    "print(\"Contextual Relevancy Reason: \", contextual_relevancy.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cd1ac8d-1f6f-47bb-87df-87528474486a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Relevancy Score:  1.0\n",
      "Answer Relevancy Reason:  The score is 1.00 because the output provides the perfect JSON response based on the given schema, which is exactly what is needed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score:  1.0\n",
      "Faithfulness Reason:  Amazing! You've got no contradictions, which means the actual output perfectly aligns with the retrieval context. Keep up the great work!\n"
     ]
    }
   ],
   "source": [
    "# Generation metrics:\n",
    "answer_relevancy = AnswerRelevancyMetric(model=custom_geminiflash)\n",
    "faithfulness = FaithfulnessMetric(model=custom_geminiflash)\n",
    "                                 \n",
    "answer_relevancy.measure(test_case)\n",
    "print(\"Answer Relevancy Score: \", answer_relevancy.score)\n",
    "print(\"Answer Relevancy Reason: \", answer_relevancy.reason)\n",
    "\n",
    "faithfulness.measure(test_case)\n",
    "print(\"Faithfulness Score: \", faithfulness.score)\n",
    "print(\"Faithfulness Reason: \", faithfulness.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c32b6c3f-c08a-439f-9996-d33d20ca6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of measuring metrics in bulk for multiple test_cases / a full dataset\n",
    "\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "\n",
    "# Load manually curated dataset\n",
    "evaldataset = EvaluationDataset()\n",
    "evaldataset.add_test_cases_from_csv_file(\n",
    "    file_path=\"datasets/manual_dataset_complete.csv\",\n",
    "    input_col_name=\"Input\",\n",
    "    actual_output_col_name=\"Actual_Output\",\n",
    "    expected_output_col_name=\"Expected_Output\",\n",
    "#    context_col_name=\"context\",\n",
    "#    context_col_delimiter= \",\",\n",
    "    retrieval_context_col_name=\"Retrieval_Context\",\n",
    "    retrieval_context_col_delimiter= \",\"\n",
    "#    additional_metadata_col_name=\"source_file\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c7d54857-001b-403c-9838-01082c365a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric\n",
    "\n",
    "# For RAG systems, DeepEval recommends the following metrics:\n",
    "# Retriever metrics:\n",
    "contextual_precision = ContextualPrecisionMetric(model=custom_geminiflash)\n",
    "contextual_recall = ContextualRecallMetric(model=custom_geminiflash)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=custom_geminiflash) # this was the only metric that would not work on the manually curated dataset (429 errors)\n",
    "\n",
    "# Generation metrics:\n",
    "answer_relevancy = AnswerRelevancyMetric(model=custom_geminiflash)\n",
    "faithfulness = FaithfulnessMetric(model=custom_geminiflash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c84960d-d52d-4178-8ef0-088e818b71db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using Gemini </span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">1.5</span><span style=\"color: #374151; text-decoration-color: #374151\"> Flash, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing Gemini \u001b[0m\u001b[1;38;2;55;65;81m1.5\u001b[0m\u001b[38;2;55;65;81m Flash, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 1 test case(s) in parallel: |                                   |  0% (0/1) [Time Taken: 01:30, ?test case/s]\n"
     ]
    },
    {
     "ename": "InstructorRetryException",
     "evalue": "RetryError[<Future at 0x7f7b803d48d0 state=finished raised ResourceExhausted>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/instructor/retry.py:161\u001b[0m, in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, validation_context, args, kwargs, max_retries, strict, mode)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     stream \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/google/generativeai/generative_models.py:331\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:827\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 827\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota).",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/instructor/retry.py:158\u001b[0m, in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, validation_context, args, kwargs, max_retries, strict, mode)\u001b[0m\n\u001b[1;32m    157\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/tenacity/__init__.py:443\u001b[0m, in \u001b[0;36mBaseRetrying.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/tenacity/__init__.py:419\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m--> 419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x7f7b803d48d0 state=finished raised ResourceExhausted>]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInstructorRetryException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2 Options for Metrics Evaluation: \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1) Iterating through test cases seems to work better than bulk evaluation with evaluate,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     eval_context_rel = evaluate(test_cases=[evaldataset.test_cases[i]], metrics=[contextual_relevancy], throttle_value=90)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     context_rel_results.append(eval_context_rel[0])\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m eval_context_rel \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mevaldataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_cases\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcontextual_relevancy\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrottle_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 2) Faithfulness, contextual_precision were fine with evaluate(...) in bulk\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# bulk evaluation of test_cases; throttle_value is for rate limiting- in seconds between queries\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#test_precision = evaluate(test_cases=evaldataset.test_cases, metrics=[contextual_precision], throttle_value=90)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/deepeval/evaluate.py:784\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(test_cases, metrics, hyperparameters, run_async, show_indicator, print_results, write_cache, use_cache, ignore_errors, verbose_mode, throttle_value)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_async:\n\u001b[1;32m    783\u001b[0m     loop \u001b[38;5;241m=\u001b[39m get_or_create_event_loop()\n\u001b[0;32m--> 784\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43ma_execute_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthrottle_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrottle_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m     test_results \u001b[38;5;241m=\u001b[39m execute_test_cases(\n\u001b[1;32m    798\u001b[0m         test_cases,\n\u001b[1;32m    799\u001b[0m         metrics,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    804\u001b[0m         show_indicator\u001b[38;5;241m=\u001b[39mshow_indicator,\n\u001b[1;32m    805\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/asyncio/tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/deepeval/evaluate.py:511\u001b[0m, in \u001b[0;36ma_execute_test_cases\u001b[0;34m(test_cases, metrics, ignore_errors, use_cache, show_indicator, throttle_value, save_to_disk, verbose_mode, test_run_manager, _use_bar_indicator)\u001b[0m\n\u001b[1;32m    508\u001b[0m                     tasks\u001b[38;5;241m.\u001b[39mappend(asyncio\u001b[38;5;241m.\u001b[39mcreate_task(task))\n\u001b[1;32m    510\u001b[0m                 \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(throttle_value)\n\u001b[0;32m--> 511\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m test_case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/asyncio/tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/deepeval/evaluate.py:592\u001b[0m, in \u001b[0;36ma_execute_llm_test_cases\u001b[0;34m(metrics, test_case, test_run_manager, test_results, count, test_run, ignore_errors, use_cache, show_indicator, _use_bar_indicator, pbar)\u001b[0m\n\u001b[1;32m    590\u001b[0m new_cached_test_case: CachedTestCase \u001b[38;5;241m=\u001b[39m CachedTestCase()\n\u001b[1;32m    591\u001b[0m test_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m--> 592\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m measure_metrics_with_indicator(\n\u001b[1;32m    593\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[1;32m    594\u001b[0m     test_case\u001b[38;5;241m=\u001b[39mtest_case,\n\u001b[1;32m    595\u001b[0m     cached_test_case\u001b[38;5;241m=\u001b[39mcached_test_case,\n\u001b[1;32m    596\u001b[0m     ignore_errors\u001b[38;5;241m=\u001b[39mignore_errors,\n\u001b[1;32m    597\u001b[0m     show_indicator\u001b[38;5;241m=\u001b[39mshow_metrics_indicator,\n\u001b[1;32m    598\u001b[0m )\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m    601\u001b[0m     metric_data \u001b[38;5;241m=\u001b[39m create_metric_data(metric)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/deepeval/metrics/indicator.py:171\u001b[0m, in \u001b[0;36mmeasure_metrics_with_indicator\u001b[0;34m(metrics, test_case, cached_test_case, ignore_errors, show_indicator)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m         tasks\u001b[38;5;241m.\u001b[39mappend(safe_a_measure(metric, test_case, ignore_errors))\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/asyncio/tasks.py:279\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/deepeval/metrics/indicator.py:181\u001b[0m, in \u001b[0;36msafe_a_measure\u001b[0;34m(metric, tc, ignore_errors)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m metric\u001b[38;5;241m.\u001b[39ma_measure(tc, _show_indicator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m metric\u001b[38;5;241m.\u001b[39ma_measure(tc)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py:94\u001b[0m, in \u001b[0;36mContextualRelevancyMetric.a_measure\u001b[0;34m(self, test_case, _show_indicator)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_cost \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musing_native_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     async_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m     _show_indicator\u001b[38;5;241m=\u001b[39m_show_indicator,\n\u001b[1;32m     92\u001b[0m ):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverdicts: List[ContextualRelevancyVerdict] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 94\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_verdicts(\n\u001b[1;32m     95\u001b[0m             test_case\u001b[38;5;241m.\u001b[39minput, test_case\u001b[38;5;241m.\u001b[39mretrieval_context\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_score()\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_reason(test_case\u001b[38;5;241m.\u001b[39minput)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py:209\u001b[0m, in \u001b[0;36mContextualRelevancyMetric._a_generate_verdicts\u001b[0;34m(self, text, retrieval_context)\u001b[0m\n\u001b[1;32m    207\u001b[0m     task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_a_generate_verdict(prompt))\n\u001b[1;32m    208\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(task)\n\u001b[0;32m--> 209\u001b[0m verdicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m verdicts\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/deepeval/metrics/contextual_relevancy/contextual_relevancy.py:190\u001b[0m, in \u001b[0;36mContextualRelevancyMetric._a_generate_verdict\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39ma_generate(\n\u001b[1;32m    191\u001b[0m             prompt, schema\u001b[38;5;241m=\u001b[39mContextualRelevancyVerdict\n\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m, in \u001b[0;36mCustomGeminiFlash.a_generate\u001b[0;34m(self, prompt, schema)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21ma_generate\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt: \u001b[38;5;28mstr\u001b[39m, schema: BaseModel) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseModel:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mCustomGeminiFlash.generate\u001b[0;34m(self, prompt, schema)\u001b[0m\n\u001b[1;32m      9\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model()\n\u001b[1;32m     10\u001b[0m instructor_client \u001b[38;5;241m=\u001b[39m instructor\u001b[38;5;241m.\u001b[39mfrom_gemini(\n\u001b[1;32m     11\u001b[0m     client\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m     12\u001b[0m     mode\u001b[38;5;241m=\u001b[39minstructor\u001b[38;5;241m.\u001b[39mMode\u001b[38;5;241m.\u001b[39mGEMINI_JSON,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43minstructor_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/instructor/client.py:116\u001b[0m, in \u001b[0;36mInstructor.create\u001b[0;34m(self, response_model, messages, max_retries, validation_context, strict, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    107\u001b[0m     response_model: \u001b[38;5;28mtype\u001b[39m[T] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T \u001b[38;5;241m|\u001b[39m Any \u001b[38;5;241m|\u001b[39m Awaitable[T] \u001b[38;5;241m|\u001b[39m Awaitable[Any]:\n\u001b[1;32m    114\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_kwargs(kwargs)\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/instructor/patch.py:143\u001b[0m, in \u001b[0;36mpatch.<locals>.new_create_sync\u001b[0;34m(response_model, validation_context, max_retries, strict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_create_sync\u001b[39m(\n\u001b[1;32m    133\u001b[0m     response_model: \u001b[38;5;28mtype\u001b[39m[T_Model] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: T_ParamSpec\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[1;32m    139\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T_Model:\n\u001b[1;32m    140\u001b[0m     response_model, new_kwargs \u001b[38;5;241m=\u001b[39m handle_response_model(\n\u001b[1;32m    141\u001b[0m         response_model\u001b[38;5;241m=\u001b[39mresponse_model, mode\u001b[38;5;241m=\u001b[39mmode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    142\u001b[0m     )\n\u001b[0;32m--> 143\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mretry_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/research2/lib/python3.11/site-packages/instructor/retry.py:195\u001b[0m, in \u001b[0;36mretry_sync\u001b[0;34m(func, response_model, validation_context, args, kwargs, max_retries, strict, mode)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InstructorRetryException(\n\u001b[1;32m    196\u001b[0m         e,\n\u001b[1;32m    197\u001b[0m         last_completion\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[1;32m    198\u001b[0m         n_attempts\u001b[38;5;241m=\u001b[39mattempt\u001b[38;5;241m.\u001b[39mretry_state\u001b[38;5;241m.\u001b[39mattempt_number,\n\u001b[1;32m    199\u001b[0m         messages\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    200\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))\n\u001b[1;32m    201\u001b[0m         ),\n\u001b[1;32m    202\u001b[0m         total_usage\u001b[38;5;241m=\u001b[39mtotal_usage,\n\u001b[1;32m    203\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mInstructorRetryException\u001b[0m: RetryError[<Future at 0x7f7b803d48d0 state=finished raised ResourceExhausted>]"
     ]
    }
   ],
   "source": [
    "# 2 Options for Metrics Evaluation: \n",
    "\n",
    "# 1) Iterating through test cases seems to work better than bulk evaluation with evaluate,\n",
    "# as errors encountered with evaluate(...) cause no results to be returned\n",
    "# Looping at least saves partial results until an error occurs\n",
    "# Encountered this sometimes with contextual_relevancy and contextual_precision on the test dataset (429 errors or Invalid JSON errors),\n",
    "# yet typically was fine if iterated through individual test_cases\n",
    "# For future: https://github.com/confident-ai/deepeval/issues/964 may assist with incorrect json errors like what was being returned\n",
    "\n",
    "# Example for faithfulness metric\n",
    "# faithfulness_results = []\n",
    "# for i in range(len(evaldataset.test_cases)):\n",
    "#     eval_faithfulness = evaluate(test_cases=[evaldataset.test_cases[i]], metrics=[faithfulness], throttle_value=90)\n",
    "#     faithfulness_results.append(eval_faithfulness[0])\n",
    "\n",
    "# 2) Evaluate through test_cases in bulk; In testing, at least faithfulness, contextual_precision metrics worked this way with the manually curated dataset\n",
    "\n",
    "# bulk evaluation of test_cases; throttle_value is for rate limiting- in seconds between queries\n",
    "test_precision = evaluate(test_cases=evaldataset.test_cases, metrics=[contextual_precision], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fd280f4f-5ec1-405b-9e3f-062f1908a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 1.00 because all relevant nodes are ranked higher than the irrelevant node, which is the last node. The last node is irrelevant because it's a general statement about presidential speeches, not about economic growth. All other nodes are relevant because they either explicitly state or imply the President's plans to grow the economy, and provide examples of economic successes. For instance, the first node states that the economy created more than 1,300,000 new jobs in 100 days, which indicates growth, and the second node quotes the President saying 'trickle-down economics has never worked and it’s time to grow the economy from the bottom and the middle out,' which directly supports the input., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: How does the President plan to grow the economy in his first speech, and what are examples of his successes in this area as shared in his next three speeches?\n",
      "  - actual output: The President plans to grow the economy by focusing on the bottom and middle class, rather than trickle-down economics.  His success in this area includes creating more than 1,300,000 new jobs in his first 100 days, a record 12 million new jobs in his first two years, and a 5.7% growth in the economy last year, which is the strongest growth in nearly 40 years. \n",
      "\n",
      "  - expected output: In his first speech, the President proposes that his American Jobs Plan will create millions of new jobs and trillions of dollars of economic growth to the economy by bringing Americans back to the workforce. He also says America should grow its economy from the bottom and middle out but does not say how to grow the economy that way. In his following speeches, he says that the American Rescue Plan has created over 6.5 million jobs in one year and 12 million jobs in two years and 15 million jobs in three years, and America�s economy increased at a rate of 5.7% the first year. American companies are also investing in increasing manufacturing capabilities in America that would help reduce costs to Americans and created 800,000 manufacturing jobs in 2 years. America�s GDP also increased over the three years covered in the speeches.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['And in the process\", ' while this was all going on', ' the economy created more than 1', '300', \"000 new jobs in 100 days — more jobs in the first — (applause) — more jobs in the first 100 days than any President on record.'\", \" 'My fellow Americans\", \" trickle-down — trickle-down economics has never worked and it’s time to grow the economy from the bottom and the middle out. (Applause.)'\", \" 'Our economy grew at a rate of 5.7% last year\", ' the strongest growth in nearly 40 years', \" the first step in bringing fundamental change to an economy that hasn’t worked for the working people of this nation for too long.'\", \" 'The International Monetary Fund — (applause) — the International Monetary Fund is now estimating our economy will grow at a rate of more than 6 percent this year.  That will be the fastest pace of economic growth in this country in nearly four decades.'\", \" 'I inherited an economy that was on the brink. Now our economy is the envy of the world!'\", \" 'As I stand here tonight\", ' we have created a record 12 million new jobs', \" more jobs created in two years than any president has ever created in four years.'\", \" 'And by the way\", ' when we do all of these things', \" we increase productivity. We increase economic growth.'\", \" 'Since I’ve come to office\", \" our GDP is up.'\", \" 'You know\", ' there’s a broad consensus of economists — left', ' right', \" center — and they agree what I’m proposing will help create millions of jobs and generate historic economic growth.  These are among the highest-value investments we can make as a nation.'\", \" 'Throughout our history\", ' Presidents have come to this chamber to speak to Congress', ' to the nation', ' and to the world to declare war', ' to celebrate peace', \" to announce new plans and possibilities.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 1.00 because all the nodes in the retrieval context were perfectly relevant to the input. The first node provides a specific example of 'investments in jobs and infrastructure' which perfectly aligns with the input. The second node highlights the 'Jobs Plan' and further mentions 'Democrats and Republicans' which directly relates to the user's request. The third node states that 'Democrats and Republicans came together' while the fourth node mentions that Biden 'believed in the potential for bipartisan cooperation' which also matches the user's request. The fifth node highlights '80 bipartisan bills into law' which directly answers the input, further making it relevant. The sixth node is particularly relevant as it specifically addresses 'Republican friends' and 'working together'. Similarly, the seventh node talks about 'bipartisan group of Senators' and 'negotiations'. The eighth node mentions 'bipartisan efforts resulted in a significant infrastructure law' which aligns with the user's request. The ninth node mentions a 'bipartisan effort resulting in border security reforms' which again perfectly answers the user's request. The tenth node is also relevant as it mentions that 'Democrats had to go it alone' which is a valid answer to the user's request., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Given Biden's statement about bipartisan cooperation, identify specific instances where Democrats and Republicans have collaborated during his presidency.\n",
      "  - actual output: The President highlights several instances of bipartisan cooperation during his presidency, including passing legislation to prevent government shutdowns, protect Asian-Americans from hate crimes, reform military justice, and passing an infrastructure bill.  He also mentions working with Republicans on border security reforms. \n",
      "\n",
      "  - expected output: Some examples of bipartisan cooperation during Biden�s administration are Congress passing a significant infrastructure law (the American Rescue Plan), helping veterans exposed to toxic burn pits with the PACT Act, reauthorizing the Violence Against Women Act, passing the Electoral Count Reform Act, passing the CHIPS and Science Act, and authorizing the Respect for Marriage Act.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['Investments in jobs and infrastructure\", ' like the ones we’re talking about', \" have often had bipartisan support in the past.  Vice President Harris and I met regularly in the Oval Office with Democrats and Republicans to discuss the Jobs Plan.  And I applaud a group of Republican senators who just put forward their own proposal.'\", \" 'You know\", \" we’re often told that Democrats and Republicans can’t work together.'\", \" 'But time and again\", \" Democrats and Republicans came together.'\", \" 'When I came to office\", \" most everyone assumed bipartisanship was impossible. But I never believed it.'\", \" 'While it often appears that we never agree\", \" that isn’t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.'\", \" 'To my Republican friends\", ' if we could work together in the last Congress', \" there is no reason we can’t work together in this new Congress.'\", \" 'In November\", \" my team began serious negotiations with a bipartisan group of Senators.'\", \" 'Came together to pass a once-in-a-generation infrastructure law\", \" building bridges to connect our nation and people.'\", \" 'The result was a bipartisan bill with the toughest set of border security reforms we’ve ever seen'\", \" 'Yes\", ' we disagreed plenty. And yes', \" there were times when Democrats had to go it alone.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Contextual Precision (score: 0.25, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 0.25 because the first three nodes are irrelevant: 'This passage talks about the \"American Jobs Plan\" and how it will be a big increase in nondefense research and development. The passage then talks about AI, stating that technological change will be more rapid in the next 10 years than the last 50, which is not a specific example of AI legislation.', 'This passage simply provides applause and does not mention any AI legislation or advancements.', and 'The passage mentions the \"Chips and Science Act\" and states that China and other countries are advancing quickly in technological development. It does not mention any instances of passing AI legislation.' The fourth node states that the \"Chips and Science Act\" is an example of legislation passed by the administration. This act is stated to be an investment in research and development, which would include AI. However, the fifth node, 'This passage mentions restoring a program and does not mention any AI legislation or advancements.', is also irrelevant. This makes the contextual precision low as relevant nodes are not ranked highly., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What does the President say his administration has successfully done related to AI?\n",
      "  - actual output: The President mentions that the United States is investing more in research and development than ever before, thanks to the Chips and Science Act. This is related to AI as the President specifically mentions that the United States will see more technological change in the next 10 years than the last 50, due in part to the rapid development of AI. \n",
      "\n",
      "  - expected output: The President does not note any instances of passing AI legislation. However, his administration did pass the American Jobs Act and the Chips and Science Act which will contribute to technological research and advancements in America, some of which, he notes, will likely be related to artificial intelligence.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['And finally\", \" the American Jobs Plan will be the biggest increase in nondefense research and development on record.  We will see more technological change — and some of you know more about this than I do — we’ll see more technological change in the next 10 years than we saw in the last 50.  That’s how rapidly artificial intelligence and so much more is changing.'\", \" 'THE PRESIDENT:  — because I know it will get done.  (Applause.)'\", \" 'Today\", ' Mr. Secretary', \" that’s less than 1 percent.  China and other countries are closing in fast.  We have to develop and dominate the products and technologies of the future:'\", \" 'Thanks to my Chips and Science Act the United States is investing more in research and development than ever before.'\", \" 'I’m restoring the program and asked Vice President Harris to lead our diplomatic effort to take care of this.  I have absolute confidence she’ll get the job done.  (Applause.)'\", \" 'I’ve made sure that the most advanced American technologies can’t be used in China’s weapons.'\", \" 'The National Institute of Health — the NIH –- I believe\", \" should create a similar Advanced Research Projects Agency for Health.  (Applause.)'\", \" 'Every Administration says they’ll do it\", \" but we are actually doing it.'\", \" 'The Secretary of Defense can tell you — and those of you on — who work on national security issues know — the Defense Department has an agency called DARPA — the Defense Advanced Research Project Agency.  The people who set up before I came here — and that’s been a long time ago — to develop breakthroughs that enhance our national security -– that’s their only job.  And it’s a semi-separate agency; it’s under the Defense Department.  It’s led to everything from the discovery of the Internet to GPS and so much more that has enhanced our security.'\", \" 'Investing in our alliances and working with our allies to protect our advanced technologies so they’re not used against us.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 1.00 because all relevant nodes are ranked higher than irrelevant nodes., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: How has the administration improved housing issues during his term in office?\n",
      "  - actual output: The administration has implemented several initiatives to improve housing issues, including providing rental assistance to prevent evictions, offering loans to small businesses to help them keep employees, and making it easier for builders to access federal financing to build more housing units.  Additionally, the administration is working to lower the cost of housing by cracking down on landlords who engage in price-fixing, providing a tax credit to help people afford their mortgages, and proposing a plan to build and renovate 2 million affordable homes. \n",
      "\n",
      "  - expected output: The American Rescue Plan provided guards to keep people from eviction during the covid-19 pandemic. Later in his presidency, he highlighted work to eliminate title insurance fees on federally backed mortgages, improve opportunities for builders to get finances, reduce price-fixing of rental units, provide housing assistance to veterans, and other work to reduce the price of housing to Americans. He called on Congress to support greater investments in housing reform going forward.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['I’ve cut red tape so more builders can get federal financing\", \" which is already helping build a record 1.7 million housing units nationwide.'\", \" 'My Administration is also eliminating title insurance fees for federally backed mortgages.'\", \" 'We’re also providing rental assistance — you all know this\", ' but the American people', ' I want to make sure they understand — keeping people from being evicted from their homes', \" providing loans to small businesses to reopen and keep their employees on the job.'\", \" 'Let’s also finish the job and get more families access to affordable and quality housing.'\", \" 'My administration is providing assistance with job training and housing\", \" and now helping lower-income veterans get VA care debt-free.'\", \" 'Now pass my plan to build and renovate 2 million  affordable homes and bring those rents down!'\", \" 'For millions of renters\", \" we’re cracking down on big landlords who break antitrust laws by price-fixing and driving up rents.'\", \" 'I want to provide an annual tax creditthat will give Americans $400 a month for the next two years as mortgage rates come down to put toward their mortgage when they buy a first home or trade up for a little more space.'\", \" 'I know the cost of housing is so important to you.'\", \" 'Helped put food on their table\", ' keep a roof over their heads', \" and cut the cost of health insurance.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 1.00 because all relevant nodes are ranked higher than irrelevant nodes. For example, the first three nodes directly address the 'boyfriend loophole' and how the Violence Against Women Act aims to prevent abusers from obtaining firearms through it. The irrelevant nodes, such as the fourth node which is 'unrelated to the topic of the Violence Against Women Act', are ranked lower., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: How does the Violence Against Women Act, originally written by Biden, aim to prevent abusers from obtaining firearms through the 'boyfriend loophole'?\n",
      "  - actual output: Empty Response\n",
      "  - expected output: The Violence Against Women Act would close the boyfriend loophole and other loopholes by requiring background checks before purchasing a gun. However, the speaker does not add additional information as to how this would be done or more details about when background checks would be required.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['Another thing: Let’s authorize the Violence Against Women Act\", ' which has been law for 27 years.  (Applause.)  Twenty-seven years ago', ' I wrote it.  It’ll close the — the act that has to be authorized now will close the “boyfriend” loophole to keep guns out of the hands of abusers.  The court order said', \" “This is an abuser.  You can’t own a gun.”  It’s to close that loophole that existed.'\", \" 'To take on crimes of domestic violence\", ' I am ramping up federal enforcement of the Violence Against Women Act', ' that I proudly wrote', \" so we can finally end the scourge of violence against women in America!'\", \" 'You know\", \" it’s estimated that 50 women are shot and killed by an intimate partner every month in America — 50 a month.  Let’s pass it and save some lives.  (Applause.)'\", \" 'And soon\", \" we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things.'\", \" 'I did do something by establishing the first-ever Office of Gun Violence Prevention in the White House that Vice President Harris is leading.'\", \" 'In fact\", ' I signed over 300 bipartisan laws since becoming President. From reauthorizing the Violence Against Women Act', ' to the Electoral Count Reform Act', \" to the Respect for Marriage Act that protects the right to marry the person you love.'\", \" 'In the 10 years the ban was law\", ' mass shootings went down. After Republicans let it expire', \" mass shootings tripled.'\", \" 'That includes things that the majority of responsible gun owners support\", \" like enhanced background checks for 18 to 21-year-olds and red flag laws keeping guns out of the hands of people who are a danger to themselves and others.'\", \" 'Thank God we did\", \" passing the most sweeping gun safety law in three decades.'\", \" 'More than two weeks ago in the Rose Garden\", \" surrounded by some of the bravest people I know — the survivors and families who lost loved ones to gun violence — I laid out several of the Department of Justice a- — actions that are being taken to — impact on this epidemic.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 0.8666666666666667, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 0.87 because the relevant nodes in the retrieval contexts are ranked higher than the irrelevant nodes. For example, the second node is ranked higher than the fourth node, which is irrelevant because it describes other laws signed by the President, but it does not mention the Violence Against Women Act or its specific aim of closing the \\\"boyfriend\\\" loophole., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: How does the Violence Against Women Act, which the President helped author 27 years ago, aim to protect women from violence, especially by closing the 'boyfriend' loophole?\n",
      "  - actual output: Empty Response\n",
      "  - expected output: The Violence Against Women Act aims to protect women from violence by closing the boyfriend loophole and other loopholes by requiring background checks before purchasing a gun. However, the speaker does not add additional information as to how this would be done or more details about when background checks would be required.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['Another thing: Let’s authorize the Violence Against Women Act\", ' which has been law for 27 years.  (Applause.)  Twenty-seven years ago', ' I wrote it.  It’ll close the — the act that has to be authorized now will close the “boyfriend” loophole to keep guns out of the hands of abusers.  The court order said', \" “This is an abuser.  You can’t own a gun.”  It’s to close that loophole that existed.'\", \" 'To take on crimes of domestic violence\", ' I am ramping up federal enforcement of the Violence Against Women Act', ' that I proudly wrote', \" so we can finally end the scourge of violence against women in America!'\", \" 'In fact\", ' I signed over 300 bipartisan laws since becoming President. From reauthorizing the Violence Against Women Act', ' to the Electoral Count Reform Act', \" to the Respect for Marriage Act that protects the right to marry the person you love.'\", \" 'And soon\", \" we’ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things.'\", \" 'You know\", \" it’s estimated that 50 women are shot and killed by an intimate partner every month in America — 50 a month.  Let’s pass it and save some lives.  (Applause.)'\", \" 'I did do something by establishing the first-ever Office of Gun Violence Prevention in the White House that Vice President Harris is leading.'\", \" 'And I am asking the Vice President to lead this effort\", \" if she would —'\", \" 'All Americans deserve the freedom to be safe\", \" and America is safer today than when I took office.'\", \" 'More than two weeks ago in the Rose Garden\", \" surrounded by some of the bravest people I know — the survivors and families who lost loved ones to gun violence — I laid out several of the Department of Justice a- — actions that are being taken to — impact on this epidemic.'\", \" 'With the support of families of victims\", ' civil rights groups', ' and law enforcement', ' I signed an executive order for all federal officers banning chokeholds', ' restricting no-knock warrants', \" and other key elements of the George Floyd Act.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 0.9166666666666666, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 0.92 because the first three nodes in the retrieval context are relevant, while the rest are irrelevant. The fourth node is relevant, but the first three nodes are more relevant because they directly address the output's claims about economic growth and job creation. The fifth, sixth, seventh, eighth, ninth, and tenth nodes are irrelevant because they do not relate to the specific initiatives mentioned in the output., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Considering the economic policies implemented by the current administration, and the stated goals to increase economic growth and job creation, explain how these initiatives contribute to the observed economic expansion.\n",
      "  - actual output: The administration's economic policies aim to increase productivity and economic growth by investing in infrastructure, education, and workforce development. These investments are intended to create jobs, boost economic activity, and ultimately lead to a stronger and more resilient economy. The administration's focus on \"building the economy from the bottom up\" suggests a belief that these investments will benefit all Americans, not just the wealthy. \n",
      "\n",
      "  - expected output: The current administration created and passed the American Rescue Plan, which was partially responsible for  the creation of over 6.5 million jobs in one year and 12 million jobs in two years and 15 million jobs in three yearsin America. America’s economy increased at a rate of 5.7% the first year of the administration. American companies also invested in increasing manufacturing capabilities in America that helps reduce costs to Americans and created 800,000 manufacturing jobs in 2 years. America’s GDP also increased over the course of the administration.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['And by the way\", ' when we do all of these things', \" we increase productivity. We increase economic growth.'\", \" 'In fact—our economy created over 6.5 Million new jobs just last year\", \" more jobs created in one year'\", \" 'You know\", ' there’s a broad consensus of economists — left', ' right', \" center — and they agree what I’m proposing will help create millions of jobs and generate historic economic growth.  These are among the highest-value investments we can make as a nation.'\", \" 'Our economy grew at a rate of 5.7% last year\", ' the strongest growth in nearly 40 years', \" the first step in bringing fundamental change to an economy that hasn’t worked for the working people of this nation for too long.'\", \" 'Economists call it “increasing the productive capacity of our economy.”'\", \" 'The International Monetary Fund — (applause) — the International Monetary Fund is now estimating our economy will grow at a rate of more than 6 percent this year.  That will be the fastest pace of economic growth in this country in nearly four decades.'\", \" 'Invest in America. Educate Americans. Grow the workforce. Build the economy from the bottom up'\", \" 'The last administration enacted a $2 Trillion tax cut that overwhelmingly benefits the very wealthy and the biggest corporations and exploded the federal deficit.'\", \" 'Independent experts estimate the American Jobs Plan will add millions of jobs and trillions of dollars to economic growth in the years to come.  It is a — it is an eight-year program.  These are good-paying jobs that can’t be outsourced.'\", \" 'Look\", ' our economy roared back faster than most predicted', \" but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 1.00 because all relevant nodes are ranked higher than irrelevant nodes. The second node from the end focuses on a hopeful future outcome and is not relevant to the input. The last node suggests an alternative way to address inflation, which is not relevant to the factors contributing to inflation mentioned in the input. , error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: In President Biden’s 2022 State of the Union speech, given that supply chain disruptions and labor shortages have impacted production, what are the factors contributing to elevated inflation in the US?\n",
      "  - actual output: The pandemic and the war in Ukraine have disrupted supply chains and energy and food supplies, leading to higher prices for goods and services. Factory closures have slowed production and delivery times, resulting in increased costs. The shortage of semiconductor chips has also driven up prices for various products.  The rapid economic recovery following the pandemic has created labor shortages, making it difficult for businesses to keep up with production demands.  These factors have contributed to a rise in inflation, affecting the cost of essential items such as food, gas, housing, and consumer goods. \n",
      "\n",
      "  - expected output: Some of the factors contributing to increased inflation in the US, as shared in the 2022 address, are a direct result of the covid-19 pandemic. During the pandemic, businesses could not hire enough factory workers, and global supply chains were disrupted, leading to fewer goods being produced as quickly and higher prices for those goods.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['Inflation has been a global problem because of the pandemic that disrupted supply chains and Putin’s war that disrupted energy and food supplies.'\", \" 'When factories close\", ' it takes longer to make goods and get them from the warehouse to the store', \" and prices go up.'\", \" 'During the pandemic a shortage of semiconductor chips drove up prices for everything from cell phones to automobiles.'\", \" 'The pandemic also disrupted global supply chains.'\", \" 'Look\", ' our economy roared back faster than most predicted', \" but the pandemic meant that businesses had a hard time hiring enough workers to keep up production in their factories.'\", \" 'And so many families are living paycheck to paycheck\", ' struggling to keep up with the rising cost of food', ' gas', ' housing', \" and so much more.'\", \" 'Car prices went up. So did everything from refrigerators to cellphones.'\", \" 'Wages keep going up and inflation keeps coming down!'\", \" 'Inflation is robbing them of the gains they might otherwise feel.'\", \" 'One way to fight inflation is to drive down wages and make Americans poorer.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 0.00 because all nodes in the retrieval context are irrelevant. The first node, 'State of the Union Address given by President Biden on February 7, 2023', provides a date and year but no information about the content. The second node, 'State of the Union Address given by President Biden on March 1, 2022', is a different speech. The third node, 'State of the Union Address given by President Biden on March 7, 2024', is another different speech. The fourth node, 'State of the Union Address given by President Biden on April 29, 2021', is yet another different speech. The fifth node, 'And my report is this: the State of the Union is strong—because you, the American people, are strong.', is an inspirational statement about national strength but doesn't address specific policy details. The sixth node, 'Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong.', is another inspirational statement about national strength without specific policy details. The seventh node, 'My fellow Americans, we have to show not just that we’re back, but that we’re back to stay and that we aren’t going to go it alone.  (Applause.)  We’re going to do it by leading with our allies.  (Applause.)', mentions international cooperation but doesn't touch on specific priorities. The eighth node, 'So on this night, in our 245th year as a nation, I have come to report on the State of the Union.', provides a brief intro without specifics on policy. The ninth node, 'With light and hope, we summoned a new strength, new resolve to position us to win the competition of the 21st century, on our way to a union more perfect, more prosperous, and more just, as one people, one nation, and one America.', is a hopeful vision but doesn't delve into specific policies. The tenth node, 'America is rallying the world again to meet those challenges, from climate and global health, to food insecurity, to terrorism and territorial aggression.', mentions global challenges but doesn't explain how Biden addresses them., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Analyze President Biden's 2023 State of the Union speech, identifying its central theme and the key messages he conveyed regarding America's domestic and foreign policy priorities.\n",
      "  - actual output: President Biden's speech emphasizes the strength and resilience of the American people, both domestically and internationally. He highlights the nation's ability to overcome challenges and emerge stronger, positioning itself to win the competition of the 21st century. The speech underscores the importance of unity and collaboration, both within the nation and with global allies, to address critical issues like climate change, global health, food insecurity, terrorism, and territorial aggression. \n",
      "\n",
      "  - expected output: In his third State of the Union address, President Biden focuses on rebuilding America, highlighting the economic progress of the country over the last two years of his Presidency and asking Congress to continue supporting legislation to help the economy improve. He also reinforces America’s support to Ukraine in their war against Russia and welcomes competition instead of conflict with China.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['State of the Union Address given by President Biden on February 7\", \" 2023'\", \" 'State of the Union Address given by President Biden on March 1\", \" 2022'\", \" 'State of the Union Address given by President Biden on March 7\", \" 2024'\", \" 'State of the Union Address given by President Biden on April 29\", \" 2021'\", \" 'And my report is this: the State of the Union is strong—because you\", ' the American people', \" are strong.'\", \" 'Because the soul of this nation is strong\", ' because the backbone of this nation is strong', ' because the people of this nation are strong', \" the State of the Union is strong.'\", \" 'My fellow Americans\", ' we have to show not just that we’re back', \" but that we’re back to stay and that we aren’t going to go it alone.  (Applause.)  We’re going to do it by leading with our allies.  (Applause.)'\", \" 'So on this night\", ' in our 245th year as a nation', \" I have come to report on the State of the Union.'\", \" 'With light and hope\", ' we summoned a new strength', ' new resolve to position us to win the competition of the 21st century', ' on our way to a union more perfect', ' more prosperous', ' and more just', ' as one people', ' one nation', \" and one America.'\", \" 'America is rallying the world again to meet those challenges\", ' from climate and global health', ' to food insecurity', \" to terrorism and territorial aggression.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 1.00 because all relevant nodes in the retrieval context are ranked higher than the irrelevant nodes, which discuss topics like 'thank you for your service', 'duty to one another', 'opening doors of opportunity', 'keeping the torch of liberty lit', and 'constitutional right affirmed in Roe v. Wade' that are not relevant to the query. The first three nodes in the retrieval context are relevant to the query, specifically mentioning Justice Breyer's retirement, his role as an Army veteran and Constitutional scholar, and his service as a Justice on the Supreme Court. These nodes directly address the query, while the irrelevant nodes are tangential and do not provide information about Justice Breyer's contribution., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Who is the retiring Justice of the United States Supreme Court mentioned in the text, and what is his contribution to the country?\n",
      "  - actual output: The retiring Justice of the United States Supreme Court mentioned in the text is an Army veteran, Constitutional scholar, and has dedicated his life to serving the country. \n",
      "\n",
      "  - expected output: Justice Stephen Breyer is the retiring Supreme Court justice mentioned by President Biden in his speech. Justice Bryer is an Army veteran, a “Constitutional scholar”, and served as a Justice on the United States Supreme Court.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['Tonight\", ' I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran', ' Constitutional scholar', ' and retiring Justice of the United States Supreme Court. Justice Breyer', \" thank you for your service.'\", \" 'And I did that 4 days ago\", ' when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds', \" who will continue Justice Breyer’s legacy of excellence.'\", \" 'One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.'\", \" 'Mr. Chief Justice\", ' Associate Justices', \" and retired Justices of the Supreme Court.'\", \" 'Equal protection under the law; that’s the covenant we have with each other in America.'\", \" 'With a duty to one another to the American people to the Constitution.'\", \" 'In its decision to overturn Roe v. Wade the Supreme Court majority wrote\", \" “Women are not without –'\", \" 'Opening doors of opportunity\", \" guaranteeing some more fairness and justice — that’s the essence of America.  That’s democracy in action.'\", \" 'We can do all this while keeping lit the torch of liberty that has led generations of immigrants to this land—my forefathers and so many of yours.'\", \" 'The constitutional right affirmed in Roe v. Wade—standing precedent for half a century—is under attack as never before.']\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 0.5, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 0.50 because the relevant node is ranked higher than the irrelevant nodes, but some irrelevant nodes are ranked higher than the relevant node. The first node was irrelevant because 'This is an incomplete quote from the context, and while it mentions a nomination, it is not clear what position was being nominated for.' and the second node was relevant because 'This quote names 'Circuit Court of Appeals Judge Ketanji Brown Jackson' as the nominee, and mentions the Supreme Court, providing a direct answer.'. The rest of the nodes were irrelevant because 'is not relevant to answering who the nominee is.', 'This quote does not include any information about the nominee.', and 'This quote does not include any information about the nominee.'., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Considering the President's statement, who was nominated to fill the Supreme Court seat vacated by Justice Breyer?\n",
      "  - actual output: The President nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to fill the Supreme Court seat vacated by Justice Breyer. \n",
      "\n",
      "  - expected output: Circuit Court of Appeals Judge Ketanji Brown Jackson was nominated by President Biden to fill the Supreme Court seat vacated by Justice Breyer.\n",
      "  - context: []\n",
      "  - retrieval context: [\"['And I did that 4 days ago\", ' when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds', \" who will continue Justice Breyer’s legacy of excellence.'\", \" 'Tonight\", ' I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran', ' Constitutional scholar', ' and retiring Justice of the United States Supreme Court. Justice Breyer', \" thank you for your service.'\", \" 'One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.'\", \" 'Madam Speaker\", ' Madam Vice President', \" our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.'\", \" 'I promised to be the president for all Americans.'\", \" 'Madam Speaker\", ' Madam Vice President — (applause) — no President has ever said those words from this podium.  No President has ever said those words', \" and it’s about time.  (Applause.)'\", \" 'THE PRESIDENT:  — because I know it will get done.  (Applause.)'\", \" 'So what are we waiting for? Let’s get this done. And while you’re at it\", ' confirm my nominees to the Federal Reserve', \" which plays a critical role in fighting inflation.'\", \" 'Mr. Chief Justice\", ' Associate Justices', \" and retired Justices of the Supreme Court.'\", \" 'THE PRESIDENT:  Thank you.  (Applause.)  Thank you.  Thank you.  Good to be back.  And Mitch and Chuck will understand it’s good to be almost home\", ' down the hall.  Anyway', \" thank you all.']\"]\n"
     ]
    }
   ],
   "source": [
    "from deepeval.evaluate import TestResult\n",
    "from deepeval.evaluate import print_test_result\n",
    "\n",
    "# quick print for results after evaluation, as needed\n",
    "for i in [test_precision]:\n",
    "    for j in i:\n",
    "        if type(j) == TestResult:\n",
    "            print_test_result(j)\n",
    "        else:\n",
    "            print_test_result(j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0128ad6f-1eac-4d26-80b2-c3df8ace688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test RAGAS metrics\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics.ragas import RagasMetric\n",
    "from deepeval.metrics.ragas import RAGASAnswerRelevancyMetric\n",
    "from deepeval.metrics.ragas import RAGASFaithfulnessMetric\n",
    "from deepeval.metrics.ragas import RAGASContextualRecallMetric\n",
    "from deepeval.metrics.ragas import RAGASContextualPrecisionMetric\n",
    "\n",
    "ragasmetric = RagasMetric(model=custom_geminiflash, embeddings=custom_geminiembeddings)\n",
    "ragas_ar = RAGASAnswerRelevancyMetric(model=custom_geminiflash, embeddings=custom_geminiembeddings)\n",
    "ragas_f = RAGASFaithfulnessMetric(model=custom_geminiflash)\n",
    "ragas_cr = RAGASContextualRecallMetric(model=custom_geminiflash)\n",
    "ragas_cp = RAGASContextualPrecisionMetric(model=custom_geminiflash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f9317-d292-48a2-9e77-72ff0671c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of evaluation for all ragas metrics\n",
    "eval_ragas = evaluate(test_cases=evaldataset.test_cases, metrics=[ragasmetric], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3feaa-39c5-4a55-bf28-54ca1a5a5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of evaluation for each ragas metric individually\n",
    "eval_ragas_all = evaluate(test_cases=evaldataset.test_cases, metrics=[ragas_ar, ragas_f, ragas_cr, ragas_cp], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236fa57e-cfcc-4077-8f66-eb278c7fe741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "654da851-9125-4c34-b1e9-19add3a09b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a synthetic dataset of \"Goldens\" (aka 'input', 'context', 'source_file' columns -- not 'Retrieval_Context') with DeepEval\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "\n",
    "dataset = EvaluationDataset()\n",
    "synthesizer = Synthesizer(model=custom_geminiflash, embedder=custom_geminiembeddings)\n",
    "dataset.generate_goldens_from_docs(\n",
    "    synthesizer=synthesizer,\n",
    "    document_paths=['Speeches/titleedits/state_of_the_union_042921.txt', 'Speeches/titleedits/state_of_the_union_030122.txt', \n",
    "                    'Speeches/titleedits/state_of_the_union_020723.txt', 'Speeches/titleedits/state_of_the_union_030724.txt'],\n",
    "    max_goldens_per_document=3\n",
    ")\n",
    "\n",
    "dataset.save_as(file_type=\"csv\", directory=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11109d-56af-4ab5-b434-c19f8ff960bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66aaad79-e985-4661-a630-d4a49e584492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional ways to form json response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b496f0a-5e7b-4730-ad59-65ea763361d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e8eaa77-27ab-43eb-adce-5e89dfc96aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(test, User)\n",
    "assert test.resp == \"Tiki\"\n",
    "assert test.age == 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f606c-f1ed-4c0d-98a4-a2ba04bd0b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research2",
   "language": "python",
   "name": "research2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
