RAGAS:

- Version 0.1.15 - edited locally to resolve a compatibility issue with Gemini

- Overview (as of August 2024):
  - Metrics seem well designed and are the most common metrics across RAG evaluation libraries. No explanations are given for the scores.
  - Synthetic testset generation (question/context/ground truth answer) function worked pretty well; questions generated for the documents were the most "human-like" of all libraries tested.  
  - Documentation was significantly lacking when tested but has since been updated and may have been improved.
  - Internal rate limiting functions didn't work well, and RAGAS didn't work well (with limited testing) with external rate limiting libraries.

- Metrics:

  - Note: RAGAS documentation/functions were updated 10/10/24, so there are a few new variations of current metrics I haven't tried.
RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics

  - Faithfulness - Measures the factual consistency of the answer to the context based on the question.
  - Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.
  - Answer_relevancy - Measures how relevant the answer is to the question.
  - Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.
  - Faithfulness with HHEM - Similar to Faithfulness but uses a HuggingFace model (Vectara's HHEM 2.1 classifier) to detect hallucinations
    - https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html#faithfullness-with-hhem-2-1-model
    - https://huggingface.co/vectara/hallucination_evaluation_model

- Data generation: RAGAS allows you to generate a synthetic testset of any size with Question/Context/Ground Truth columns, using the docs/model in your RAG pipeline.
  - On a very small generation (n=10), results seemed pretty good; saves a lot of time vs creating this manually.
  - Generate synthetic test data : When generating a synthetic test dataset, the columns generated are 'question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', 'episode_done'.
  - You specify the distribution of the kinds of questions generated: 'simple', 'reasoning', or 'multi_context' questions, as well as the percentage of multi-context questions
  - Ground truth is supposed to be the 'human' level answer vs the RAG answer. 
  - We then have to generate the answer separately with our RAG, which then (obviously) generates new context used. My best guess is to use the context that was used to generate the answer for the metrics calculation. The original 'contexts' column was pretty good overall and typically could answer the question. Occasionally the generated context was not relevant/specific enough to answer the question.
  - Example: testset_answer_newcontext_flash_pro15.csv : Synthetically generated testset with RAGAS, with the addition of answers generated by Gemini 1.5 Flash along with the context used to create those answers. The 'context_gt' column contains the original contexts used to generate the 'ground_truth' column, all of which was created with the RAGAS TestsetGenerator.
    - Generator LLM: Gemini 1.5 Flash
    - Critic LLM: Gemini 1.5 Pro
    - Embeddings: models/text-embedding-004

- Required datasets:
  - Dataset for evaluation needs to be in Dataset format, not pandas dataframe (per issues raised on GitHub repo)
  - Dataset columns: ['question', 'answer', 'ground_truth', 'contexts', 'source_file']

- Custom/local model integration:
  - Works with LangchainLLM and LangchainEmbeddings : I used ChatGoogleGenerativeAI and GoogleGenerativeAIEmbeddings. Since Langchain integrates with VLLM, in theory RAGAS should work with VLLM or other local models with a Langchain wrapper.
  - There is also a BaseRagasLLM and BaseRagasEmbeddings class you can inherit from, but I did not test this.

- Performance results:
  - Below results were on my manually created dataset: manual_dataset_complete.csv, n=11, using all 4 speeches
  - RAGAS metrics simply output a score for the chosen metric without an additional explanation.
  - RAGAS metrics guide does not have example ranges to compare anything to, so below is my best guess.

  - Faithfulness - Measures the factual consistency of the answer to the context based on the question.
  - 0.9167 - 1.0000 indicates that the LLM is staying true to the facts provided in the context for answering the question.
  - Faithfulness with HHEM - This uses a huggingface model to help detect hallucination : https://huggingface.co/vectara/hallucination_evaluation_model
  - 0.6319: This doesn't really agree with the RAGAS faithfulness score... may need to dive in further another time.
  - Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.
  - At 0.4171 - 0.5979, suggests that the context isn't particularly relevant to the question. After reviewing the results, I think this should probably be  a little higher, but the score makes sense.
  - Answer_relevancy - Measures how relevant the answer is to the question.
  - 0.6509 - 0.6533 seems moderately low, just going off of a manual review of the answer. This may be due to questions asked that do not have an answer in the context.
  - Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.
  - 0.8 indicates that the llm context is decently good and can typically answer the question or most of it. I concur with this after manual review of the dataset.

- Other notes:
  - The RAGAS internal RunConfig settings do a decent job at limiting the 429 resource exhausted warnings when max_workers=1. Ratelimit and backoff Python libraries didn't seem to affect performance, but I didn't spend a lot of time with that.
  - There's a flag you have to set initially for RAGAS not to track your usage.


DeepEval:

- Overview:

- Metrics:

- Data generation:

- Required datasets:

- Performance results:

- Other notes:

ARES:

- Overview:

- Metrics: 

- Context Relevance: Determines if the retrieved information is pertinent to the query.
- Answer Faithfulness: Checks if the response generated by the language model is properly grounded in the retrieved context and does not include hallucinated or extraneous information.
- Answer Relevance: Evaluates whether the generated response is relevant to the query, addressing all aspects of the question appropriately.

- Data generation:

- Required datasets:

- Performance results:

- Other notes:
