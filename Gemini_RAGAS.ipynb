{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd552471-bc05-4749-b207-9be5e7e2d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook uses the RAGAS evaluation library to evaluate several metrics for a RAG pipeline\n",
    "# I use the Google Gemini API (free tier, local API key), but RAGAS is compatible with several LLMs\n",
    "\n",
    "# Google Gemini: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# RAGAS: https://docs.ragas.io/en/stable/\n",
    "# I used RAGAS v0.1.15 (mine was dev version from Github, but 0.1.15 from PyPI should work fine). \n",
    "\n",
    "# Note: I had to edit underlying RAGAS library (cloned locally, edited files, then pip -e installed locally) for this issue re: temperature with Gemini:\n",
    "# https://github.com/explodinggradients/ragas/pull/657/files\n",
    "# https://github.com/explodinggradients/ragas/issues/678\n",
    "# Edits simply remove the temperature variable from the relevant source files; see notes.txt for more specific info\n",
    "\n",
    "# RAGAS had a large redesign from version 0.1 to 0.2 : https://docs.ragas.io/en/stable/howtos/migrations/migrate_from_v01_to_v02/\n",
    "# - Note that on RAGAS version 0.2.6, the temperature edits did not make RAGAS compatible with Gemini.\n",
    "# - There are open Github issues about Gemini's compatibility for RAGAS version 2+; more time and edits would be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44922bc8-8feb-44cd-9541-84618200beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics\n",
    "\n",
    "# Faithfulness - Measures the factual consistency of the answer to the context based on the question.\n",
    "# Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.\n",
    "# Answer_relevancy - Measures how relevant the answer is to the question.\n",
    "# Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.\n",
    "\n",
    "# Faithfulness with HHEM - Similar to Faithfulness but uses a HuggingFace model (Vectara's HHEM 2.1 classifier) to detect hallucinations\n",
    "# https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html#faithfullness-with-hhem-2-1-model\n",
    "# https://huggingface.co/vectara/hallucination_evaluation_model\n",
    "\n",
    "# RAGAS has other metrics as well : https://docs.ragas.io/en/latest/concepts/metrics/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f13f959-5e0b-4e76-b116-da5da6a04ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set do not track variable for RAGAS\n",
    "# More info: https://github.com/explodinggradients/ragas/issues/49\n",
    "import os\n",
    "os.environ[\"RAGAS_DO_NOT_TRACK\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30fd81d-ce93-4e03-bbec-862ece105b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import textwrap\n",
    "import ast\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "# Replace these two Google Gemini imports with imports for your LLM\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "import ragas\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56373c08-f6b4-452a-8eba-63a20455d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11b65e5d-d679-49c4-ade6-2719fecb55d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check the RAGAS do not track setting\n",
    "ragas._analytics.do_not_track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca08633-0877-4773-93ab-d4a78b8f5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up local API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e22c5-2176-4d06-a69e-d1aa3eb23ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179c80c7-7695-4fa7-9b85-c48d949374d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish RAG pipeline with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5248aab9-6fcd-46bf-b260-773c23b8845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Faiss vector store for RAG\n",
    "# # If you already have an index created, skip a few coding cells to the LLM / embeddings setup\n",
    "\n",
    "# # Example of creating a small vector store\n",
    "# # Using 4 State of the Union speeches, all text from whitehouse.gov briefing room speeches posted online, edited to include a title with the date of the speech\n",
    "# # Example from 2024:\n",
    "# # https://www.whitehouse.gov/briefing-room/speeches-remarks/2024/03/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery-2/\n",
    "\n",
    "# # load and parse files\n",
    "# sotu = []\n",
    "# newfiles = [\"./Speeches/titleedits/state_of_the_union_042921.txt\", \"./Speeches/titleedits/state_of_the_union_030122.txt\", \"./Speeches/titleedits/state_of_the_union_020723.txt\", \"./Speeches/titleedits/state_of_the_union_030724.txt\"]\n",
    "# for i in newfiles:\n",
    "#     with open(i) as file:\n",
    "#         for line in file:\n",
    "#             nl = line.rstrip()\n",
    "#             if nl != '':\n",
    "#                 sotu.append(nl)\n",
    "\n",
    "# # Convert into Document format for Faiss\n",
    "# documents = [Document(text=line) for line in sotu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb9bf47-2a72-41a7-af8e-ae663bf95e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='235d1f3b-a216-412c-8459-51d27c73c8d0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='May God protect our troops.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Example of a loaded Document line\n",
    "# documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed3997-3a20-4a99-a9c1-7cc6d6a217e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the Faiss index\n",
    "# d = 768 # dimensions of the input vector of the embedding model that we're going to use; in this case, the google embedding model\n",
    "# faiss_index = faiss.IndexFlatL2(d)\n",
    "# print(faiss_index.is_trained) # double check that the training worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490599ac-6499-4174-999d-4ddc0609fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the llm, embeddings, and Settings for Faiss \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\") # Replace with your LLM\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # Replace with your embeddings model\n",
    "Settings.embed_model = doc_embeddings # used for LlamaIndex FaissVectorStore\n",
    "Settings.llm = llm # used for LlamaIndex FaissVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24d9337-5832-47d6-90df-6e73b3de8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3171f73-723f-40f5-91d3-14e0f26173e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment for when you need to re-embed and vectorize documents\n",
    "\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, show_progress=True\n",
    "# )\n",
    "\n",
    "# # Save index to disk\n",
    "# index.storage_context.persist()\n",
    "\n",
    "# # Save/remember index id for loading next time\n",
    "# index.index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c8a330-0b12-434d-bc1c-4d8a7a1e32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading llama_index.vector_stores.faiss.base from ./storage/default__vector_store.json.\n",
      "Loading llama_index.vector_stores.faiss.base from ./storage/default__vector_store.json.\n",
      "INFO:llama_index.core.indices.loading:Loading indices with ids: ['95634851-570e-454e-983f-6634eeb72aee']\n",
      "Loading indices with ids: ['95634851-570e-454e-983f-6634eeb72aee']\n"
     ]
    }
   ],
   "source": [
    "# After you have a saved index, load that index for RAG answer generation:\n",
    "\n",
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "# My local index id '3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5' uses the 4 speeches including a title that includes the date it was given\n",
    "# My local index id '95634851-570e-454e-983f-6634eeb72aee' contains 3200 documents from the rag_mini_wikipedia dataset\n",
    "index = load_index_from_storage(storage_context=storage_context, index_id='95634851-570e-454e-983f-6634eeb72aee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba1e938-d2bc-4ef3-a9d2-f2bc01a41e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional- if you'd like to query your index\n",
    "# # Set up query and chat engines with the index\n",
    "# query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "# chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026088c-515b-4933-9ec9-d3a616870533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query and response with Gemini and query_engine\n",
    "# query = \"What has the President done related to healthcare?\"\n",
    "# response = query_engine.query(query) \n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0178b0-ee47-4443-b8fa-c5cb3b4de228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get ranked scores for top k RAG source nodes\n",
    "# for node in response.source_nodes:\n",
    "#     print(f\"{node.get_score()} -> {node.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506d603-b9cb-43f1-b90b-99645cad7060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of using the chat engine with our index\n",
    "# query = \"You are an expert speech analyst and specialize in analyzing Presidential State of the Union speeches. Could you please analyze the speeches and generate 2 questions and answers from each speech, providing the document filename of each speech that relates to each question?\"\n",
    "# response = chat_engine.chat(query) \n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35114936-2d0c-4b06-99d7-16b4699d9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View chat history\n",
    "# chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf198ee7-45f6-4573-8bcb-60b25e2b7af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1f8b91f-afc2-4916-a65b-4a4bcb088d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for RAGAS evaluation library to work with Gemini and our local RAG setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af3f8028-582d-49b5-9939-45eb0d810501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of generating synthetic dataset with RAGAS\n",
    "\n",
    "# In a synthetic dataset, columns generated are 'question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', and 'episode_done'\n",
    "# Ground truth is supposed to be the 'human' level answer vs the RAG answer\n",
    "\n",
    "# Notes: \n",
    "# - We have to generate the answer separately with our RAG, which then generates new context used.\n",
    "# - I use the context that was used to generate the answer for the metrics calculation, while still saving the old contexts column.\n",
    "# - The best thing to do would be to generate the answer when creating the synthetic test dataset, but this is not available in RAGAS.\n",
    "# - From a Github issue: Since you use the same LLM to generate your synthetic dataset ground_truth and your answer, \n",
    "# - it is possible the results of the RAG evaluation might be biased. This has not been studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2002e239-4a5d-491d-b02b-611ae0f6afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents for use in generating synthetic dataset with RAGAS\n",
    "loader = DirectoryLoader(\"./Speeches/titleedits\") # Loads all documents in the directory; there are parameters for ignoring or matching certain files\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83aac065-04c4-46fc-baed-cdb9bf2d9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add 'filename' metadata for RAGAS to process documents\n",
    "for document in documents:\n",
    "    document.metadata['filename'] = document.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2b846e0-b319-4e00-ae3a-8796cb4be771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic testset generator with Gemini models\n",
    "generator_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", timeout=240) # Other notable parameters: temperature=0.7, transport=\"rest\"\n",
    "critic_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", timeout=240) \n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", request_options={\"timeout\": 240}) \n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd22ec1-0bb5-4e12-8401-3866b2900d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the timeout settings with RAGAS's RunConfig class\n",
    "\n",
    "# Note: For Gemini, the RAGAS internal RunConfig settings do a decent job at limiting the 429 resource exhausted warnings\n",
    "# (max_workers=1 still can send more requests to Gemini than the 15 requests per minute it allows)\n",
    "# Still very difficult to have the testset generation run successfully with Gemini free tier\n",
    "# I also tried the ratelimit and backoff libraries in Python, but I still got so many 429 warnings that the generation failed\n",
    "# Sometimes even the 1 max worker will not finish, but it will finish occasionally\n",
    "\n",
    "run_config = RunConfig(timeout=240, max_retries=20, max_wait=240, max_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea76249e-a95d-421f-bc02-a290bbc94e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the synthetic dataset/testset\n",
    "def generate_testset_rate(docs):\n",
    "    \"\"\"\n",
    "    Calls the LLM and embeddings model to generate the synthetic dataset with rate limit run_config\n",
    "    Can change the distribution of simple, reasoning, and multi-context questions generated\n",
    "    \"\"\"\n",
    "    testset = generator.generate_with_langchain_docs(documents=documents, # Uses LangChain's DirectoryLoader source documents\n",
    "                                                     test_size=50, # Number of test samples to generate\n",
    "                                                     distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}, # Different types of queries to generate\n",
    "                                                     is_async=False,\n",
    "                                                     raise_exceptions=True, \n",
    "                                                     run_config=run_config)\n",
    "    return testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6006a-7672-4bc2-80ee-af126e85d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = generate_testset_rate(documents=documents, run_config=run_config) # ToDo: Test run_config here, may not have passed it before\n",
    "testset_pd = testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87fd9f5e-145f-4f79-a430-07c32e5b7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save generated testset to csv \n",
    "# testset_pd.to_csv('datasets/testset_flash_pro15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e411a9d-7aa3-4783-aa48-d6467a2b25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now generate answers for the testset, as answers are not automatically generated at creation\n",
    "\n",
    "# testset_pd = pd.read_csv(\"datasets/testset_flash_pro15.csv\", index_col = None)\n",
    "\n",
    "# Note: When saving, the 'contexts' column is saved as a string but needs to be a list\n",
    "# If you are importing testset_pd from a csv file, use the below code to change the column to a list\n",
    "\n",
    "# testset_pd['contexts'] = testset_pd['contexts'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfd0ca-b9cd-4003-b6e3-addfc065a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers using our query engine & Faiss vector database\n",
    "# Alternatively can use the chat_engine if memory between queries is needed (i.e., queries reference previous queries)\n",
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "answers = [query_engine.query(q) for q in testset_pd['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33893af4-a200-4df9-9843-2e21a27d6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse out new 'answer' and 'contexts' columns\n",
    "answers_new = []\n",
    "context_new = []\n",
    "for i in answers:\n",
    "    answers_new.append(i.response)\n",
    "    context_new.append([c.node.get_content() for c in i.source_nodes])\n",
    "\n",
    "testset_pd = testset_pd.rename(columns={\"contexts\":\"contexts_gt\"}) # Keeping old contexts that were used for testset/query generation (gt = ground truth)\n",
    "testset_pd['contexts'] = context_new\n",
    "testset_pd['answer'] = answers_new\n",
    "\n",
    "# Save complete synthetically created dataset/testset\n",
    "# testset_pd.to_csv('datasets/ragas_full_testset_flash_pro15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fbfc8109-0336-43b4-bf49-1ed6142b5cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe228932-95ed-4d8c-a556-0bfac692fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a dataset with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7866f0b2-d32b-45af-8a91-7d77fbb0a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset for evaluation\n",
    "testset_pd = pd.read_csv(\"datasets/rag_mini_wikipedia_complete_chat.csv\", index_col = None) \n",
    "\n",
    "# RAGAS expects the following columns (rename in dataset as needed) : \"question\", \"answer\", \"ground_truth\", \"source_file\", \"contexts\"\n",
    "#testset_pd = testset_pd.rename(columns={\"Query\": \"question\", \"Answer\": \"answer\", \"Expected_Output\": \"ground_truth\", \"Contexts\": \"orig_contexts\", \"Source_File\":\"source_file\", \"Document\": \"contexts\"})\n",
    "\n",
    "# Note: When saving a synthetic dataset, the 'contexts' column is saved as a string but needs to be a list for evaluation\n",
    "# If you are importing testset_pd from a csv file that is a synthetic dataset, use the below code to change the column to a list\n",
    "# This may apply to other datasets as well\n",
    "testset_pd['contexts'] = testset_pd['contexts'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e25f6c-47d0-4846-9856-b04be90b122e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d735bf4-b628-4b08-b504-61c67b137d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At least for RAGAS v0.1.15, we need to convert the pandas testset into Dataset format for the evaluate function to work\n",
    "# Note: I am also dropping the original contexts column here, as well as any extra columns\n",
    "testset_ds = Dataset.from_pandas(testset_pd.drop(\"id\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104fced3-164c-44df-a0b2-55e953d33d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'ground_truth', 'contexts', 'answer'],\n",
       "    num_rows: 918\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243bd2c-df3a-462f-b051-063368a1a088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a39ae55f-204a-4b42-8811-710bd26502cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I'm using the normal LLM, not the RAG context-loaded query engine\n",
    "# There is code at the bottom of the notebook for using the query engine, but it appears to just use the query engine to develop\n",
    "# new answers and contexts and then to use the non-RAG LLM for the metrics evaluation\n",
    "# That code also appears to be broken from RAGAS right now, so I was forced to use the regular Gemini LLM anyway\n",
    "\n",
    "# Note: The RAGAS evaluate function (below) may re-run the query and give new answers and contexts\n",
    "# See this issue: https://github.com/explodinggradients/ragas/issues/1211\n",
    "# In testing, the results still output the same answers and contexts as I started with, so I'm not concerned by this\n",
    "\n",
    "ragas_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", timeout=600) \n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47977c82-14b7-4291-9b53-3c9ce0510487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the timeout settings\n",
    "run_config = RunConfig(timeout=300, max_wait=3000, max_workers=1, max_retries=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d845d9f-6d08-4fde-8ded-361b7d12e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two coding options for running evaluate:\n",
    "# 1) Bulk run with the evaluate function, as intended. \n",
    "# Unfortunately rate-limiting does not work well with this, \n",
    "# and 2/3 of my 800 example dataset received NaN results because of rate limiting issues.\n",
    "# Ex: Evaluating 1 example for 1 metric resulted in 10 API calls.\n",
    "\n",
    "# 2) Run the evaluation in small batches\n",
    "# This allowed me to finish the evaluation of the entire dataset without rate limiting errors.\n",
    "\n",
    "# Bulk evaluation of the dataset\n",
    "evalresult = evaluate(\n",
    "    metrics = [\n",
    "        context_precision\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall\n",
    "    ],\n",
    "    dataset = testset_ds,\n",
    "    llm = ragas_llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config\n",
    ")\n",
    "\n",
    "# Optional parameter: in_ci: bool, Whether the evaluation is running in CI or not. \n",
    "# If set to True then some metrics will be run to increase the reproducability of the evaluations. \n",
    "# This will increase the runtime and cost of evaluations. Default is False.\n",
    "# In practice, setting in_ci = True resulted in a lot of timeouts / no score calculated / NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5ecd8cf-8642-4ea0-ac1a-c6bcbb05a621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.5979, 'faithfulness': 1.0000, 'answer_relevancy': 0.6533, 'context_recall': 0.8000}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example result:\n",
    "# {'context_precision': 0.4676, 'faithfulness': 1.0000, 'answer_relevancy': 0.6515, 'context_recall': 0.8000}\n",
    "evalresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "494b292a-c1e2-4da7-a737-9adb5267fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalresult.to_pandas() # Returns the dataset with scores for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e33577-7183-4d31-90d6-72134e9cf5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for smaller batches\n",
    "testset_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17709e07-b809-4342-be28-33e65503ad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9adde3a6a3343daa6466e0957d04592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "909\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd74d6f23134addbd6500d406692228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "910\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463c3fd1a37d4ef386c6c7265025e3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3a968b91fd451283f99f8b8e24856c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb09b311d3a84bc2971df1e9607ad1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c9c734bab74ed2b30dee651b0c5634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72bbf47eef0430299d9154dd3a006e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a925d475b07d410a9f6ff03c14f4a912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303de57935114b17a8435bb46a31654b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aaf176f89ed403fb3bc6ee9b71d80a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Iterate through dataset for smaller batches to be evaluated\n",
    "for i in range(908,918):  # Choose the batch size in the range selection # last 297 didn't execute # If I missed one it was in 390-393 #895 & 896 didn't execute; didn't save\n",
    "    tempdataset = testset_ds.select(range(i, i+1)) # Match the batch size in the dataset selection\n",
    "    print(i)\n",
    "    evalresult = evaluate(\n",
    "        metrics = [\n",
    "            context_precision\n",
    "            #faithfulness,\n",
    "            #answer_relevancy,\n",
    "            #context_recall\n",
    "        ],\n",
    "        dataset = tempdataset,\n",
    "        llm = ragas_llm,\n",
    "        embeddings=embeddings,\n",
    "        run_config=run_config\n",
    "    )\n",
    "    testset_results = pd.concat([testset_results, evalresult.to_pandas()])\n",
    "    time.sleep(60) # RAGAS generates ~10 API calls per example. Gemini free-tier limit is 15RPM, so we need to wait a minute between calls for the limit to reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c32d0891-ed1f-4d26-a3ec-d8cfbbd6f57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question             0\n",
       "ground_truth         0\n",
       "contexts             0\n",
       "answer               0\n",
       "context_precision    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_results.isnull().sum()\n",
    "#testset_results #testset_results.iloc[0:147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4c6624e-ce8d-4888-8483-d1bc68ea403c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What caused Wilson to ask Congress to declare ...</td>\n",
       "      <td>German began unrestricted submarine warfare</td>\n",
       "      <td>[When Germany resumed unrestricted submarine w...</td>\n",
       "      <td>Several factors led Woodrow Wilson to ask Cong...</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was more damaging than moving students in...</td>\n",
       "      <td>His confrontation with Andrew Fleming West</td>\n",
       "      <td>[The shortage of places in post-secondary educ...</td>\n",
       "      <td>In the context of Woodrow Wilson's presidency ...</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Wilson a member of the Phi Kappa Psi frate...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[* Wilson was a member of the Phi Kappa Psi fr...</td>\n",
       "      <td>Yes, Woodrow Wilson was a member of the Phi Ka...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Wilson an automobile enthusiast?</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Wilson's Pierce Arrow, which resides in his h...</td>\n",
       "      <td>While not an *avid* enthusiast in the sense of...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Wilson's father own slaves?</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Thomas Woodrow Wilson was born in Staunton, V...</td>\n",
       "      <td>Yes, Woodrow Wilson's father, Joseph Ruggles W...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where is Wilson buried?</td>\n",
       "      <td>He was buried in Washington National Cathedral</td>\n",
       "      <td>[* Wilson is the only U.S. President buried in...</td>\n",
       "      <td>Woodrow Wilson is buried in Washington Nationa...</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where did Wilson attend law school?</td>\n",
       "      <td>Wilson attended law school at University of Vi...</td>\n",
       "      <td>[* Wilson was a member of the Phi Kappa Psi fr...</td>\n",
       "      <td>Woodrow Wilson attended the University of Virg...</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where was Woodrow Wilson born?</td>\n",
       "      <td>Woodrow Wilson was born in Staunton, Virginia</td>\n",
       "      <td>[Thomas Woodrow Wilson (December 28, 1856âFe...</td>\n",
       "      <td>Woodrow Wilson was born in Staunton, Virginia.\\n</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Wilson support desegregation?</td>\n",
       "      <td>no</td>\n",
       "      <td>[Wilson allowed many of his cabinet officials ...</td>\n",
       "      <td>No, Woodrow Wilson did not support desegregati...</td>\n",
       "      <td>0.634921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Wilson support the committee system?</td>\n",
       "      <td>no</td>\n",
       "      <td>[In addition to their undemocratic nature, Wil...</td>\n",
       "      <td>No, Woodrow Wilson was a strong critic of the ...</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Wilson have any siblings?</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Wilson's first wife Ellen died on August 6, 1...</td>\n",
       "      <td>Yes, Woodrow Wilson had three siblings.\\n</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was Scots-Irish and Scottish?</td>\n",
       "      <td>His ancestry</td>\n",
       "      <td>[There is an organized propaganda against the ...</td>\n",
       "      <td>That describes Woodrow Wilson's ancestry.  His...</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What defended slavery, owned slaves and S08_se...</td>\n",
       "      <td>His father</td>\n",
       "      <td>[* American School, Lincoln's economic views.,...</td>\n",
       "      <td>That describes Joseph Ruggles Wilson, Woodrow ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who did Wilson win in 1917?</td>\n",
       "      <td>Irish Americans</td>\n",
       "      <td>[* Wilson was also the first Democrat elected ...</td>\n",
       "      <td>In 1917, Wilson didn't win an election; he led...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Wilson awarded the 1919 Nobel Peace Prize?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[In the late stages of the war he took persona...</td>\n",
       "      <td>Yes, Woodrow Wilson was awarded the 1919 Nobel...</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Wilson a remarkably effective writer and t...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Wilson was a remarkably effective writer and ...</td>\n",
       "      <td>Yes, Woodrow Wilson is widely considered to ha...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What lived in Columbia?</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>[A melanistic leopard, or \"black panther\", Sca...</td>\n",
       "      <td>That's too broad a question.  Many things live...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Wilson president of the American Political...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[* Wilson was president of the American Politi...</td>\n",
       "      <td>Yes, Woodrow Wilson was president of the Ameri...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did he not cast his ballot for John M. Palmer ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[By the time Wilson finished Congressional Gov...</td>\n",
       "      <td>The provided text states that Wilson *did* cas...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did Wilson not spend 1914 through the beginnin...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Wilson spent 1914 through the beginning of 19...</td>\n",
       "      <td>That statement is correct.  Wilson's administr...</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Wilson , a staunch opponent of antisemitis...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Wilson, a staunch opponent of antisemitism , ...</td>\n",
       "      <td>Yes, according to the provided text, Wilson wa...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What happened in 1917?</td>\n",
       "      <td>raised billions through Liberty loans, imposed...</td>\n",
       "      <td>[Canadian soldiers would win the Battle of Vim...</td>\n",
       "      <td>Several significant events happened in 1917:\\n...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What caused Wilson to ask Congress to declare ...   \n",
       "0  What was more damaging than moving students in...   \n",
       "0  Was Wilson a member of the Phi Kappa Psi frate...   \n",
       "0               Was Wilson an automobile enthusiast?   \n",
       "0                    Did Wilson's father own slaves?   \n",
       "0                            Where is Wilson buried?   \n",
       "0                Where did Wilson attend law school?   \n",
       "0                     Where was Woodrow Wilson born?   \n",
       "0                  Did Wilson support desegregation?   \n",
       "0           Did Wilson support the committee system?   \n",
       "0                      Did Wilson have any siblings?   \n",
       "0                 What was Scots-Irish and Scottish?   \n",
       "0  What defended slavery, owned slaves and S08_se...   \n",
       "0                        Who did Wilson win in 1917?   \n",
       "0     Was Wilson awarded the 1919 Nobel Peace Prize?   \n",
       "0  Was Wilson a remarkably effective writer and t...   \n",
       "0                            What lived in Columbia?   \n",
       "0  Was Wilson president of the American Political...   \n",
       "0  Did he not cast his ballot for John M. Palmer ...   \n",
       "0  Did Wilson not spend 1914 through the beginnin...   \n",
       "0  Was Wilson , a staunch opponent of antisemitis...   \n",
       "0                             What happened in 1917?   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0        German began unrestricted submarine warfare   \n",
       "0         His confrontation with Andrew Fleming West   \n",
       "0                                                yes   \n",
       "0                                                yes   \n",
       "0                                                yes   \n",
       "0     He was buried in Washington National Cathedral   \n",
       "0  Wilson attended law school at University of Vi...   \n",
       "0      Woodrow Wilson was born in Staunton, Virginia   \n",
       "0                                                 no   \n",
       "0                                                 no   \n",
       "0                                                yes   \n",
       "0                                       His ancestry   \n",
       "0                                         His father   \n",
       "0                                    Irish Americans   \n",
       "0                                                Yes   \n",
       "0                                                Yes   \n",
       "0                                             Wilson   \n",
       "0                                                Yes   \n",
       "0                                                Yes   \n",
       "0                                                Yes   \n",
       "0                                                Yes   \n",
       "0  raised billions through Liberty loans, imposed...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [When Germany resumed unrestricted submarine w...   \n",
       "0  [The shortage of places in post-secondary educ...   \n",
       "0  [* Wilson was a member of the Phi Kappa Psi fr...   \n",
       "0  [Wilson's Pierce Arrow, which resides in his h...   \n",
       "0  [Thomas Woodrow Wilson was born in Staunton, V...   \n",
       "0  [* Wilson is the only U.S. President buried in...   \n",
       "0  [* Wilson was a member of the Phi Kappa Psi fr...   \n",
       "0  [Thomas Woodrow Wilson (December 28, 1856âFe...   \n",
       "0  [Wilson allowed many of his cabinet officials ...   \n",
       "0  [In addition to their undemocratic nature, Wil...   \n",
       "0  [Wilson's first wife Ellen died on August 6, 1...   \n",
       "0  [There is an organized propaganda against the ...   \n",
       "0  [* American School, Lincoln's economic views.,...   \n",
       "0  [* Wilson was also the first Democrat elected ...   \n",
       "0  [In the late stages of the war he took persona...   \n",
       "0  [Wilson was a remarkably effective writer and ...   \n",
       "0  [A melanistic leopard, or \"black panther\", Sca...   \n",
       "0  [* Wilson was president of the American Politi...   \n",
       "0  [By the time Wilson finished Congressional Gov...   \n",
       "0  [Wilson spent 1914 through the beginning of 19...   \n",
       "0  [Wilson, a staunch opponent of antisemitism , ...   \n",
       "0  [Canadian soldiers would win the Battle of Vim...   \n",
       "\n",
       "                                              answer  context_precision  \n",
       "0  Several factors led Woodrow Wilson to ask Cong...           0.791667  \n",
       "0  In the context of Woodrow Wilson's presidency ...           0.200000  \n",
       "0  Yes, Woodrow Wilson was a member of the Phi Ka...           1.000000  \n",
       "0  While not an *avid* enthusiast in the sense of...           0.700000  \n",
       "0  Yes, Woodrow Wilson's father, Joseph Ruggles W...           1.000000  \n",
       "0  Woodrow Wilson is buried in Washington Nationa...           0.416667  \n",
       "0  Woodrow Wilson attended the University of Virg...           0.416667  \n",
       "0   Woodrow Wilson was born in Staunton, Virginia.\\n           0.416667  \n",
       "0  No, Woodrow Wilson did not support desegregati...           0.634921  \n",
       "0  No, Woodrow Wilson was a strong critic of the ...           0.611111  \n",
       "0          Yes, Woodrow Wilson had three siblings.\\n           0.166667  \n",
       "0  That describes Woodrow Wilson's ancestry.  His...           0.200000  \n",
       "0  That describes Joseph Ruggles Wilson, Woodrow ...           0.000000  \n",
       "0  In 1917, Wilson didn't win an election; he led...           0.000000  \n",
       "0  Yes, Woodrow Wilson was awarded the 1919 Nobel...           0.333333  \n",
       "0  Yes, Woodrow Wilson is widely considered to ha...           1.000000  \n",
       "0  That's too broad a question.  Many things live...           0.000000  \n",
       "0  Yes, Woodrow Wilson was president of the Ameri...           1.000000  \n",
       "0  The provided text states that Wilson *did* cas...           0.700000  \n",
       "0  That statement is correct.  Wilson's administr...           0.866667  \n",
       "0  Yes, according to the provided text, Wilson wa...           1.000000  \n",
       "0  Several significant events happened in 1917:\\n...           0.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70246798-5d45-4f74-99cc-ec5eeab6da51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What happened in 1917?']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_ds.select([917])['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cf1eed5-c9d2-4c3b-9b21-5ac79affdc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend checking contexts column to be sure context nodes are separated from each other after saving\n",
    "testset_results.to_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_896_918.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a2df6e7-09b1-4f12-a4b3-fbe47fcda817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_150_297.csv\", index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a25cf3dc-ea64-4a15-88d9-40ab6d6b4811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question             0\n",
       "ground_truth         0\n",
       "contexts             0\n",
       "answer               0\n",
       "context_precision    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0a5aa-f1b4-4945-957b-9e30a0dd43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: received warning for the answer where there was no response from the llm, definitely reduced faithfulness score\n",
    "\n",
    "# Results:\n",
    "# Using contexts generated when produced answers from LLM:\n",
    "# new testset_answer_newcontext_flash_pro15.csv result, with contexts_gt (aka contexts generated with ground truth generation) column removed\n",
    "# {'context_precision': 0.4171, 'faithfulness': 0.9167, 'answer_relevancy': 0.6509, 'context_recall': 0.8000}\n",
    "# reran\n",
    "# {'context_precision': 0.4676, 'faithfulness': 1.0000, 'answer_relevancy': 0.6515, 'context_recall': 0.8000}\n",
    "# reran\n",
    "# {'context_precision': 0.5979, 'faithfulness': 1.0000, 'answer_relevancy': 0.6533, 'context_recall': 0.8000}\n",
    "\n",
    "# Compared to using contexts generated for ground truth (probably not correct):\n",
    "# new testset_answer_newcontext_flash_pro15.csv result, using old contexts\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.7392, 'answer_relevancy': 0.6041, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.8500, 'faithfulness': 0.7123, 'answer_relevancy': 0.5934, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.6556, 'answer_relevancy': 0.5638, 'context_recall': 1.0000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a027af-7dff-483f-97ff-9072e71d7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results on metrics:\n",
    "\n",
    "# RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics\n",
    "# I don't have example ranges to compare anything to, so below is my best guess.\n",
    "\n",
    "# Faithfulness - Measures the factual consistency of the answer to the context based on the question.\n",
    "# 0.9167 - 1.0000 indicates that the LLM is staying true to the facts provided in the context for answering the question.\n",
    "# There is another Faithfulness metric: from ragas.metrics import FaithulnesswithHHEM\n",
    "# This uses a huggingface model to help detect hallucination : https://huggingface.co/vectara/hallucination_evaluation_model\n",
    "# See below for code : {'faithfulness_with_hhem': 0.6319} \n",
    "# This doesn't really agree with the RAGAS faithfulness score... may need to dive in further another time.\n",
    "# Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.\n",
    "# At 0.4171 - 0.5979, suggests that the context isn't particularly relevant to the question.\n",
    "# Answer_relevancy - Measures how relevant the answer is to the question.\n",
    "# 0.6509 - 0.6533 seems moderately low, just going off of the number.\n",
    "# Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.\n",
    "# 0.8 indicates that the llm context is decently good and can typically answer the question or most of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "04f4d426-e499-475e-8459-a497368b8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run, just compare to using contexts_gt column instead of the newer context generated with the answer\n",
    "testset_ds_oldcontext = Dataset.from_pandas(testset_pd.drop(\"contexts\", axis=1).rename(columns={'contexts_old':'contexts'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7b251-9833-4931-baf2-c575bf900721",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalresult_old2 = evaluate(\n",
    "    testset_ds_oldcontext,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm = ragas_llm,\n",
    "    embeddings=embeddings, \n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7170d4-ca68-46b1-a962-3c71093374c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new testset_answer_newcontext_flash_pro15.csv result, using old contexts\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.7392, 'answer_relevancy': 0.6041, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.8500, 'faithfulness': 0.7123, 'answer_relevancy': 0.5934, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.6556, 'answer_relevancy': 0.5638, 'context_recall': 1.0000}\n",
    "evalresult_old2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79fac0-ea9d-473f-b33c-02ee96df9559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a0ae2-f6ae-47e4-bda8-1c66c0104ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS also has an additional Faithfulness with HHEM metric (yes- it is misspelled in their documentation) \n",
    "# that uses a HuggingFace model to detect hallucinations\n",
    "# Note: There's a message on HuggingFace about the token indices sequence length error being normal and an artifact; thus, ignoring the below error\n",
    "# https://huggingface.co/vectara/hallucination_evaluation_model\n",
    "from ragas.metrics import FaithulnesswithHHEM\n",
    "faithfulness_with_hhem = FaithulnesswithHHEM()\n",
    "result_faithfulness_hhem = evaluate(\n",
    "    testset_ds,\n",
    "    metrics=[faithfulness_with_hhem],\n",
    "    llm = ragas_llm,\n",
    "    embeddings=embeddings, \n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7f158bea-0167-43ab-b4fb-0b5c0bc145c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness_with_hhem': 0.6319}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with context from answer generation:\n",
    "# {'faithfulness_with_hhem': 0.6319}\n",
    "# testing: with context from ground truth/synthetic testset generation\n",
    "# {'faithfulness_with_hhem': 0.5241}\n",
    "# this seems to agree with the RAGAS faithfulness score in that answers seem to be partially made up.\n",
    "result_faithfulness_hhem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cbe4c-3937-4d48-829a-f0b262698e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "02abe059-1129-4f60-81b7-55cef6b958ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra non-working code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "38993aa7-31ed-438e-96c6-63de9c608016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to use the query_engine in the evaluation \n",
    "# Modeled after this tutorial: https://docs.ragas.io/en/latest/howtos/applications/compare_llms.html\n",
    "\n",
    "# Does not currently work: for some metrics, it is not finding the 'ground_truth' column in the dataset\n",
    "# For other metrics, appears to run but returns the below errors and returns 'nan' for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b983f067-90e5-465a-9014-26fc5593f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of testing to try and get rag query engine for evaluate\n",
    "def generate_responses(query_engine, test_questions, test_answers):\n",
    "  responses = [query_engine.query(q) for q in test_questions]\n",
    "\n",
    "  answers = []\n",
    "  contexts = []\n",
    "  for r in responses:\n",
    "    answers.append(r.response)\n",
    "    contexts.append([c.node.get_content() for c in r.source_nodes])\n",
    "  dataset_dict = {\n",
    "        \"question\": test_questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "  }\n",
    "  if test_answers is not None:\n",
    "    dataset_dict[\"ground_truth\"] = test_answers\n",
    "  ds = Dataset.from_dict(dataset_dict)\n",
    "  return ds\n",
    "\n",
    "test_questions = testset_pd['question'].values.tolist()\n",
    "test_answers = [[item] for item in testset_pd['answer'].values.tolist()]\n",
    "\n",
    "result_ds = generate_responses(query_engine, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0cf99-4ede-4081-9dbe-2516b092da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up query and chat engines with the index\n",
    "# query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "# chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f75a3-602b-49b8-b269-dab9f3ce817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This evaluate function that uses the query_engine does not return results (nan for all metrics)\n",
    "# In reviewing the code for the evaluate function below from ragas, it uses the query_engine to develop new answers and contexts\n",
    "# before ultimately calling the normal ragas evaluate function that uses the non-RAG llm to evaluate the metrics\n",
    "# Thus, I don't think this code does what I'm looking for;\n",
    "# That is, it doesn't evaluate a dataset for a given metric by using the query_engine as opposed to the non-RAG llm\n",
    "\n",
    "# Errors (below are repeated many times):\n",
    "# WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
    "# n values greater than 1 not support for LlamaIndex LLMs\n",
    "# INFO:ragas.llms.base:callbacks not supported for LlamaIndex LLMs, ignoring callbacks\n",
    "# callbacks not supported for LlamaIndex LLMs, ignoring callbacks\n",
    "# ERROR:ragas.executor:Exception raised in Job[5]: TimeoutError()\n",
    "# Exception raised in Job[5]: TimeoutError()\n",
    "# ERROR:ragas.executor:Exception raised in Job[19]: AttributeError('ChatGoogleGenerativeAI' object has no attribute 'acomplete')\n",
    "# Exception raised in Job[19]: AttributeError('ChatGoogleGenerativeAI' object has no attribute 'acomplete')\n",
    "\n",
    "from ragas.integrations.llama_index import evaluate\n",
    "\n",
    "eval_qe2 = evaluate(\n",
    "    query_engine=query_engine,\n",
    "    dataset=result_ds,\n",
    "    metrics=[faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_utilization],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=embeddings, \n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "296c8c6d-737d-4233-ae5a-4114597a76f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': nan, 'answer_relevancy': nan, 'context_utilization': nan}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_qe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c19d1-71a0-40ba-ad28-f689cba75277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d2b9ae-1207-48bb-a130-76cd31bb9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/21 combination of results from ragas run of 918 examples\n",
    "# Contextual Precision metric for wikipedia_chat dataset from huggingface\n",
    "# Will need to refine to just the first 903 examples, as the last 15 examples have now been labeled and separately used for ARES\n",
    "# (I had forgotten that ARES required labeled examples; hence I'm removing the last few results from the calculations for the metric in RAGAS accordingly)\n",
    "\n",
    "data1 = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_1_and134.csv\", index_col=None)\n",
    "data2 = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_150_297.csv\", index_col=None)\n",
    "data3 = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_297_447.csv\", index_col=None)\n",
    "data4 = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_447.csv\", index_col=None)\n",
    "data5 = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_448_597.csv\", index_col=None)\n",
    "data6 = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_597_747.csv\", index_col=None)\n",
    "data7 = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_747_895.csv\", index_col=None)\n",
    "data8 = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_895.csv\", index_col=None)\n",
    "data9 = pd.read_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_896_918.csv\", index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d677a2c1-3b6e-4288-a325-fc5ea05d67c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data9['context_precision'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aea5a8ae-8486-400a-b9d7-6829fbde4d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding length of all csv to ensure = full 918 dataset\n",
    "150+147+150+1+149+150+148+1+22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce69a160-62d3-47f7-969f-3e939a1397ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all results\n",
    "allresults = pd.concat([data1, data2, data3, data4, data5, data6, data7, data8, data9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad2d6ac9-b32a-402d-ab2d-2c3b54a14444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "      <td>['Abraham Lincoln (February 12, 1809 â\\x80\\x93...</td>\n",
       "      <td>Yes, Abraham Lincoln was the sixteenth Preside...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[\"Lincoln believed in the Whig theory of the p...</td>\n",
       "      <td>Yes, the provided text states that Lincoln sig...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did his mother die of pneumonia?</td>\n",
       "      <td>no</td>\n",
       "      <td>['Soon thereafter, Tesla hastened from Paris t...</td>\n",
       "      <td>The provided text doesn't contain information ...</td>\n",
       "      <td>0.240741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many long was Lincoln's formal education?</td>\n",
       "      <td>18 months</td>\n",
       "      <td>[\"Lincoln's formal education consisted of abou...</td>\n",
       "      <td>The text states that Lincoln's formal educatio...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did Lincoln begin his political career?</td>\n",
       "      <td>1832</td>\n",
       "      <td>['Lincoln began his political career in 1832, ...</td>\n",
       "      <td>Lincoln began his political career in 1832, at...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Was Wilson president of the American Political...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['* Wilson was president of the American Polit...</td>\n",
       "      <td>Yes, Woodrow Wilson was president of the Ameri...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Did he not cast his ballot for John M. Palmer ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['By the time Wilson finished Congressional Go...</td>\n",
       "      <td>The provided text states that Wilson *did* cas...</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Did Wilson not spend 1914 through the beginnin...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['Wilson spent 1914 through the beginning of 1...</td>\n",
       "      <td>That statement is correct.  Wilson's administr...</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Was Wilson , a staunch opponent of antisemitis...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>['Wilson, a staunch opponent of antisemitism ,...</td>\n",
       "      <td>Yes, according to the provided text, Wilson wa...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>What happened in 1917?</td>\n",
       "      <td>raised billions through Liberty loans, imposed...</td>\n",
       "      <td>['Canadian soldiers would win the Battle of Vi...</td>\n",
       "      <td>Several significant events happened in 1917:\\n...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>918 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Was Abraham Lincoln the sixteenth President of...   \n",
       "1   Did Lincoln sign the National Banking Act of 1...   \n",
       "2                    Did his mother die of pneumonia?   \n",
       "3       How many long was Lincoln's formal education?   \n",
       "4        When did Lincoln begin his political career?   \n",
       "..                                                ...   \n",
       "17  Was Wilson president of the American Political...   \n",
       "18  Did he not cast his ballot for John M. Palmer ...   \n",
       "19  Did Wilson not spend 1914 through the beginnin...   \n",
       "20  Was Wilson , a staunch opponent of antisemitis...   \n",
       "21                             What happened in 1917?   \n",
       "\n",
       "                                         ground_truth  \\\n",
       "0                                                 yes   \n",
       "1                                                 yes   \n",
       "2                                                  no   \n",
       "3                                           18 months   \n",
       "4                                                1832   \n",
       "..                                                ...   \n",
       "17                                                Yes   \n",
       "18                                                Yes   \n",
       "19                                                Yes   \n",
       "20                                                Yes   \n",
       "21  raised billions through Liberty loans, imposed...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   ['Abraham Lincoln (February 12, 1809 â\\x80\\x93...   \n",
       "1   [\"Lincoln believed in the Whig theory of the p...   \n",
       "2   ['Soon thereafter, Tesla hastened from Paris t...   \n",
       "3   [\"Lincoln's formal education consisted of abou...   \n",
       "4   ['Lincoln began his political career in 1832, ...   \n",
       "..                                                ...   \n",
       "17  ['* Wilson was president of the American Polit...   \n",
       "18  ['By the time Wilson finished Congressional Go...   \n",
       "19  ['Wilson spent 1914 through the beginning of 1...   \n",
       "20  ['Wilson, a staunch opponent of antisemitism ,...   \n",
       "21  ['Canadian soldiers would win the Battle of Vi...   \n",
       "\n",
       "                                               answer  context_precision  \n",
       "0   Yes, Abraham Lincoln was the sixteenth Preside...           1.000000  \n",
       "1   Yes, the provided text states that Lincoln sig...           1.000000  \n",
       "2   The provided text doesn't contain information ...           0.240741  \n",
       "3   The text states that Lincoln's formal educatio...           1.000000  \n",
       "4   Lincoln began his political career in 1832, at...           1.000000  \n",
       "..                                                ...                ...  \n",
       "17  Yes, Woodrow Wilson was president of the Ameri...           1.000000  \n",
       "18  The provided text states that Wilson *did* cas...           0.700000  \n",
       "19  That statement is correct.  Wilson's administr...           0.866667  \n",
       "20  Yes, according to the provided text, Wilson wa...           1.000000  \n",
       "21  Several significant events happened in 1917:\\n...           0.000000  \n",
       "\n",
       "[918 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c83e080a-7593-45fc-987d-d60542871a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allresults.to_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_all918.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82e69844-f9e8-4713-8072-53071de4cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "results903 = allresults[:903]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "94cb684d-6ca1-4167-9984-ec42943ac8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results903.to_csv(\"results/results_ragas_rag_mini_wiki_complete_chat_903.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9b5e805-8305-4610-8d09-c7ace3d9c1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7714837243775122"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average Contextual Precision score for 903 examples\n",
    "results903['context_precision'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a67cf-1dcb-447c-b4a7-b7221c0e789e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7013aace-aef2-4d7f-9c92-2e0596c68ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12/11 comparison of results from deepeval and ragas\n",
    "# This was using a dataset synthetically generated by deepeval\n",
    "# Just comparing Contextual Precision metric\n",
    "deepeval_results = pd.read_csv(\"results/deepeval_contextprecision_unlabeled.csv\", index_col=None)\n",
    "ragas_results = pd.read_csv(\"results/results_ragas_unlabeled_all.csv\", index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b3ca6ecb-1999-47ff-8ae7-c44abb13a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([ragas_results, deepeval_results], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0c1f84d7-9b3c-4171-8a0c-0de80ef778b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.rename(columns={'context_precision': 'ragas_cp', 'score':'deepeval_cp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "685aad5c-f593-4cab-809f-6e75e5f67c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "364d3330-7c8a-46a1-8873-a335d09ff27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.540941989035695"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 169/800 rows were within 0.1 score of each other\n",
    "# 234/800 rows were within 0.2 score of each other\n",
    "# 278/800 rows were within 0.3 score of each other, \n",
    "# so 522/800 were > 0.3 score of each other\n",
    "# average difference between results was 0.5409... this tells me they were really far apart, given a restricted scoring range of 0-1\n",
    "np.mean(abs(results['ragas_cp'] - results['deepeval_cp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "04a64ba7-9a6c-4f34-a76b-35b25b0ad951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.plot.hist(column=['deepeval_cp', 'ragas_cp'], bins=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1ebdc6d2-9959-434d-802f-67fadc9895be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.plot.hist(column=['ragas_cp'], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f54a993b-63ab-4af1-9024-46e667c6ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"results/ragas_and_deepeval_unlabeled_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2054ff98-334d-427e-be5e-965a1b2551c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled = pd.read_csv(\"datasets/unlabeled_dataset/unlabeled_dataset.csv\", index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "239944cd-733c-4482-871d-926aa3400c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled['Document'] = unlabeled['Document'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1d545-cda3-404b-baa2-846a6ff5078c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0008564c-ffa1-48d8-9eb4-50a635000493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "154\n",
      "400\n",
      "163\n",
      "741\n",
      "704\n",
      "126\n",
      "167\n",
      "724\n",
      "18\n",
      "379\n",
      "51\n",
      "514\n",
      "556\n",
      "586\n"
     ]
    }
   ],
   "source": [
    "# for my manual comparison review of results (above)\n",
    "import random\n",
    "for i in range(0,15):\n",
    "    print(random.randint(2, 801))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2aaff3-568f-44f2-bb90-1324bdf8cd82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research2",
   "language": "python",
   "name": "research2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
