{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf756bfd-87bf-4b11-a819-e205176d9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG with Gemini Flash 1.5 LLM and ARES evaluation\n",
    "# Google Gemini: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# ARES: https://github.com/stanford-futuredata/ARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7693b8-cf90-4d82-b1ba-89e6d55abcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish RAG pipeline with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65781f17-9358-4d53-9305-939013e2a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "#from llama_index.llms.gemini import Gemini\n",
    "#from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeabfa92-2145-4086-9ba0-d1327a8e2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66957101-56ea-4dbb-86f9-347a3380ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up local API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b7c6c6-4cac-4b1d-bd60-a409f549759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\") # old function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dcd6cf4-f33c-47a0-aa5b-9c236b84822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document database\n",
    "# using 4 State of the Union speeches, all text from whitehouse.gov briefing room speeches posted online, including a title with the date of the speech\n",
    "# Example from 2024:\n",
    "# https://www.whitehouse.gov/briefing-room/speeches-remarks/2024/03/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery-2/\n",
    "sotu = []\n",
    "newfiles = [\"./Speeches/titleedits/state_of_the_union_042921.txt\", \"./Speeches/titleedits/state_of_the_union_030122.txt\", \"./Speeches/titleedits/state_of_the_union_020723.txt\", \"./Speeches/titleedits/state_of_the_union_030724.txt\"]\n",
    "for i in newfiles:\n",
    "    with open(i) as file:\n",
    "        for line in file:\n",
    "            nl = line.rstrip()\n",
    "            if nl != '':\n",
    "                sotu.append(nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c33d54-9e37-4e1d-834c-669026fe4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(text=line) for line in sotu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4cadba0-1efd-47a7-849c-586713f3fc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='e982b1b7-8145-4dd8-8be5-240f36964602', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='May God protect our troops.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a loaded Document line\n",
    "documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8534e02-a129-42c1-871f-31ab6919ddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Set up the faiss index\n",
    "d = 768 # dimensions of the input vector of the embedding model that we're going to use; in this case, the google embedding model\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "print(faiss_index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29482b9f-dbea-4e8e-bdb3-1dbc262b7324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = Gemini(model=\"models/gemini-1.5-flash\", api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab6ce235-d506-4151-9970-d4a34b52a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the embeddings\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # optional: task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "#doc_embeddings = GeminiEmbedding(model=\"models/text-embedding-004\")\n",
    "Settings.embed_model = doc_embeddings\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5e63d9f-afda-4661-a477-d9192d710779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test llm\n",
    "#response = llm.complete(\"Write the text for an invitation for a two year old's penguin themed birthday party.\")\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe61b42-b268-4f15-a8cc-6ee6130aff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment for when you need to re-embed and vectorize documents\n",
    "## otherwise, doing local loading below\n",
    "#vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "#storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#index = VectorStoreIndex.from_documents(\n",
    "#    documents, storage_context=storage_context, show_progress=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efff8caa-8a55-441a-9706-12e0f27487f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "#index.storage_context.persist()\n",
    "#index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c032469-b39e-47bf-a328-d884a9e717f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "# index id 'cef7ae30-ff1e-404a-bce6-85d59ca4b376' uses the speeches with a title that includes the date it was given\n",
    "index = load_index_from_storage(storage_context=storage_context, index_id='cef7ae30-ff1e-404a-bce6-85d59ca4b376')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "833ae491-6e3e-497e-af35-f2823c7da9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query and chat engines\n",
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3b0844-f59f-4d12-856e-d2f28fa05857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query and response\n",
    "#query = \"What does the President say about his administration's first 100 days and covid-19?\"\n",
    "#response = query_engine.query(query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d0bee64-2b5d-4ae3-8b54-fda538ad476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The President is proud of his administration's progress in fighting the pandemic, citing the successful rollout of COVID-19 vaccines. He highlights that they have surpassed their goal of administering 100 million vaccine shots in 100 days, reaching over 220 million shots. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34897696-e3d2-4a40-b741-d839cc308931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to get ARES to work with Gemini and our local RAG setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2984ff52-f184-49ec-a3d8-8124c8667a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM not imported.\n"
     ]
    }
   ],
   "source": [
    "from ares import ARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4ce3cc2-a413-4ce2-8891-593347deddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ues_idp_config = {\n",
    "    \"in_domain_prompts_dataset\": \"ARES_files/nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
    "    \"unlabeled_evaluation_set\": \"ARES_files/nq_unlabeled_output.tsv\", \n",
    "    \"model_choice\" : \"models/gemini-1.5-flash\",\n",
    "    \"request_delay\" : 60,\n",
    "    \"documents\" : 3\n",
    "} \n",
    "\n",
    "ares = ARES(ues_idp=ues_idp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0957b957-09c2-4b0d-bda1-a87945225f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a67a3e9b7a47168bcb0e97f7c05f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating large subset with models/gemini-1.5-flash:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configured gemini for context relevance\n",
      "Testing response.text\n",
      "[[Yes]] \n",
      "\n",
      "Testing candidates\n",
      "['[[Yes]] \\n']\n",
      "configured gemini for answer relevance\n",
      "Testing response.text\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for answer faithfulness\n",
      "Testing response.text\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for context relevance\n",
      "Testing response.text\n",
      "[[Yes]]\n",
      "\n",
      "Testing candidates\n",
      "['[[Yes]]\\n']\n",
      "configured gemini for answer relevance\n",
      "Testing response.text\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for answer faithfulness\n",
      "Testing response.text\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for context relevance\n",
      "Attempt 1 failed with error: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n",
      "Attempt 2 failed with error: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n",
      "Attempt 3 failed with error: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n",
      "Attempt 4 failed with error: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n",
      "All attempts failed. Last error was: Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. Please check the `candidate.safety_ratings` to determine if the response was blocked.\n",
      "Number of times did not extract Yes or No: 0\n",
      "{'Context Relevance Scores': 0.667, 'Answer Faithfulness Scores': 0.667, 'Answer Relevance Scores': 0.667}\n"
     ]
    }
   ],
   "source": [
    "results = ares.ues_idp()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c815504d-3470-4047-9e4f-35332844fa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Context Relevance Scores': 0.667,\n",
       " 'Answer Faithfulness Scores': 0.667,\n",
       " 'Answer Relevance Scores': 0.667}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results # exact scores repeated on second run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca8ee8b-e105-436f-8f52-692c736bbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopped here: Need to generate synthetic question/answer/document file\n",
    "\n",
    "# document refers to a line in a document_filepath_file; unsure if this setup of 1 large document per line will work\n",
    "synth_config = { \n",
    "    \"document_filepaths\": [\"Speeches/titleedits/Speeches_Docs6.tsv\"], # requires tsv file...\n",
    "    \"few_shot_prompt_filename\": \"datasets/manual_dataset_complete_ares_synthetic.tsv\",\n",
    "    \"synthetic_queries_filenames\": [\"results/synthetic_results.tsv\"],\n",
    "    \"model_choice\": \"models/gemini-1.5-flash\", # ex: \"google/flan-t5-xxl\" \n",
    "    \"documents_sampled\": 4, # was 10000\n",
    "    \"api_model\": True\n",
    "}\n",
    "\n",
    "ares_synth = ARES(synthetic_query_generator=synth_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57f5c628-84b7-4222-bd8d-5499dd1b585c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Saving synthetic queries to:  ['results/synthetic_results.tsv']\n",
      "========================================\n",
      "\n",
      "=======================================\n",
      "| Starting Synthetic Query Generation |\n",
      "=======================================\n",
      "\n",
      "Generating positive queries for the first 2 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating positive synthetic queries for documents 0 to 2...:   0%|                              | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configured gemini for synthetic query generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating positive synthetic queries for documents 0 to 2...:  25%|█████▌                | 1/4 [00:06<00:18,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configured gemini for synthetic query generation\n",
      "Error generating synthetic queries: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating positive synthetic queries for documents 0 to 2...:  50%|███████████           | 2/4 [01:12<01:22, 41.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configured gemini for synthetic query generation\n",
      "Error generating synthetic queries: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating positive synthetic queries for documents 0 to 2...:  75%|████████████████▌     | 3/4 [02:17<00:52, 52.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configured gemini for synthetic query generation\n",
      "Error generating synthetic queries: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating positive synthetic queries for documents 0 to 2...: 100%|██████████████████████| 4/4 [03:23<00:00, 50.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries generated before filtering: 48\n",
      "Total queries after length filtering: 48\n",
      "Total queries after deduplication: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings...:   0%|                                                                   | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings...: 100%|███████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings...: 100%|███████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.97it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678b8f1f5d774ac881041a44e6a8db54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▊                                                                                 | 1/30 [00:00<00:07,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▌                                                                              | 2/30 [00:00<00:06,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▍                                                                           | 3/30 [00:00<00:07,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|███████████▏                                                                        | 4/30 [00:01<00:06,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████                                                                      | 5/30 [00:01<00:06,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▊                                                                   | 6/30 [00:01<00:06,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|███████████████████▌                                                                | 7/30 [00:01<00:05,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████▍                                                             | 8/30 [00:02<00:05,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████▏                                                          | 9/30 [00:02<00:04,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███████████████████████████▋                                                       | 10/30 [00:02<00:04,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████████████████████▍                                                    | 11/30 [00:02<00:04,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 12/30 [00:02<00:04,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████▉                                               | 13/30 [00:03<00:04,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████████████▋                                            | 14/30 [00:03<00:03,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 15/30 [00:03<00:03,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|████████████████████████████████████████████▎                                      | 16/30 [00:03<00:03,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████                                    | 17/30 [00:04<00:03,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 18/30 [00:04<00:02,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|████████████████████████████████████████████████████▌                              | 19/30 [00:04<00:02,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████████████████████████████████▎                           | 20/30 [00:04<00:02,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 21/30 [00:05<00:02,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████▊                      | 22/30 [00:05<00:02,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████████████████████████████████████████████████████████████▋                   | 23/30 [00:05<00:01,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 24/30 [00:06<00:01,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|█████████████████████████████████████████████████████████████████████▏             | 25/30 [00:06<00:01,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████▉           | 26/30 [00:06<00:01,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 27/30 [00:06<00:00,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████████▍     | 28/30 [00:07<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|████████████████████████████████████████████████████████████████████████████████▏  | 29/30 [00:07<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using Gemini text embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total queries after filtering: 30\n",
      "Generating negative queries for the remaining 2 documents...\n",
      "Saved synthetic queries to: results/synthetic_results.tsv\n",
      "\n",
      "=========================================\n",
      "| Synthetic query generation completed. |\n",
      "=========================================\n",
      "\n",
      "Total queries saved: 6 (Positive: 2, Duplicate: 2, Negative: 2)\n",
      "\n",
      "================================\n",
      "| Beginning answer generation! |\n",
      "================================\n",
      "\n",
      "Generating answers for 2 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers... (models/gemini-1.5-flash):   0%|                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configured gemini for synthetic answer generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers... (models/gemini-1.5-flash): 100%|████████████████████████████████████| 2/2 [00:01<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configured gemini for synthetic answer generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers... (models/gemini-1.5-flash): 100%|████████████████████████████████████| 2/2 [00:02<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answers for 2 queries.\n",
      "Saved positive answers to: results/synthetic_results.tsv\n",
      "Generating negative answers for the second chunk of queries...\n",
      "Saved answers to: results/synthetic_results.tsv\n",
      "Completed synthetic generation!\n",
      "Saved synthetic queries file to: results/synthetic_results.tsv\n",
      "\n",
      "===============================================\n",
      "| Answer generation and processing completed. |\n",
      "===============================================\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = ares_synth.generate_synthetic_data() # stopped here, need to debug/edit to work with Gemini; next is embeddings error below\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027370c9-e2af-48a3-96e2-edc9755cda26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437ef8e-f8f2-4428-b5d3-a4a5654c444e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research2",
   "language": "python",
   "name": "research2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
