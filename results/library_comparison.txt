RAGAS:

- Version 0.1.15 - edited locally to resolve a compatibility issue with Gemini

- Overview (as of August 2024):
  - Metrics seem well designed and are the most common metrics across RAG evaluation libraries. No explanations are given for the scores.
  - Synthetic testset generation (question/context/ground truth answer) function worked pretty well; questions generated for the documents were the most "human-like" of all libraries tested.  
  - Documentation was significantly lacking when tested but has since been updated and may have been improved.
  - Internal rate limiting functions didn't work well, and RAGAS didn't work well (with limited testing) with external rate limiting libraries.

- Metrics:

  - Note: RAGAS documentation/functions were updated 10/10/24, so there are a few new variations of current metrics I haven't tried.
RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics

  - Faithfulness - Measures the factual consistency of the answer to the context based on the question.
  - Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.
  - Answer_relevancy - Measures how relevant the answer is to the question.
  - Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.
  - Faithfulness with HHEM - Similar to Faithfulness but uses a HuggingFace model (Vectara's HHEM 2.1 classifier) to detect hallucinations
    - https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html#faithfullness-with-hhem-2-1-model
    - https://huggingface.co/vectara/hallucination_evaluation_model

- Data generation: RAGAS allows you to generate a synthetic testset of any size with Question/Context/Ground Truth columns, using the docs/model in your RAG pipeline.
  - On a very small generation (n=10), results seemed pretty good; saves a lot of time vs creating this manually.
  - Generate synthetic test data : When generating a synthetic test dataset, the columns generated are 'question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', 'episode_done'.
  - You specify the distribution of the kinds of questions generated: 'simple', 'reasoning', or 'multi_context' questions, as well as the percentage of multi-context questions
  - Ground truth is supposed to be the 'human' level answer vs the RAG answer. 
  - We then have to generate the answer separately with our RAG, which then (obviously) generates new context used. My best guess is to use the context that was used to generate the answer for the metrics calculation. The original 'contexts' column was pretty good overall and typically could answer the question. Occasionally the generated context was not relevant/specific enough to answer the question.
  - Example: testset_answer_newcontext_flash_pro15.csv : Synthetically generated testset with RAGAS, with the addition of answers generated by Gemini 1.5 Flash along with the context used to create those answers. The 'context_gt' column contains the original contexts used to generate the 'ground_truth' column, all of which was created with the RAGAS TestsetGenerator.
    - Generator LLM: Gemini 1.5 Flash
    - Critic LLM: Gemini 1.5 Pro
    - Embeddings: models/text-embedding-004

- Required datasets:
  - Dataset for evaluation needs to be in Dataset format, not pandas dataframe (per issues raised on GitHub repo)
  - Dataset columns: ['question', 'answer', 'ground_truth', 'contexts', 'source_file']

- Custom/local model integration:
  - Works with LangchainLLM and LangchainEmbeddings : I used ChatGoogleGenerativeAI and GoogleGenerativeAIEmbeddings. Since Langchain integrates with VLLM, in theory RAGAS should work with VLLM or other local models with a Langchain wrapper.
  - There is also a BaseRagasLLM and BaseRagasEmbeddings class you can inherit from, but I did not test this.

- Performance results:
  - Below results were on my manually created dataset: manual_dataset_complete.csv, n=11, using all 4 speeches
  - RAGAS metrics simply output a score for the chosen metric without an additional explanation.
  - RAGAS metrics guide does not have example ranges to compare anything to, so below is my best guess.

  - Faithfulness - Measures the factual consistency of the answer to the context based on the question.
  - 0.9167 - 1.0000 indicates that the LLM is staying true to the facts provided in the context for answering the question.
  - Faithfulness with HHEM - This uses a huggingface model to help detect hallucination : https://huggingface.co/vectara/hallucination_evaluation_model
  - 0.6319: This doesn't really agree with the RAGAS faithfulness score... may need to dive in further another time.
  - Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.
  - At 0.4171 - 0.5979, suggests that the context isn't particularly relevant to the question. After reviewing the results, I think this should probably be  a little higher, but the score makes sense.
  - Answer_relevancy - Measures how relevant the answer is to the question.
  - 0.6509 - 0.6533 seems moderately low, just going off of a manual review of the answer. This may be due to questions asked that do not have an answer in the context.
  - Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.
  - 0.8 indicates that the llm context is decently good and can typically answer the question or most of it. I concur with this after manual review of the dataset.

- Other notes:
  - The RAGAS internal RunConfig settings do a decent job at limiting the 429 resource exhausted warnings when max_workers=1. Ratelimit and backoff Python libraries didn't seem to affect performance, but I didn't spend a lot of time with that.
  - There's a flag you have to set initially for RAGAS not to track your usage.


DeepEval:

- Version 1.1.6

- Overview:

- Metrics:  
  - DeepEval provides a score for each metric and a reason (output by the LLM) for each metric's score

  - For RAG systems, DeepEval recommends running the following metrics:

  - Retrieval metrics:
    - Contextual Precision: Average: 0.776
      - Calculated fairly similarly (if not the same) as RAGAS's contextual precision; would need to look into code further
      - Manual review: Decent overall, but some definitely had the single answer needed higher ranked in the nodes list, yet received a low score (0.25), likely for giving extra context that was not as relevant.
      - Compared with RAGAS in DeepEval Contextual Precision: 0.493 (not close at all)
      - Compared with RAGAS package Contextual Precision: 0.6944 (closer to result here, but not to the above, which should be the same)

    - Contextual Recall: Average: 0.723
      - Calculated approximately the same way as RAGAS, according to their brief algorithm description
      - Manual review: Good overall, not great. For example, for the first test case, it said that a specific fact was not mentioned in the context when it actually was in the context. The output reasoning left a lot to be desired.
        - Example of poor output reason: "and the JSON output is formatted correctly to match the given schema" has nothing to do with the metric scoring for this metric
        - In another reason given for a perfect score, it gave additional context that is not found in the retrieval context; it seemed to more try to answer the prompt vs doing the evaluation of the metric.
      - Compared with RAGAS in DeepEval Contextual Recall: 0.627
      - Compared with RAGAS package Context Recall: 0.6455 (pretty close)

    - Contextual Relevance: "Measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval_context for a given input."
      - Unfortunately, I could not test this metric due to 429 Resource Exhausted Errors, regardless of rate limiting

  - Generation metrics:
    - Answer Relevancy: Average: 0.989
      - Calculated differently than RAGAS's answer relevancy
      - Manual review: Not great. If it's working, it seems to work fine generally. However some scores/reasons were VERY off, as not-relevant reasons for the scores were given and were inconsistent with the scoring.
        - Example 1: "The score is 1.00 because you provided an input, and asked for a JSON output. That's perfect! Let's keep going." 
        - Example 2: "Reason: The score is 1.00 because the input asks for a specific reason and expects a JSON object with that reason. The provided JSON does not contain any object to address the request, therefore the score is 1.00, implying that the output does not address the input."
      - Compared with RAGAS in DeepEval Answer Relevancy: 0.578 (not close at all to DeepEval, but makes sense with different algorithmic calculation)
      - Compared with RAGAS package Answer Relevancy: 0.6770 (pretty close to above, not to DeepEval)

    - Faithfulness: Average: 1.0
      - Calculated approximately the same way as RAGAS, according to their brief algorithm description
      - Manual review: This metric only returned perfect 1.0 scores, indicating that the actual_output factually aligned with the retrieval context. This seems mostly accurate except for the example where the actual_output definitely brought in outside knowledge. I suppose that technically doesn't factually contradict the retrieval_context, and that would fall under a "hallucination" instead. 
      - Compared with RAGAS in DeepEval Faithfulness: 0.984
      - Compared with RAGAS package Faithfulness': 1.0000 (close)
  


- Data generation:

- Required datasets:

- Custom/local model integration:
  - Non Open-AI models require a custom LLM class and a custom embeddings class for using DeepEval, inheriting from DeepEvalBaseLLM and DeepEvalBaseEmbeddingModel, respectively

- Performance results:

- Other notes:
  - DeepEval requires a json response. In practice, this has led to malformed json returned from the llm, even with as simple of a schema as possible (i.e., return string)
  - There's a flag you have to set initially for DeepEval not to track your usage.

ARES:

- Overview:

- Metrics: 

- Context Relevance: Determines if the retrieved information is pertinent to the query.
- Answer Faithfulness: Checks if the response generated by the language model is properly grounded in the retrieved context and does not include hallucinated or extraneous information.
- Answer Relevance: Evaluates whether the generated response is relevant to the query, addressing all aspects of the question appropriately.

- Data generation:

- Required datasets:

- Performance results:

- Other notes:
