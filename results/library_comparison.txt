Overall overview:
- Each library uses LLM-based metrics, meaning that it uses an LLM to decide whether each question/answer/context matched the criteria for each metric. The library then calculates an overall value for the metric based off of the results.

RAGAS:

- Version 0.1.15 - edited locally to resolve a compatibility issue with Gemini
- Also updated to Version 0.2.12 - no local library edits needed besides one function in notebook to assist with compatibility issue

- Overview:
  - Metrics seem well designed and are the most common metrics across RAG evaluation libraries. No explanations are given for the scores.
  - Synthetic testset generation (question/context/ground truth answer) function worked pretty well, assuming you do not encounter rate-limit issues (didn't seem to work well with traditional rate limiting libraries). Questions generated for the documents were the most "human-like" of all libraries tested.  
    - In addition to maintaining the same kind of synthetic testset generation in version 0.2.12, RAGAS also added the ability to better specify how a knowledge graph is structured for creating synthetic queries. I did not test this new functionality, but it looks promising.	
  - Documentation was significantly updated and improved over the last 6+ months.
  - Internal rate limiting functions do not work well, and RAGAS didn't work well (with limited testing in version 0.1.15) with external rate limiting libraries.
  - Integrates with llamaindex; was able to resolve a local compatibility issue and have it work with Gemini as well
  - Supports all LLMs and Embeddings available in Langchain

- Metrics:

  - Note: RAGAS documentation/functions were updated 10/10/24, so there are a few new variations of current metrics I haven't tried.
RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics

  - Faithfulness - Measures the factual consistency of the answer to the context based on the question. 2 API calls per example.
  - Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline. 1 API call per context node per example.
  - Answer_relevancy - Measures how relevant the answer is to the question.
  - Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.
  - Faithfulness with HHEM - Similar to Faithfulness but uses a HuggingFace model (Vectara's HHEM 2.1 classifier) to detect hallucinations
    - https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html#faithfullness-with-hhem-2-1-model
    - https://huggingface.co/vectara/hallucination_evaluation_model

- Data generation: RAGAS allows you to generate a synthetic testset of any size with Question/Context/Ground Truth columns, using the docs/model in your RAG pipeline.
  - On a very small generation (n=10), results seemed pretty good; saves a lot of time vs creating this manually.
  - Generate synthetic test data : When generating a synthetic test dataset, the columns generated are 'question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', 'episode_done'.
  - You specify the distribution of the kinds of questions generated: 'simple', 'reasoning', or 'multi_context' questions, as well as the percentage of multi-context questions
  - Ground truth is supposed to be the 'human' level answer vs the RAG answer. 
  - We then have to generate the answer separately with our RAG, which then (obviously) generates new context used. My best guess is to use the context that was used to generate the answer for the metrics calculation. The original 'contexts' column was pretty good overall and typically could answer the question. Occasionally the generated context was not relevant/specific enough to answer the question.
  - Example: testset_answer_newcontext_flash_pro15.csv : Synthetically generated testset with RAGAS, with the addition of answers generated by Gemini 1.5 Flash along with the context used to create those answers. The 'context_gt' column contains the original contexts used to generate the 'ground_truth' column, all of which was created with the RAGAS TestsetGenerator.
    - Generator LLM: Gemini 1.5 Flash
    - Critic LLM: Gemini 1.5 Pro
    - Embeddings: models/text-embedding-004

- Required datasets:
  - Dataset for evaluation needs to be in Dataset format, not pandas dataframe (per issues raised on GitHub repo)
  - Dataset columns: ['question', 'answer', 'ground_truth', 'contexts', 'source_file']

- Custom/local model integration:
  - Works with LangchainLLM and LangchainEmbeddings : I used ChatGoogleGenerativeAI and GoogleGenerativeAIEmbeddings. Since Langchain integrates with VLLM, in theory RAGAS should work with VLLM or other local models with a Langchain wrapper.
  - There is also a BaseRagasLLM and BaseRagasEmbeddings class you can inherit from, but I did not test this.

- Performance results:
  - Below results were on my manually created dataset (I created the questions): manual_dataset_complete.csv, n=11, using all 4 speeches
  - RAGAS metrics simply output a score for the chosen metric without an additional explanation.
  - RAGAS metrics guide does not have example ranges to compare anything to, so below is my best guess.
  - I found out months later that calling .to_pandas() on the result after evaluation will give the dataset back along with the metric scores for each example in a pandas dataframe
  - Note: There's an optional parameter in_ci (bool) that if set to True, then some metrics will be run to increase the reproducibility of the evaluations. This will increase the runtime and cost of evaluations. Default is False.
    - In testing, setting in_ci = True resulted in a lot of timeouts / no score calculated / NaN results.
  - Note: When running the context precision metric, RAGAS generates 1 API request per context node in each example 10 in order to get the resulting score for the metric. That is, if you have 10 context nodes with an example query/answer, it will generate 10 API requests, as it is developing a score per context node for each example.
  - For the faithfulness metric, RAGAS generates 2 API calls per example. The first API request breaks up the answer into simpler statements. The second request judges the faithfulness of the simpler statements based on the context, returning either a 0 or 1 as a faithfulness measure for each statement. Note that a "reason" for the score is also returned by RAGAS in the internal metric but isn't passed back to the user in this version of the library. 

  - Faithfulness - Measures the factual consistency of the answer to the context based on the question.
  - 0.9167 - 1.0000 indicates that the LLM is staying true to the facts provided in the context for answering the question.
  - Faithfulness with HHEM - This uses a huggingface model to help detect hallucination : https://huggingface.co/vectara/hallucination_evaluation_model
  - 0.6319: This doesn't really agree with the RAGAS faithfulness score. I didn't investigate this further to see the difference. 
  - Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.
  - At 0.4171 - 0.5979, suggests that the context isn't particularly relevant to the question. After reviewing the results, I think this should probably be  a little higher, but the score makes sense.
  - Answer_relevancy - Measures how relevant the answer is to the question.
  - 0.6509 - 0.6533 seems moderately low, just going off of a manual review of the answer. This may be due to questions asked that do not have an answer in the context.
  - Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.
  - 0.8 indicates that the llm context is decently good and can typically answer the question or most of it. I concur with this after manual review of the dataset.

- Other notes:
  - When evaluating for the context precision metric, RAGAS sends 1 request to the LLM per context node in each example. 10 context nodes for 1 metric evaluation for 1 example = 10 API requests.
  - When evaluating for the faithfulness metric, RAGAS sends 2 requests to the LLM per example: 
      1) the first breaks up the answer into simpler statements
      2) the second judges the faithfulness of the simpler statements based on the context, returning 0 or 1 as a faithfulness measure for each statement.

  - The RAGAS internal RunConfig settings do a decent job at limiting the 429 resource exhausted warnings when max_workers=1. max_workers is supposed to control the number of concurent requests allowed together, and in testing this with max_workers=1, 6-10 requests for 1 example were sent to the API concurrently. Perhaps they are defining concurrent requests differently than I expect, or else this isn't working properly in the code. The ratelimit and backoff Python libraries didn't seem to affect performance with this either, but I didn't spend a lot of time with that.
  - There's a flag you have to set initially for RAGAS not to track your usage.


DeepEval:

- Version 1.1.6

- Overview (as of September 2024):
  - Metrics are well designed and have commonalities across all 3 libraries evaluated. DeepEval offered 6 different metrics for RAG systems, the most of the three libraries.
  - Numerical results for the metrics were good overall/somewhat comparable to RAGAS, but the reasoning returned for why a particular score was given for a metric was not consistently reliable. It probably depends on the RAG system & LLM, but I wouldn't rely on the LLM's reason for a given score when using DeepEval.
  - Documentation was more prevalent than other libraries but did not always reflect the exact code.
  - Requires the model to output valid json
  - Integrates with llamaindex, huggingface (not tested), any llm according to documentation

- Metrics:  
  - DeepEval provides a score for each metric and a reason (output by the LLM) for each metric's score

  - For RAG systems, DeepEval recommends running the following metrics:

  - Retrieval metrics:
    - Contextual Precision: evaluates whether the reranker in your retriever ranks more relevant nodes in your retrieval context higher than irrelevant ones.
    - Contextual Recall: evaluates whether the embedding model in your retriever is able to accurately capture and retrieve relevant information based on the context of the input.
    - Contextual Relevance: evaluates whether the text chunk size and top-K of your retriever is able to retrieve information without much irrelevancies.

  - Generation metrics:
    - Answer Relevancy: evaluates whether the prompt template in your generator is able to instruct your LLM to output relevant and helpful outputs based on the retrieval_context.
    - Faithfulness: evaluates whether the LLM used in your generator can output information that does not hallucinate AND contradict any factual information presented in the retrieval_context. 3-4 API requests per example (in progress)
  
- Data generation:
  - The synthetic data generation was the overall best working of the three libraries, as even though I hit rate-limiting errors, I was able to overcome them and generate large size (n=800) datasets by appending several small ones together. Questions and answers generated were the second most "human-like" of the libraries, and after manual review to remove a few non-relevant queries, they were good enough to stand in for a manually created dataset with some very minor edits (specifying years/dates of documents), especially when considering it took a lot less time to use the LLM to generate the dataset vs creating it manually.
  - Generate synthetic data : When generating a synthetic dataset, the columns generated are 'input', 'actual_output', 'expected_output', 'context', and 'source_file
    - The 'actual_output' column is blank; you need to re-run the input and context through your LLM to get the actual output. 'expected_output' acts as that answer otherwise.
  - Example: labeled_dataset_sotu.csv : Synthetically generated testset with DeepEval, with the addition of answers generated by Gemini 1.5 Flash, along with the new context used to create those answers, and manually labeled columns for the metrics (ex: context_relevance). The 'Contexts_QueryGen' column contains the original contexts, and the 'Contexts_1file' column contains the new context retrieved when I re-ran the queries through the RAG system (each time indexed with only the one relevant sotu speech). Likewise, 'Expected_Output' contains the originally projected output with Contexts_QueryGen, and 'Answer' contains the newly generated answer with 'Contexts_1file'.
    - Generator LLM: Gemini 1.5 Flash
    - Critic LLM: Gemini 1.5 Pro
    - Embeddings: models/text-embedding-004

- Required datasets:
  - To get results for metrics in DeepEval, you can either use a single "LLMTestCase"- one 'input', one 'actual_output', one 'expected_output', and one 'retrieval_context', or you can create an "EvaluationDataset" of multiple sets of input, actual_output, expected_output, and retrieval_context.
    - I was able to input a csv file and create an EvaluationDataset fairly easily.

- Custom/local model integration:
  - Non Open-AI models require a custom LLM class and a custom embeddings class for using DeepEval, inheriting from DeepEvalBaseLLM and DeepEvalBaseEmbeddingModel, respectively. This wasn't too difficult to set up.

- Performance results:
  - To get results, you can either:
    1) Iterate through LLMTestCases (evaldataset.test_cases) - Looping through test cases seems to work a bit better than option #2, as errors encountered with evaluate() cause no results to be returned, and you can save partial results with looping.
    2) Try to use the evaluate() function to iterate through test cases in bulk. Despite this not returning partial results if you hit an error, it did work for my testing of my small manually curated dataset and a high throttle_value for rate limiting.

  - Note: In more recent versions of DeepEval, there is a "ConversationalTestCase", aka a list of conversation turns represented by a list of LLMTestCases. While an LLMTestCase represents an individual LLM system interaction, a ConversationalTestCase encapsulates a series of LLMTestCases that make up an LLM-based conversation. This could be useful for evaluating a conversation between a user and an LLM-based chatbot. I did not test this capability.

  - DeepEval metrics do not have example ranges to compare anything to, so below is my best guess.

  - Retrieval metrics:
    - Contextual Precision: Average: 0.776
      - Calculated fairly similarly (if not the same) as RAGAS's contextual precision
      - Manual review: Decent overall, but some definitely had the single answer needed higher ranked in the nodes list, yet received a low score (0.25), likely for giving extra context that was not as relevant.
      - Compared with RAGAS in DeepEval Contextual Precision: 0.493 (not close at all)
      - Compared with RAGAS package Contextual Precision: 0.6944 (closer to result here, but not to the above, which should be the same)

    - Contextual Recall: Average: 0.723
      - Calculated approximately the same way as RAGAS, according to their brief algorithm description
      - Manual review: Good overall, not great. For example, for the first test case, it said that a specific fact was not mentioned in the context when it actually was in the context. The output reasoning left a lot to be desired.
        - Example of poor output reason: "and the JSON output is formatted correctly to match the given schema" has nothing to do with the metric scoring for this metric
        - In another reason given for a perfect score, it gave additional context that is not found in the retrieval context; it seemed to more try to answer the prompt vs doing the evaluation of the metric.
      - Compared with RAGAS in DeepEval Contextual Recall: 0.627
      - Compared with RAGAS package Context Recall: 0.6455 (pretty close)

    - Contextual Relevance: "Measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval_context for a given input."
      - Unfortunately, I could not test this metric due to 429 Resource Exhausted Errors, regardless of rate limiting

  - Generation metrics:
    - Answer Relevancy: Average: 0.989
      - Calculated differently than RAGAS's answer relevancy
      - Manual review: Not great. If it's working, it seems to work fine generally. However some scores/reasons were VERY off, as not-relevant reasons for the scores were given and were inconsistent with the scoring.
        - Example 1: "The score is 1.00 because you provided an input, and asked for a JSON output. That's perfect! Let's keep going." 
        - Example 2: "Reason: The score is 1.00 because the input asks for a specific reason and expects a JSON object with that reason. The provided JSON does not contain any object to address the request, therefore the score is 1.00, implying that the output does not address the input."
      - Compared with RAGAS in DeepEval Answer Relevancy: 0.578 (not close at all to DeepEval, but makes sense with different algorithmic calculation)
      - Compared with RAGAS package Answer Relevancy: 0.6770 (pretty close to above, not to DeepEval)

    - Faithfulness: Average: 1.0
      - Calculated approximately the same way as RAGAS, according to their brief algorithm description
      - Manual review: This metric only returned perfect 1.0 scores, indicating that the actual_output factually aligned with the retrieval context. This seems mostly accurate except for the example where the actual_output definitely brought in outside knowledge. I suppose that technically doesn't factually contradict the retrieval_context, and that would fall under a "hallucination" instead. 
      - Compared with RAGAS in DeepEval Faithfulness: 0.984
      - Compared with RAGAS package Faithfulness': 1.0000 (close)

- Other notes:
  - DeepEval requires a json response. In practice, this has led to malformed json returned from the llm for metric evaluation, even with as simple of a schema as possible (i.e., return a string).
  - There's a flag you have to set initially for DeepEval not to track your usage.

ARES:

- Version 0.6.6

- Overview:
  - Integrates with several main LLMs + vllm; I was able to get it working with Google Gemini (not currently supported by the library) with moderate effort in adapting the source code for Gemini and installing locally

- Metrics: 

  - Context Relevance: Determines if the retrieved information is pertinent to the query.
  - Answer Faithfulness: Checks if the response generated by the language model is properly grounded in the retrieved context and does not include hallucinated or extraneous information.
  - Answer Relevance: Evaluates whether the generated response is relevant to the query, addressing all aspects of the question appropriately.

  - Of note, ARES also offers Prediction Powered Inference (PPI) to "significantly enhance evaluation accuracy when evaluating RAG systems". See the Performance results section for more details.

  - LLM-based metric: ARES uses a separate finetuned LLM to decide whether a query-document-answer triple matches the criteria for each metric. It uses a manually labeled dataset for each metric to evaluate improvement when tuning the LLM.

- Data generation:

  - Dataset generated included Query, Document, Answer, along with Context_Relevance, Answer_Faithfulness, and Answer_Relevance labels. Questions generated included positive, negative, and duplicate queries. 
  - Resulting questions were very, very basic (What was the date of the address? Who was mentioned at the beginning?) and only addressed the first part of the document, despite confirming that the entire document was provided.
  - After reviewing other sample datasets from ARES, ARES seems to perform better with shorter texts; would probably need to chuck these speeches to have more relevant/specific questions generated. ARES does not confirm how long a "document" should be, but in some of its example datasets, documents were only a couple sentences long.

- Required datasets - in tsv format:

  - A human preference validation set of annotated query, document, and answer triples for the evaluation criteria (e.g. context relevance, answer faithfulness, and/or answer relevance). ARES requires >50 examples but prefers several hundred. I used 58 examples for my labeled dataset. (labeled_dataset_sotu.tsv)
  - A set of labeled few-shot examples for scoring context relevance, answer faithfulness, and/or answer relevance in your RAG system. In my testing, I used 11 examples for this. (manual_dataset_complete_ares.tsv)	
  - A much larger set of unlabeled query-document-answer triples output by your RAG system for scoring. I created this by generating a synthetic dataset of ~800 examples with my RAG system & LLM (using DeepEval, see above). (unlabeled_dataset.tsv)

- Performance results:

  - From the paper: "Compared to RAGAS, ARES is 0.16 higher for context relevance and 0.15 higher for answer relevance, on average."

  - ARES offers two different RAG evaluation functions:
    1) ares.ues_idp() : After specifying the LLM, the unlabeled dataset, and the few-shot examples, ARES will return the Context Relevance, Answer Faithfulness, and Answer Relevance scores for the RAG system.
      - UES/IDP means evaluating an Unlabeled Evaluation Set in conjunction with In-Domain Prompts (UES/IDP)
      - From examining the code, the LLM uses all nodes in context to evaluate the relevancy score per example as 0 or 1 (as opposed to a score for each node), and all of the scores for the dataset are averaged to produce an overall metric score.
      - Note that when testing, 1 API call was produced per example/per metric.
    2) ares.evaluate_RAG() : By specifying an LLM judge, the unlabeled and labeled datasets, and the few-shot examples for the metrics of interest, ARES will use Prediction Powered Inference (PPI) to enhance the evaluation accuracy and to estimate a confidence interval for the quality of each RAG system.
      - From the paper: "PPI is a recent statistical method that provides tighter confidence intervals on a small set of annotated datapoints (i.e., our validation set) by leveraging predictions on a much larger set of non-annotated datapoints. PPI can leverage both the labeled datapoints and the ARES judge predictions on the non-annotated datapoints to construct confidence intervals for our RAG system’s performance."
      - When testing with the ARES provided testset, I got the following example results:

--------------------------------------------------
Context_Relevance_Label Scoring
ARES Ranking
Evaluation_Set:ARES_files/nq_unlabeled_output_100examples.tsv
Checkpoint:None
ARES Prediction: [0.6395238095238103]
ARES Confidence Interval: [[0.513, 0.766]]
Number of Examples in Evaluation Set: [70]
Ground Truth Performance: [0.686]
ARES LLM Judge Accuracy on Ground Truth Labels: [0.857]
Annotated Examples used for PPI: 300
--------------------------------------------------

[[{'Label_Column': 'Context_Relevance_Label', 'Evaluation_Set': 'ARES_files/nq_unlabeled_output_100examples.tsv', 'ARES_Prediction': 0.6395238095238103, 'ARES_Confidence_Interval': [0.513, 0.766], 'Number_of_Examples_in_Evaluation_Set': 70, 'Ground_Truth_Performance': 0.686, 'ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels': 0.857, 'Annotated_Examples_used_for_PPI': 300}]]

      - I like getting the confidence interval, and you can specify at what % confidence you would like it to run. Default = 0.95 (alpha = 0.5 default).


- Other notes:
  - Code version 0.6.6 references ARES code that doesn't exist in the repo, and I had to edit the library not to import it / use it.
    - LLMJudge_RAG_Compared_Scoring.py : from ares.LLM_as_a_Judge_Adaptation.Late_Chunking_Classifier import CustomClassifier, get_late_chunked_embeddings, get_query_embedding
  - Code version 0.6.6 PPI run with LLMJudge requires labels of '[[Yes]]' and '[[No]]' for metrics, yet the code in LLMJudge_RAG_Compared_Scoring.py later tries to evaluate the labels as integers. Edited locally



Head to head comparison for Contextual Precision / Context Relevance:

Dataset: datasets/unlabeled_dataset/unlabeled_dataset_deepeval.csv

- RAGAS: 
For large batch run with rate-limiting issues, the mean Contextual Precision score on the unlabeled dataset is 0.3405. That only gave 267/800 scores, with the rest as NA. Will rerun in small batches; this probably happened because of the rate limiting.

For performing small batch runs to avoid rate-limiting issues, the mean Contextual Precision score on the unlabeled dataset is 0.3578320194131311. 

Note: 364/800 scores were given a contextual precision score of 0. If you average the remaining 436 non-zero scores, the mean Contextual Precision score is 0.6565725126846442. 

Prompt: "Given question, answer and context verify if the context was useful in arriving at the given answer. Give verdict as "1" if useful and "0" if not with json output."

 examples=[
        {
            "question": """What can you tell me about albert Albert Einstein?""",
            "context": """Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, he also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century. His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called "the world's most famous equation". He received the 1921 Nobel Prize in Physics "for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect", a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time. His intellectual achievements and originality have made Einstein synonymous with genius.""",
            "answer": """Albert Einstein born in 14 March 1879 was German-born theoretical physicist, widely held to be one of the greatest and most influential scientists of all time. He received the 1921 Nobel Prize in Physics for his services to theoretical physics. He published 4 papers in 1905. Einstein moved to Switzerland in 1895""",
            "verification": ContextPrecisionVerification(
                reason="The provided context was indeed useful in arriving at the given answer. The context includes key information about Albert Einstein's life and contributions, which are reflected in the answer.",
                verdict=1,
            ).dict(),
        },
        {
            "question": """who won 2020 icc world cup?""",
            "context": """The 2022 ICC Men's T20 World Cup, held from October 16 to November 13, 2022, in Australia, was the eighth edition of the tournament. Originally scheduled for 2020, it was postponed due to the COVID-19 pandemic. England emerged victorious, defeating Pakistan by five wickets in the final to clinch their second ICC Men's T20 World Cup title.""",
            "answer": """England""",
            "verification": ContextPrecisionVerification(
                reason="the context was useful in clarifying the situation regarding the 2020 ICC World Cup and indicating that England was the winner of the tournament that was intended to be held in 2020 but actually took place in 2022.",
                verdict=1,
            ).dict(),
        },
        {
            "question": """What is the tallest mountain in the world?""",
            "context": """The Andes is the longest continental mountain range in the world, located in South America. It stretches across seven countries and features many of the highest peaks in the Western Hemisphere. The range is known for its diverse ecosystems, including the high-altitude Andean Plateau and the Amazon rainforest.""",
            "answer": """Mount Everest.""",
            "verification": ContextPrecisionVerification(
                reason="the provided context discusses the Andes mountain range, which, while impressive, does not include Mount Everest or directly relate to the question about the world's tallest mountain.",
                verdict=0,
            ).dict(),
        },


- DeepEval:
Average score for Contextual Precision run on unlabeled dataset is 0.8642868215033659
DeepEval gives explanations for each score given to an example in the dataset. The explanations given are all generated by the LLM, and they can be quite specific and reasonable, but it's still difficult to distinguish between scores that are .1-.3 apart based off of the explanations or why exactly a given numerical value was chosen. I'm not sure if it's worth generating these explanations; it may be situation dependent for if they are wanted.

Prompt: 
Given the input, expected output, and retrieval context, please generate a list of JSON objects to determine whether each node in the retrieval context was remotely useful in arriving at the expected output.

**
IMPORTANT: Please make sure to only return in JSON format, with the 'verdicts' key as a list of JSON. These JSON only contain the `verdict` key that outputs only 'yes' or 'no', and a `reason` key to justify the verdict. In your reason, you should aim to quote parts of the context.
Example Retrieval Context: ["Einstein won the Nobel Prize for his discovery of the photoelectric effect", "He won the Nobel Prize in 1968.", "There was a cat."]
Example Input: "Who won the Nobel Prize in 1968 and for what?"
Example Expected Output: "Einstein won the Nobel Prize in 1968 for his discovery of the photoelectric effect."

Example:
{{
    "verdicts": [
        {{
            "verdict": "yes",
            "reason": "It clearly addresses the question by stating that 'Einstein won the Nobel Prize for his discovery of the photoelectric effect.'"
        }},
        {{
            "verdict": "yes",
            "reason": "The text verifies that the prize was indeed won in 1968."
        }},
        {{
            "verdict": "no",
            "reason": "'There was a cat' is not at all relevant to the topic of winning a Nobel Prize."
        }}
    ]  
}}
Since you are going to generate a verdict for each context, the number of 'verdicts' SHOULD BE STRICTLY EQUAL to that of the contexts.
**

				

- ARES:
Context_Relevance_Label Scoring with PPI:
ARES Ranking
Evaluation_Set: datasets/unlabeled_dataset/unlabeled_dataset.tsv
Checkpoint: None
ARES Prediction: 0.7324999999999967
ARES Confidence Interval: 	
Number of Examples in Evaluation Set: 800
ARES LLM Judge Accuracy on Ground Truth Labels: 0.695
Annotated Examples used for PPI: 59

Context_Relevance, Answer Faithfulness, and Answer Relevance scoring with ues_idp: 
{'Context Relevance Scores': 0.879, 'Answer Faithfulness Scores': 0.84, 'Answer Relevance Scores': 0.821}


context_relevance_system_prompt = (
    "You are an expert dialogue agent. "
    "Your task is to analyze the provided document and determine whether it is relevant for responding to the dialogue. "
    "In your evaluation, you should consider the content of the document and how it relates to the provided dialogue. "
    'Output your final verdict by strictly following this format: "[[Yes]]" if the document is relevant and "[[No]]" if the document provided is not relevant. '
    "Do not provide any additional explanation for your decision.\n\n"




Head to head comparison for Contextual Precision / Context Relevance and Faithfulness:

Version 1:

Dataset: first 903 examples of datasets/rag_mini_wikipedia_complete_chat.csv (originally ran all 918 examples for RAGAS and DeepEval, then had to remove and label the results for the last 15 examples for the ARES run, as I had forgotten ARES requires labeled examples)
LLM: Gemini 1.5 Flash for both RAG and metric eval

RAGAS:

Average Contextual Precision Score (903 examples): 0.7714837243775122
Average Faithfulness Score (899 examples, as 4 examples did not produce results): 0.8667681244698191
- Index #199, 450, 533, 548 gave this message: "No statements were generated from the answer." and couldn't calculate a score, despite multiple reruns.

DeepEval:

Average Contextual Precision Score (903 examples): 0.792454
Average Faithfulness Score (903 examples): 0.958

ARES:

Average Context Relevance Score (903 examples): 0.711
Average Answer Faithfulness Score (903 examples): 0.692
Average Answer Relevance Score (903 examples): 0.694 

Note that probably 5 examples on the ARES run had API rate limit errors, but the end result still gave "Number of times did not extract Yes or No: 0"


Version 2:

Dataset: first 903 examples of datasets/rag_mini_wikipedia_complete_chat.csv (originally ran all 918 examples for RAGAS and DeepEval, then had to remove and label the results for the last 15 examples for the ARES run, as I had forgotten ARES requires labeled examples)
LLM: Gemini 2.0 Flash Experimental for metric eval, Gemini 1.5 Flash for RAG/dataset answers and context

RAGAS:

Average Faithfulness Score: 0.8887978046437826

DeepEval:

Average Contextual Precision Score: 0.756079
Average Faithfulness Score: 0.967882

ARES:

Average Context Relevance Score: 0.852
Answer Faithfulness Score: 0.792
Answer Relevance Score: 0.779

Note that probably 12 examples on the ARES run had API rate limit errors, but the end result still gave "Number of times did not extract Yes or No: 0"

?s from team:
- diff retriever and generator models
- try diff llm retrieval v metric llm
- try open source rag dataset

Result from above: Things to try for comparison:
- changing dataset from DeepEval generated dataset to a neutral created dataset (found on HuggingFace: rag_mini_wikipedia_complete_chat.csv )
- change LLM from Gemini to using the chat LLM (used for answer generation for rag_mini_wikipedia_complete_chat.csv)
- using different LLM for judging metrics : using models/gemini-2.0-flash-exp




Other notes:
- While the calculation for RAGAS and DeepEval for the Contextual Precision metric appears to be the same (ask the LLM to judge whether a context node is useful for the query, then calculate weighted cumulative precision), and the prompts for the LLM for each library are almost the same, there are variable inputs into the queries/prompts fed into the LLMs that impact the number of API calls that are made for each library. For example, RAGAS sends 1 API call/prompt query for each context node to judge whether it is useful the query (i.e., 10 context nodes or 1 example = 10 API calls to the LLM). In contrast, DeepEval sends all of the context nodes for evaluation all in the same API call/LLM query (assuming a large enough input token limit). DeepEval later calculates the result by parsing through the output from the 1 API request accordingly.
  - This is not readily apparent in the code/prompts/documentation when I was searching for why the libraries had such a different number of requests; it took a lot of testing and digging into the code to find this difference.
- Number of API calls per metric per library (there is no documentation on this that I could find; this are the results through my testing):
  - RAGAS:
    - Contextual Precision: 1 API call per context node per example
  - DeepEval:
    - Contextual Precision: 1 API call for the metric, 1 optional API call for a reason (which can be turned off as a flag)
  


Overview of results from Results_Analysis.ipynb:

Across libraries:
- 

Across judge LLMs:

ARES:
- Results did vary a good amount in the average total when changing the judge LLM: 
  - 0.1 (Faithfulness) and 0.141 (Context Precision) difference in the average score

DeepEval: 
- Results did not vary much for the average totals when changing the judge LLM:	
  - 0.01 (Faithfulness) and 0.036 (Context Precision) difference in the average score

- Results did vary significantly on an example-by-example basis for Context Precision but not very much for Faithfulness:
  - 91% of Faithfulness scores were within 0.1 of each other on an example-by-example basis
  - 66% of Context Precision scores were within 0.1 of each other on an example-by-example basis
  - 75% of Context Precision scores were within 0.2 of each other on an example-by-example basis
  - Average difference between CP results was 0.145; between Faithfulness results was 0.053

RAGAS:
- Results varied very little for the average totals when changing the judge LLM:
  - 0.02 (Faithfulness) and 0.001 (Context Precision) difference in the average scores

- Results did vary on an example-by-example basis for both metrics
  - 81% of Faithfulness scores were within 0.1 of each other on an example-by-example basis
  - 84% of Faithfulness scores were within 0.2 of each other on an example-by-example basis
  - 79% of Context Precision scores were within 0.1 of each other on an example-by-example basis
  - 87% of Context Precision scores were within 0.2 of each other on an example-by-example basis
