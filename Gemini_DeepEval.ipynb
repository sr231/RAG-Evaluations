{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e651c99a-fc6c-4b3e-927e-5b7dae9813fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG with Gemini Flash 1.5 LLM and DeepEval evaluation\n",
    "# Google Gemini: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# DeepEval: https://docs.confident-ai.com/docs/guides-rag-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dafebe7-76cf-41d8-8734-0f06d15b9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish RAG pipeline with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbdda733-b425-42f1-831a-6d71952ac5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from typing import List # , Optional\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "import deepeval\n",
    "from deepeval.models import DeepEvalBaseLLM, DeepEvalBaseEmbeddingModel\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval import evaluate\n",
    "from deepeval.evaluate import TestResult, print_test_result\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    FaithfulnessMetric\n",
    ")\n",
    "from deepeval.metrics.ragas import (\n",
    "    RagasMetric,\n",
    "    RAGASAnswerRelevancyMetric,\n",
    "    RAGASFaithfulnessMetric, \n",
    "    RAGASContextualRecallMetric,\n",
    "    RAGASContextualPrecisionMetric,\n",
    "    RAGASContextualRelevancyMetric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8297c3a4-1366-4e38-9a4c-10f22905e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental variable to opt out of DeepEval tracking telemetry data\n",
    "os.environ[\"DEEPEVAL_TELEMETRY_OPT_OUT\"] = \"YES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03796ba-74cc-4bad-99a2-a594b3898399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepeval.telemetry_opt_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90ecc13-1002-4dad-82e5-cf42b656718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc3144c-2ac2-4448-a86a-a514ee26b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up local API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c3cd9fd-5949-4b1e-9894-c26618049ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eab73af-ed30-4840-be7b-20eb1c66807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document database\n",
    "# using 4 State of the Union speeches, all text from whitehouse.gov briefing room speeches posted online, including a title with the date of the speech\n",
    "# Example from 2024:\n",
    "# https://www.whitehouse.gov/briefing-room/speeches-remarks/2024/03/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery-2/\n",
    "sotu = []\n",
    "newfiles = [\"./Speeches/titleedits/state_of_the_union_042921.txt\", \"./Speeches/titleedits/state_of_the_union_030122.txt\", \"./Speeches/titleedits/state_of_the_union_020723.txt\", \"./Speeches/titleedits/state_of_the_union_030724.txt\"]\n",
    "for i in newfiles:\n",
    "    with open(i) as file:\n",
    "        for line in file:\n",
    "            nl = line.rstrip()\n",
    "            if nl != '':\n",
    "                sotu.append(nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fcdf1fa-c3c4-4f71-9585-a96bc9c02c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(text=line) for line in sotu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab816039-1c34-4073-84a6-e57ad84832e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='d360b6f4-541f-4577-9fe6-a576913b36a1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='May God protect our troops.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a loaded Document line\n",
    "documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41cce1c9-8df4-492b-9d15-18dd67cf3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Set up the faiss index\n",
    "d = 768 # dimensions of the input vector of the embedding model that we're going to use; in this case, the google embedding model\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "print(faiss_index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e48c5f84-8fc4-4970-9505-3c67705b151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the embeddings\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # optional: task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "Settings.embed_model = doc_embeddings\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82ce1780-061d-4d6d-bf3c-6853db1eb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment for when you need to re-embed and vectorize documents\n",
    "## otherwise, doing local loading below\n",
    "#vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "#storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#index = VectorStoreIndex.from_documents(\n",
    "#    documents, storage_context=storage_context, show_progress=True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "433fb37c-fb44-409c-8b42-066cf7a8e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save index to disk\n",
    "#index.storage_context.persist()\n",
    "#index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77d046f3-293f-4bac-be0c-bedc5d88012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "# index id 'cef7ae30-ff1e-404a-bce6-85d59ca4b376' uses the speeches with a title that includes the date it was given\n",
    "index = load_index_from_storage(storage_context=storage_context, index_id='cef7ae30-ff1e-404a-bce6-85d59ca4b376')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1ba96f-37e3-4160-bf13-5536c2cbaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query and chat engines\n",
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb6cee44-9396-41b0-b9f8-e6fd633fef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query and response\n",
    "query = \"In detail, what has the President done to improve the economy over the four years of his speeches?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48698c67-efba-4fe3-a2e1-864a3f43131b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The President highlights the creation of over 1.3 million jobs in the first 100 days of his term, a record 12 million jobs created in two years, and a strong economic growth rate of 5.7% in the previous year. He also emphasizes the International Monetary Fund's prediction of an economic growth rate exceeding 6% for the current year. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05980859-d7e3-44e8-bc4f-10a41fdd111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of DeepEval implementation, following their guide for RAG\n",
    "# https://docs.confident-ai.com/docs/guides-rag-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "002bf206-84b1-4bae-ad0c-d23dad165360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval requires a json response. In practice, this has led to malformed json returned from the llm, even with as simple of a schema as this\n",
    "class Response(BaseModel):\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad054ba9-9fb0-4e3d-9f48-7e60eb379ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Open-AI requieres a custom LLM class for using DeepEval\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel: \n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Gemini 1.5 Flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b903c42-fc2d-4fe2-bb1e-faaa7de1f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly, a custom embedding model class is required for non Open-AI embeddings\n",
    "class CustomGeminiEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load_model(self):\n",
    "        return GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\"\n",
    "        )\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_query(text)\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_query(text)\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_documents(texts)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        \"Custom Gemini Embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e11dcd46-4410-4ae9-bb68-3a386d6484c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_geminiflash = CustomGeminiFlash()\n",
    "custom_geminiembeddings = CustomGeminiEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1da71503-d604-4936-a243-c3d30b6461f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_precision = ContextualPrecisionMetric(model=custom_geminiflash)\n",
    "contextual_recall = ContextualRecallMetric(model=custom_geminiflash)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=custom_geminiflash)\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What measures does the speaker propose to lower prescription drug costs in America?\",\n",
    "    actual_output=\"The speaker proposes giving Medicare the power to negotiate lower prescription drug prices, capping prescription drug costs at $2,000 a year for everyone, and allowing Medicare to negotiate lower prices for 500 drugs over the next decade.s\",\n",
    "    expected_output=\"The speaker proposes that Medicare should be given the power to negotiate lower drug prescription prices. They argue that this would save hundreds of billions of dollars and lower prescription drug costs for everyone. The speaker also states that the money saved could be used to strengthen the Affordable Care Act and expand Medicare coverage benefits without costing taxpayers an additional penny.\",\n",
    "    retrieval_context=['Letâ€™s do what weâ€™ve always talked about for all the years I was down here in this â€” in this body â€” in Congress.  Letâ€™s give Medicare the power to save hundreds of billions of dollars by negotiating lower drug prescription prices.  (Applause.)', 'In fact, we pay the highest prescription drug prices of anywhere in the world right here in America â€” nearly three times â€” for the same drug, nearly three times what other countries pay.  We have to change that, and we can.', 'And weâ€™re finally giving Medicare the power to negotiate drug prices. Bringing down prescription drug costs doesnâ€™t just save seniors money.', 'For years people have talked about it but I finally got it done and gave Medicare the power to negotiate lower prices for prescription drugs just like the VA does for our veterans.', 'And, by the way, that wonâ€™t just â€” that wonâ€™t just help people on Medicare; it will lower prescription drug costs for everyone.', 'Now I want to cap prescription drug costs at $2,000 a year for everyone!', 'We know how to do this.  The last President had that as an objective.  We all know how outrageously expensive drugs are in America.', 'Make no mistake, if you try to do anything to raise the cost of prescription drugs, I will veto it.', 'Now itâ€™s time to go further and give Medicare the power to negotiate lower prices for 500 drugs over the next decade.', 'It will cut the federal deficit, saving tax payers hundreds of billions of dollars on the prescription drugs the government buys for Medicare.']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ef1ea0f-8367-4747-84eb-f16f79895d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Precision Score:  0.9472222222222222\n",
      "Contextual Precision Reason:  The score is 0.95 because the first five nodes in the retrieval context correctly identify the speaker's proposals to lower prescription drug costs. However, the eighth node is a threat, not a proposal, and the seventh node is a general statement about the high cost of drugs in America, which is irrelevant to the speaker's specific proposals. This places two irrelevant nodes before four relevant nodes, which slightly lowers the contextual precision score.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Recall Score:  0.7\n",
      "Contextual Recall Reason:  The score is 0.7 because the first sentence of the expected output is fully supported by the first node in retrieval context. However, the remaining sentences of the expected output are only partially supported by the first node, as the node only explicitly mentions 'save hundreds of billions of dollars' and does not explicitly state that these savings would be used to strengthen the Affordable Care Act or expand Medicare coverage without costing taxpayers an additional penny.  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Relevancy Score:  0.5\n",
      "Contextual Relevancy Reason:  The score is 0.50 because the context discusses prescription drug costs in America but doesn't directly propose specific measures to lower them, only stating the proposal will help people on Medicare and lower costs for everyone.  It also mentions saving taxpayer money but lacks concrete measures. \"The context discusses the high cost of drugs in America but does not offer any specific measures to lower prescription drug costs.\"\n"
     ]
    }
   ],
   "source": [
    "# Example of measuring metrics individually for one test_case\n",
    "\n",
    "# Retrieval metrics:\n",
    "contextual_precision.measure(test_case)\n",
    "print(\"Contextual Precision Score: \", contextual_precision.score)\n",
    "print(\"Contextual Precision Reason: \", contextual_precision.reason)\n",
    "\n",
    "contextual_recall.measure(test_case)\n",
    "print(\"Contextual Recall Score: \", contextual_recall.score)\n",
    "print(\"Contextual Recall Reason: \", contextual_recall.reason)\n",
    "\n",
    "contextual_relevancy.measure(test_case)\n",
    "print(\"Contextual Relevancy Score: \", contextual_relevancy.score)\n",
    "print(\"Contextual Relevancy Reason: \", contextual_relevancy.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cd1ac8d-1f6f-47bb-87df-87528474486a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Relevancy Score:  1.0\n",
      "Answer Relevancy Reason:  The speaker does not propose any measures to lower prescription drug costs in America.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score:  1.0\n",
      "Faithfulness Reason:  This is a perfect example of flawless output! Keep up the fantastic work!\n"
     ]
    }
   ],
   "source": [
    "# Generation metrics:\n",
    "answer_relevancy = AnswerRelevancyMetric(model=custom_geminiflash)\n",
    "faithfulness = FaithfulnessMetric(model=custom_geminiflash)\n",
    "                                 \n",
    "answer_relevancy.measure(test_case)\n",
    "print(\"Answer Relevancy Score: \", answer_relevancy.score)\n",
    "print(\"Answer Relevancy Reason: \", answer_relevancy.reason)\n",
    "\n",
    "faithfulness.measure(test_case)\n",
    "print(\"Faithfulness Score: \", faithfulness.score)\n",
    "print(\"Faithfulness Reason: \", faithfulness.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c32b6c3f-c08a-439f-9996-d33d20ca6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of measuring metrics in bulk for multiple test_cases / a full dataset\n",
    "\n",
    "# Load manually curated dataset\n",
    "evaldataset = EvaluationDataset()\n",
    "evaldataset.add_test_cases_from_csv_file(\n",
    "    file_path=\"datasets/manual_dataset_complete.csv\",\n",
    "    input_col_name=\"Input\",\n",
    "    actual_output_col_name=\"Actual_Output\",\n",
    "    expected_output_col_name=\"Expected_Output\",\n",
    "#    context_col_name=\"context\",\n",
    "#    context_col_delimiter= \",\",\n",
    "    retrieval_context_col_name=\"Retrieval_Context\",\n",
    "    retrieval_context_col_delimiter= \",\"\n",
    "#    additional_metadata_col_name=\"source_file\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7d54857-001b-403c-9838-01082c365a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For RAG systems, DeepEval recommends the following metrics:\n",
    "# Retriever metrics:\n",
    "contextual_precision = ContextualPrecisionMetric(model=custom_geminiflash)\n",
    "contextual_recall = ContextualRecallMetric(model=custom_geminiflash)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=custom_geminiflash) # this was the only metric that would not work on the manually curated dataset (429 errors)\n",
    "\n",
    "# Generation metrics:\n",
    "answer_relevancy = AnswerRelevancyMetric(model=custom_geminiflash)\n",
    "faithfulness = FaithfulnessMetric(model=custom_geminiflash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84960d-d52d-4178-8ef0-088e818b71db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 Options for Metrics Evaluation: \n",
    "\n",
    "# 1) Iterating through test cases seems to work better than bulk evaluation with evaluate,\n",
    "# as errors encountered with evaluate(...) cause no results to be returned\n",
    "# Looping at least saves partial results until an error occurs\n",
    "# Encountered this sometimes with contextual_relevancy and contextual_precision on the test dataset (429 errors or Invalid JSON errors),\n",
    "# yet typically was fine if iterated through individual test_cases\n",
    "# For future: https://github.com/confident-ai/deepeval/issues/964 may assist with incorrect json errors like what was being returned\n",
    "\n",
    "# Example for faithfulness metric\n",
    "# faithfulness_results = []\n",
    "# for i in range(len(evaldataset.test_cases)):\n",
    "#     eval_faithfulness = evaluate(test_cases=[evaldataset.test_cases[i]], metrics=[faithfulness], throttle_value=90)\n",
    "#     faithfulness_results.append(eval_faithfulness[0])\n",
    "\n",
    "# 2) Evaluate through test_cases in bulk; In testing, at least faithfulness, contextual_precision metrics worked this way with the manually curated dataset\n",
    "\n",
    "# bulk evaluation of test_cases; throttle_value is for rate limiting- in seconds between queries\n",
    "test_precision = evaluate(test_cases=evaldataset.test_cases, metrics=[contextual_precision], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd280f4f-5ec1-405b-9e3f-062f1908a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick print for results after evaluation, as needed\n",
    "for i in [test_precision]:\n",
    "    for j in i:\n",
    "        if type(j) == TestResult:\n",
    "            print_test_result(j)\n",
    "        else:\n",
    "            print_test_result(j[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63c2fc3a-e543-4abe-9a17-5bf68f2933c0",
   "metadata": {},
   "source": [
    "Results/Notes on DeepEval metrics:\n",
    "\n",
    "- Contextual Precision: Average: 0.776\n",
    "  - Calculated fairly similarly (if not the same) as RAGAS's contextual precision; would need to look into code further\n",
    "  - Manual review: Decent overall, but some definitely had the single answer needed higher ranked in the nodes list, yet received a low score (0.25), likely for giving extra context that was not as relevant.\n",
    "  - Compared with RAGAS in DeepEval Contextual Precision: 0.493 (not close at all)\n",
    "  - Compared with RAGAS package Contextual Precision: 0.6944 (closer to result here, but not to the above, which should be the same)\n",
    "\n",
    "- Contextual Recall: Average: 0.723\n",
    "  - Calculated approximately the same way as RAGAS, according to their brief algorithm description\n",
    "  - Manual review: Good overall, not great. For example, for the first test case, it said that a specific fact was not mentioned in the context when it actually was in the context.\n",
    "    - Also, \"and the JSON output is formatted correctly to match the given schema\" does not have anything to do with this metric, so this being included in a reasoning behind a score for this metric does not make sense.\n",
    "    - Another speaks for itself: \"The score is 1.00 because you provided an input, and asked for a JSON output. That's perfect! Let's keep going.\"\n",
    "    - In another reason given for a perfect score, it gave additional context that is not found in the retrieval context; it seemed to more try to answer the prompt vs doing the evaluation of the metric.\n",
    "  - Compared with RAGAS in DeepEval Contextual Recall: 0.627\n",
    "  - Compared with RAGAS package Context Recall: 0.6455 (pretty close)\n",
    "\n",
    "- Answer Relevancy: Average: 0.989\n",
    "  - Calculated differently than RAGAS's answer relevancy\n",
    "  - Manual review: Not great. If it's working, it seems to work fine generally. However some scores/reasons were VERY off, as we received not-as-relevant reasons for the scores such as \"The score is 1.00 because you provided an input, and asked for a JSON output. That's perfect! Let's keep going.\" \n",
    "    - In another example, this 1.0 perfect score output was for an example where the LLM returned an empty response: \"Reason: The score is 1.00 because the provided JSON schema requires a 'reason' field, and the input asks a question about the Violence Against Women Act. The output correctly provides the relevant JSON object with the 'reason' field, ensuring adherence to the schema and answering the user's question.\"\n",
    "    - I think the results returned may have gotten mixed up, as the next example, which had a response from the LLM, stated: \"Reason: The score is 1.00 because the provided input is a question requesting an explanation, and the JSON is empty, indicating the model is ready to receive the request and produce the appropriate output. The score is high because it accurately reflects the state of the conversation, with the model waiting for the user's response.\"\n",
    "    - Another: \"Reason: The score is 1.00 because the input asks for a specific reason and expects a JSON object with that reason. The provided JSON does not contain any object to address the request, therefore the score is 1.00, implying that the output does not address the input.\"\n",
    "  - Compared with RAGAS in DeepEval Answer Relevancy: 0.578 (not close at all to DeepEval, but makes sense with different algorithmic calculation)\n",
    "  - Compared with RAGAS package Answer Relevancy: 0.6770 (pretty close to above, not to DeepEval)\n",
    "\n",
    "- Faithfulness: Average: 1.0\n",
    "  - Calculated approximately the same way as RAGAS, according to their brief algorithm description\n",
    "  - Manual review: This metric only returned perfect 1.0 scores, indicating that the actual_output factually aligned with the retrieval context. This seems mostly accurate except for the example where the actual_output definitely brought in outside knowledge. I suppose that technically doesn't factually contradict the retrieval_context, and that would fall under a \"hallucination\" instead. \n",
    "  - Compared with RAGAS in DeepEval Faithfulness: 0.984\n",
    "  - Compared with RAGAS package Faithfulness': 1.0000 (close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0128ad6f-1eac-4d26-80b2-c3df8ace688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test RAGAS metrics\n",
    "\n",
    "# Unforuntately the ragas metric only accept langhcain chat models... so the class for the Gemini DeepEvalBaseLLM will not work with these metrics\n",
    "# Need to use our langchainllm created earlier:  \n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "# doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # optional: task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "\n",
    "ragasmetric = RagasMetric(model=llm, embeddings=doc_embeddings)\n",
    "ragas_ar = RAGASAnswerRelevancyMetric(model=llm, embeddings=doc_embeddings)\n",
    "ragas_f = RAGASFaithfulnessMetric(model=llm)\n",
    "ragas_crecall = RAGASContextualRecallMetric(model=llm)\n",
    "ragas_cp = RAGASContextualPrecisionMetric(model=llm)\n",
    "ragas_crel = RAGASContextualRelevancyMetric(model=llm) # this metric did not work in testing; returned errors related to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f9317-d292-48a2-9e77-72ff0671c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of evaluation for all ragas metrics\n",
    "eval_ragas = evaluate(test_cases=evaldataset.test_cases, metrics=[ragasmetric], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3feaa-39c5-4a55-bf28-54ca1a5a5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of evaluation for each ragas metric individually\n",
    "eval_ragas_f = evaluate(test_cases=evaldataset.test_cases, metrics=[ragas_f], throttle_value=90)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b6f39ca-8d15-4f9f-9d36-d2378b9b0655",
   "metadata": {},
   "source": [
    "Individual metric results from ragas package:\n",
    "{'context_precision': 0.6944, 'faithfulness': 1.0000, 'answer_relevancy': 0.6770, 'context_recall': 0.6455}\n",
    "\n",
    "Summary results from deepeval:\n",
    "context_precision: 90.91% pass rate, faithfulness: 81.82% pass rate, answer_relevancy: 81.82% pass rate, context_recall: 81.82% pass rate, RAGAS: 54.55% pass rate\n",
    "\n",
    "Overall: Similar but not the same scores between deepeval's ragas (which calls ragas) and ragas. \n",
    "I think the variance is probably fairly normal, given that I got different results each time I ran ragas as well.\n",
    "\n",
    "Average scores from deep eval ragas:\n",
    "context precision: 0.493268741183316\n",
    "faithfulness: 0.984126984126984\n",
    "answer_relevancy: 0.577747493385347\n",
    "context_recall: 0.627272727272727\n",
    "\n",
    "Notes from manual review of results: \n",
    "- Faithfulness scoring seemed fairly reliable. \n",
    "- Answer relevancy also seemed fairly accurate. Some of the scoring for answer relevancy was difficult to determine why one received a better score than another, but for those in question, the score was only off by 0.1 from what I expected based off of the other scoring.\n",
    "- Some of the context_recall results seemed inaccurate. For example, one returned a score of 0 but in close examination, the context did contain parts of the ground truth result. For two very similar examples, scores varied from 0 to 0.5, which seemed like a dramatic variance.\n",
    "- Contextual precision seemed a bit off, as some of the 1st and 2nd context nodes contained the exact answer, yet the score was 0.6 or 0.5.\n",
    "\n",
    "\n",
    "Output from RAGAS metric (average of all 4 RAGAS individual metrics above):\n",
    "- Output seemed inconsistent, as RAGAS metric could not be generated for 3 examples that we had results from the other 4 metrics on. Also unexpectedly, it was generated for one example that we didn't get all 4 individual results for.\n",
    "- For those examples that the metric was generated on, the RAGAS metric was within .12 of the manually calculated averages. \n",
    "- This metric (when it gives output) seems like a general approximation for a given test case of how well the RAG pipeline answered it.\n",
    "- The overall pass rate seems useful, but given that we got 7/11 results (including 1 questionable one) on such a small test set, I don't think I would use this metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "654da851-9125-4c34-b1e9-19add3a09b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a synthetic dataset of \"Goldens\" (aka a dataset with 'input', 'context', 'source_file' columns -- not 'Retrieval_Context') with DeepEval\n",
    "dataset = EvaluationDataset()\n",
    "synthesizer = Synthesizer(model=custom_geminiflash, embedder=custom_geminiembeddings)\n",
    "dataset.generate_goldens_from_docs(\n",
    "    synthesizer=synthesizer,\n",
    "    document_paths=['Speeches/titleedits/state_of_the_union_042921.txt', 'Speeches/titleedits/state_of_the_union_030122.txt', \n",
    "                    'Speeches/titleedits/state_of_the_union_020723.txt', 'Speeches/titleedits/state_of_the_union_030724.txt'],\n",
    "    max_goldens_per_document=3\n",
    ")\n",
    "\n",
    "dataset.save_as(file_type=\"csv\", directory=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf11109d-56af-4ab5-b434-c19f8ff960bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66aaad79-e985-4661-a630-d4a49e584492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional ways to form json response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b496f0a-5e7b-4730-ad59-65ea763361d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e8eaa77-27ab-43eb-adce-5e89dfc96aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(test, User)\n",
    "assert test.resp == \"Tiki\"\n",
    "assert test.age == 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f606c-f1ed-4c0d-98a4-a2ba04bd0b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research2",
   "language": "python",
   "name": "research2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
