{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd552471-bc05-4749-b207-9be5e7e2d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook uses the Google Gemini API (free tier, local API key) \n",
    "# and the RAGAS evaluation library to evaluate several metrics for a RAG pipeline\n",
    "# Google Gemini: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# RAGAS: https://docs.ragas.io/en/stable/\n",
    "# Note: This notebook is with RAGAS 2-12 (last updated: 01/12/25) and Python 3.11.8\n",
    "\n",
    "# Note for earlier RAGAS versions; did not need this for RAGAS 2-12: \n",
    "# I had to edit underlying RAGAS library (cloned locally, edited files, then pip -e installed locally) for this issue re: temperature with Gemini:\n",
    "# https://github.com/explodinggradients/ragas/pull/657/files\n",
    "# https://github.com/explodinggradients/ragas/issues/678\n",
    "# Edits simply remove the temperature variable, see notes.txt for more specific info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44922bc8-8feb-44cd-9541-84618200beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics\n",
    "\n",
    "# Faithfulness - Measures the factual consistency of the answer to the context based on the question.\n",
    "# Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.\n",
    "# Answer_relevancy - Measures how relevant the answer is to the question.\n",
    "# Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.\n",
    "\n",
    "# Faithfulness with HHEM - Similar to Faithfulness but uses a HuggingFace model (Vectara's HHEM 2.1 classifier) to detect hallucinations\n",
    "# https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html#faithfullness-with-hhem-2-1-model\n",
    "# https://huggingface.co/vectara/hallucination_evaluation_model\n",
    "\n",
    "# RAGAS has other metrics as well : https://docs.ragas.io/en/latest/concepts/metrics/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f13f959-5e0b-4e76-b116-da5da6a04ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set do not track variable to RAGAS\n",
    "# more info: https://github.com/explodinggradients/ragas/issues/49\n",
    "import os\n",
    "os.environ[\"RAGAS_DO_NOT_TRACK\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30fd81d-ce93-4e03-bbec-862ece105b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import google.generativeai as genai\n",
    "import textwrap\n",
    "import ast\n",
    "import time\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import ragas\n",
    "from ragas.testset import TestsetGenerator\n",
    "#from ragas.testset.evolution import simple, reasoning, multi_context\n",
    "from ragas.run_config import RunConfig\n",
    "#from ragas.metrics import (\n",
    "#    answer_relevancy,\n",
    "#    faithfulness,\n",
    "#    context_recall,\n",
    "#    context_precision,\n",
    "#)\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    LLMContextPrecisionWithoutReference, # most similar to previous RAGAS version of Context Precision metric\n",
    "    LLMContextPrecisionWithReference,\n",
    "    NonLLMContextPrecisionWithReference\n",
    ")\n",
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from datasets import Dataset\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56373c08-f6b4-452a-8eba-63a20455d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca08633-0877-4773-93ab-d4a78b8f5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b65e5d-d679-49c4-ade6-2719fecb55d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas._analytics.do_not_track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ba768e-13e3-4ff7-92a6-e7f840329f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af7c1f7a-4007-4d19-af53-a3ccc078e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish RAG pipeline with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5248aab9-6fcd-46bf-b260-773c23b8845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Faiss vector store for RAG\n",
    "# # If you already have an index created, skip a few coding cells to the LLM / embeddings setup\n",
    "\n",
    "# # Example of creating a small vector store\n",
    "# # Using 4 State of the Union speeches, all text from whitehouse.gov briefing room speeches posted online, edited to include a title with the date of the speech\n",
    "# # Example from 2024:\n",
    "# # https://www.whitehouse.gov/briefing-room/speeches-remarks/2024/03/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery-2/\n",
    "\n",
    "# # load and parse files\n",
    "# sotu = []\n",
    "# newfiles = [\"./Speeches/titleedits/state_of_the_union_042921.txt\", \"./Speeches/titleedits/state_of_the_union_030122.txt\", \"./Speeches/titleedits/state_of_the_union_020723.txt\", \"./Speeches/titleedits/state_of_the_union_030724.txt\"]\n",
    "# for i in newfiles:\n",
    "#     with open(i) as file:\n",
    "#         for line in file:\n",
    "#             nl = line.rstrip()\n",
    "#             if nl != '':\n",
    "#                 sotu.append(nl)\n",
    "\n",
    "# # Convert into Document format for Faiss\n",
    "# documents = [Document(text=line) for line in sotu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb9bf47-2a72-41a7-af8e-ae663bf95e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='235d1f3b-a216-412c-8459-51d27c73c8d0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='May God protect our troops.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Example of a loaded Document line\n",
    "# documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ed3997-3a20-4a99-a9c1-7cc6d6a217e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# # Set up the Faiss index\n",
    "# d = 768 # dimensions of the input vector of the embedding model that we're going to use; in this case, the google embedding model\n",
    "# faiss_index = faiss.IndexFlatL2(d)\n",
    "# print(faiss_index.is_trained) # double check that the training worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490599ac-6499-4174-999d-4ddc0609fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the llm, embeddings, and Settings for Faiss \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\") # Replace with your LLM\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # Replace with your embeddings model\n",
    "Settings.embed_model = doc_embeddings # used for LlamaIndex FaissVectorStore\n",
    "Settings.llm = llm # used for LlamaIndex FaissVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24d9337-5832-47d6-90df-6e73b3de8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3171f73-723f-40f5-91d3-14e0f26173e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment for when you need to re-embed and vectorize documents\n",
    "\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, show_progress=True\n",
    "# )\n",
    "\n",
    "# # Save index to disk\n",
    "# index.storage_context.persist()\n",
    "\n",
    "# # Save/remember index id for loading next time\n",
    "# index.index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c8a330-0b12-434d-bc1c-4d8a7a1e32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading llama_index.vector_stores.faiss.base from ./storage/default__vector_store.json.\n",
      "Loading llama_index.vector_stores.faiss.base from ./storage/default__vector_store.json.\n",
      "INFO:llama_index.core.indices.loading:Loading indices with ids: ['95634851-570e-454e-983f-6634eeb72aee']\n",
      "Loading indices with ids: ['95634851-570e-454e-983f-6634eeb72aee']\n"
     ]
    }
   ],
   "source": [
    "# After you have a saved index, load that index for RAG answer generation:\n",
    "\n",
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "# My local index id '3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5' uses the 4 speeches including a title that includes the date it was given\n",
    "# Update: As of 2/4, the speeches index has somehow been deleted\n",
    "# My local index id '95634851-570e-454e-983f-6634eeb72aee' contains 3200 documents from the rag_mini_wikipedia dataset\n",
    "index = load_index_from_storage(storage_context=storage_context, index_id='95634851-570e-454e-983f-6634eeb72aee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcff60d7-d40f-4346-af9f-cfd9ab812e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7faf185372d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Optional- if you'd like to query your index\n",
    "# # Set up query and chat engines with the index\n",
    "# query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "# chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c4949-af9b-43da-9954-bebdd2cdbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query and response with Gemini and query_engine\n",
    "# query = \"What has the President done related to healthcare?\"\n",
    "# response = query_engine.query(query) \n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0178b0-ee47-4443-b8fa-c5cb3b4de228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get ranked scores for top k RAG source nodes\n",
    "# for node in response.source_nodes:\n",
    "#     print(f\"{node.get_score()} -> {node.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35114936-2d0c-4b06-99d7-16b4699d9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to embed a sentence with Gemini\n",
    "#result = genai.embed_content(\n",
    "#    model=\"models/text-embedding-004\",\n",
    "#    content=\"What is the meaning of life?\",\n",
    "#    task_type=\"retrieval_document\",\n",
    "#    title=\"Embedding of single string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "972fd7ce-ab58-42b9-962a-e604172eb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of using the chat engine with our index\n",
    "# query = \"You are an expert speech analyst and specialize in analyzing Presidential State of the Union speeches. Could you please analyze the speeches and generate 2 questions and answers from each speech, providing the document filename of each speech that relates to each question?\"\n",
    "# response = chat_engine.chat(query) \n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97057f9c-ffbb-493c-9ee6-819fb1fdf2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View chat history\n",
    "# chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf198ee7-45f6-4573-8bcb-60b25e2b7af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1f8b91f-afc2-4916-a65b-4a4bcb088d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for RAGAS evaluation library to work with Gemini and our local RAG setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27adc197-074d-4a2e-8ad3-066668419df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing code to have Gemini work with RAGAS\n",
    "from langchain_core.outputs import LLMResult\n",
    "\n",
    "def custom_is_finished_parser(response: LLMResult):\n",
    "    is_finished_list = []\n",
    "    for g in response.flatten():\n",
    "        resp = g.generations[0][0]\n",
    "        if resp.generation_info is not None:\n",
    "            # generation_info is provided - so we parse that\n",
    "\n",
    "            # Gemini uses \"STOP\" to indicate that the generation is finished\n",
    "            # and is stored in 'finish_reason' key in generation_info\n",
    "            if resp.generation_info.get(\"finish_reason\") is not None:\n",
    "                is_finished_list.append(\n",
    "                    resp.generation_info.get(\"finish_reason\") == \"STOP\"\n",
    "                )\n",
    "\n",
    "        # if generation_info is empty, we parse the response_metadata\n",
    "        # this is less reliable\n",
    "        elif (\n",
    "            isinstance(resp, ChatGeneration)\n",
    "            and t.cast(ChatGeneration, resp).message is not None\n",
    "        ):\n",
    "            resp_message: BaseMessage = t.cast(ChatGeneration, resp).message\n",
    "            if resp_message.response_metadata.get(\"finish_reason\") is not None:\n",
    "                is_finished_list.append(\n",
    "                    resp_message.response_metadata.get(\"finish_reason\") == \"STOP\"\n",
    "                )\n",
    "        # default to True\n",
    "        else:\n",
    "            is_finished_list.append(True)\n",
    "    return all(is_finished_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af3f8028-582d-49b5-9939-45eb0d810501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of generating synthetic dataset with RAGAS\n",
    "\n",
    "# In a synthetic dataset, columns generated are 'question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', and 'episode_done'\n",
    "# Ground truth is supposed to be the 'human' level answer vs the RAG answer\n",
    "\n",
    "# Notes: \n",
    "# - We have to generate the answer separately with our RAG, which then generates new context used.\n",
    "# - I use the context that was used to generate the answer for the metrics calculation, while still saving the old contexts column.\n",
    "# - The best thing to do would be to generate the answer when creating the synthetic test dataset, but this is not available in RAGAS.\n",
    "# - From a Github issue: Since you use the same LLM to generate your synthetic dataset ground_truth and your answer, \n",
    "# - it is possible the results of the RAG evaluation might be biased. This has not been studied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2002e239-4a5d-491d-b02b-611ae0f6afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents for use in generating synthetic dataset with RAGAS\n",
    "\n",
    "#corpus = pd.read_csv(\"datasets/rag_mini_wikipedia_corpus.csv\", index_col=['id'])\n",
    "#documents = [Document(text=passage[0], doc_id=str(i)) for i, passage in corpus.iterrows()] \n",
    "loader = DirectoryLoader(\"./Speeches/titleedits\") # Loads all documents in the directory; there are parameters for ignoring or matching certain files\n",
    "documents = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fefe4e-a9ea-4828-8a5e-90ba7dffc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83aac065-04c4-46fc-baed-cdb9bf2d9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add 'filename' metadata for RAGAS to process documents\n",
    "for document in documents:\n",
    "    #document.metadata['filename'] = document.id_\n",
    "    document.metadata['filename'] = document.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f893a9a9-ced5-4355-9155-6d6ca7d9fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic testset generator with Gemini models\n",
    "generator_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", timeout=240) # Other notable parameters: temperature=0.7, transport=\"rest\"\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    generator_llm,\n",
    "    is_finished_parser=custom_is_finished_parser,\n",
    ")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", request_options={\"timeout\": 240}) \n",
    "\n",
    "generator = TestsetGenerator(llm=ragas_llm, embedding_model=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06863fa3-f603-4a30-b2ba-44130f3d310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the timeout settings with RAGAS's RunConfig class\n",
    "\n",
    "# Note: For Gemini, the RAGAS internal RunConfig settings do a decent job at limiting the 429 resource exhausted warnings\n",
    "# (max_workers=1 still can send more requests to Gemini than the 15 requests per minute it allows)\n",
    "# Still very difficult to have the testset generation run successfully with Gemini free tier\n",
    "# I also tried the ratelimit and backoff libraries in Python, but I still got so many 429 warnings that the generation failed\n",
    "# Sometimes even the 1 max worker will not finish, but it will finish occasionally\n",
    "\n",
    "run_config = RunConfig(timeout=300, max_retries=20, max_wait=300, max_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82700f42-34a2-4cd5-b7d2-f1d509a4cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional- edit the distribution of query types desired for the testset generation\n",
    "# from ragas.testset.synthesizers import default_query_distribution\n",
    "# query_distribution = default_query_distribution(ragas_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde504a-85ec-4218-a409-ea9b52504cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RAGAS internal RunConfig settings do a decent job at limiting the 429 resource exhausted warnings when max_workers=1\n",
    "# Still receive many, many 429 errors\n",
    "gendataset = generator.generate_with_langchain_docs(documents, testset_size=10, run_config=run_config, ) # stopped here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cd22ec1-0bb5-4e12-8401-3866b2900d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What did President Biden say about President Z...</td>\n",
       "      <td>[State of the Union Address given by President...</td>\n",
       "      <td>President Biden said, \"From President Zelensky...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How is America standing with Ukraine, and what...</td>\n",
       "      <td>[60 Million barrels of oil from reserves aroun...</td>\n",
       "      <td>America will lead an effort, releasing 30 Mill...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is the term 'Amerrican' used in the contex...</td>\n",
       "      <td>[buy American to make sure everything from the...</td>\n",
       "      <td>The text discusses buying 'American' to ensure...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What specific actions are being taken to suppo...</td>\n",
       "      <td>[shouldn’t have to pay more than 7% of their i...</td>\n",
       "      <td>The plan includes increasing Pell Grants and i...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Given President Biden's State of the Union Add...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nState of the Union Address given b...</td>\n",
       "      <td>In his State of the Union Address on April 29,...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How iz the US addressing gas prices in light o...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nState of the Union Address given b...</td>\n",
       "      <td>In response to Russia's invasion of Ukraine, t...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Given President Biden's State of the Union Add...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nState of the Union Address given b...</td>\n",
       "      <td>In his State of the Union Address on April 29,...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Given President Biden's focus on climate chang...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nState of the Union Address given b...</td>\n",
       "      <td>During his State of the Union Address, Preside...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Considering President Biden's State of the Uni...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nState of the Union Address given b...</td>\n",
       "      <td>In his State of the Union address on March 1, ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Considering the historical significance of hav...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nState of the Union Address given b...</td>\n",
       "      <td>In his 2021 State of the Union address, Presid...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What discussions did President Obama initiate ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nAnd you know what they say? The co...</td>\n",
       "      <td>President Obama initiated the Cancer Moonshot,...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How does the current administration view the r...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\na job in her hometown providing st...</td>\n",
       "      <td>The current administration believes that Wall ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What did President Biden say about President Z...   \n",
       "1   How is America standing with Ukraine, and what...   \n",
       "2   How is the term 'Amerrican' used in the contex...   \n",
       "3   What specific actions are being taken to suppo...   \n",
       "4   Given President Biden's State of the Union Add...   \n",
       "5   How iz the US addressing gas prices in light o...   \n",
       "6   Given President Biden's State of the Union Add...   \n",
       "7   Given President Biden's focus on climate chang...   \n",
       "8   Considering President Biden's State of the Uni...   \n",
       "9   Considering the historical significance of hav...   \n",
       "10  What discussions did President Obama initiate ...   \n",
       "11  How does the current administration view the r...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [State of the Union Address given by President...   \n",
       "1   [60 Million barrels of oil from reserves aroun...   \n",
       "2   [buy American to make sure everything from the...   \n",
       "3   [shouldn’t have to pay more than 7% of their i...   \n",
       "4   [<1-hop>\\n\\nState of the Union Address given b...   \n",
       "5   [<1-hop>\\n\\nState of the Union Address given b...   \n",
       "6   [<1-hop>\\n\\nState of the Union Address given b...   \n",
       "7   [<1-hop>\\n\\nState of the Union Address given b...   \n",
       "8   [<1-hop>\\n\\nState of the Union Address given b...   \n",
       "9   [<1-hop>\\n\\nState of the Union Address given b...   \n",
       "10  [<1-hop>\\n\\nAnd you know what they say? The co...   \n",
       "11  [<1-hop>\\n\\na job in her hometown providing st...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   President Biden said, \"From President Zelensky...   \n",
       "1   America will lead an effort, releasing 30 Mill...   \n",
       "2   The text discusses buying 'American' to ensure...   \n",
       "3   The plan includes increasing Pell Grants and i...   \n",
       "4   In his State of the Union Address on April 29,...   \n",
       "5   In response to Russia's invasion of Ukraine, t...   \n",
       "6   In his State of the Union Address on April 29,...   \n",
       "7   During his State of the Union Address, Preside...   \n",
       "8   In his State of the Union address on March 1, ...   \n",
       "9   In his 2021 State of the Union address, Presid...   \n",
       "10  President Obama initiated the Cancer Moonshot,...   \n",
       "11  The current administration believes that Wall ...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   multi_hop_abstract_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  \n",
       "11  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Save generated testset to csv\n",
    "gendataset_pd = gendataset.to_pandas()\n",
    "gendataset_pd.to_csv(\"results/test_ragas_generation_0_2_12_sotu.csv\", index=False)\n",
    "gendataset_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8875f73-7c57-48bb-92e5-f7aa386ad8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e411a9d-7aa3-4783-aa48-d6467a2b25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now generate answers for the testset, as answers are not automatically generated at creation\n",
    "# # Below code uses a previously generated testset\n",
    "\n",
    "# testset_pd = pd.read_csv(\"datasets/testset_flash_pro15.csv\", index_col = None)\n",
    "\n",
    "# Note: When saving, the 'contexts' column is saved as a string but needs to be a list\n",
    "# If you are importing testset_pd from a csv file, use the below code to change the column to a list\n",
    "\n",
    "# testset_pd['contexts'] = testset_pd['contexts'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfd0ca-b9cd-4003-b6e3-addfc065a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers using our query engine & Faiss vector database\n",
    "# Alternatively can use the chat_engine if memory between queries is needed (i.e., queries reference previous queries)\n",
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "answers = [query_engine.query(q) for q in testset_pd['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33893af4-a200-4df9-9843-2e21a27d6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse out new 'answer' and 'contexts' columns\n",
    "answers_new = []\n",
    "context_new = []\n",
    "for i in answers:\n",
    "    answers_new.append(i.response)\n",
    "    context_new.append([c.node.get_content() for c in i.source_nodes])\n",
    "\n",
    "testset_pd = testset_pd.rename(columns={\"contexts\":\"contexts_gt\"}) # Keeping old contexts that were used for testset/query generation (gt = ground truth)\n",
    "testset_pd['contexts'] = context_new\n",
    "testset_pd['answer'] = answers_new\n",
    "\n",
    "# Save complete synthetically created dataset/testset\n",
    "# testset_pd.to_csv('datasets/ragas_full_testset_flash_pro15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aad7d0-9911-418e-bb26-1da441d61077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4984d6ef-7536-47a0-be34-8f4f3a1cacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate a dataset with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7866f0b2-d32b-45af-8a91-7d77fbb0a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset for evaluation\n",
    "testset_pd = pd.read_csv(\"datasets/rag_mini_wikipedia_complete_chat_ares.csv\", index_col = None) \n",
    "\n",
    "# RAGAS expects the following columns for evaluation (rename in dataset as needed) : \"user_input\", \"retrieved_contexts\", \"response\", \"reference\"\n",
    "testset_pd = testset_pd.rename(columns={\"question\": \"user_input\", \"answer\": \"response\", \"ground_truth\": \"reference\", \"contexts\": \"retrieved_contexts\"})\n",
    "\n",
    "# Note: When saving a synthetic dataset, the 'contexts' column is saved as a string but needs to be a list for evaluation\n",
    "# If you are importing testset_pd from a csv file that is a synthetic dataset, use the below code to change the column to a list\n",
    "# This may apply to other datasets as well\n",
    "testset_pd['retrieved_contexts'] = testset_pd['retrieved_contexts'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2492f767-6a59-472d-966c-4992b4cbcbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Abraham Lincoln (February 12, 1809 â April ...</td>\n",
       "      <td>Yes, Abraham Lincoln was the sixteenth Preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Lincoln believed in the Whig theory of the pr...</td>\n",
       "      <td>Yes, the provided text states that Lincoln sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Did his mother die of pneumonia?</td>\n",
       "      <td>no</td>\n",
       "      <td>[Soon thereafter, Tesla hastened from Paris to...</td>\n",
       "      <td>The provided text doesn't contain information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>How many long was Lincoln's formal education?</td>\n",
       "      <td>18 months</td>\n",
       "      <td>[Lincoln's formal education consisted of about...</td>\n",
       "      <td>The text states that Lincoln's formal educatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>When did Lincoln begin his political career?</td>\n",
       "      <td>1832</td>\n",
       "      <td>[Lincoln began his political career in 1832, a...</td>\n",
       "      <td>Lincoln began his political career in 1832, at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>1686</td>\n",
       "      <td>Was Wilson a member of the Phi Kappa Psi frate...</td>\n",
       "      <td>yes</td>\n",
       "      <td>[* Wilson was a member of the Phi Kappa Psi fr...</td>\n",
       "      <td>Yes, Woodrow Wilson was a member of the Phi Ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>1688</td>\n",
       "      <td>Was Wilson an automobile enthusiast?</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Wilson's Pierce Arrow, which resides in his h...</td>\n",
       "      <td>While not an *avid* enthusiast in the sense of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>1690</td>\n",
       "      <td>Did Wilson's father own slaves?</td>\n",
       "      <td>yes</td>\n",
       "      <td>[Thomas Woodrow Wilson was born in Staunton, V...</td>\n",
       "      <td>Yes, Woodrow Wilson's father, Joseph Ruggles W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>1692</td>\n",
       "      <td>Where is Wilson buried?</td>\n",
       "      <td>He was buried in Washington National Cathedral</td>\n",
       "      <td>[* Wilson is the only U.S. President buried in...</td>\n",
       "      <td>Woodrow Wilson is buried in Washington Nationa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>1694</td>\n",
       "      <td>Where did Wilson attend law school?</td>\n",
       "      <td>Wilson attended law school at University of Vi...</td>\n",
       "      <td>[* Wilson was a member of the Phi Kappa Psi fr...</td>\n",
       "      <td>Woodrow Wilson attended the University of Virg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>903 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                         user_input  \\\n",
       "0       0  Was Abraham Lincoln the sixteenth President of...   \n",
       "1       2  Did Lincoln sign the National Banking Act of 1...   \n",
       "2       4                   Did his mother die of pneumonia?   \n",
       "3       6      How many long was Lincoln's formal education?   \n",
       "4       8       When did Lincoln begin his political career?   \n",
       "..    ...                                                ...   \n",
       "898  1686  Was Wilson a member of the Phi Kappa Psi frate...   \n",
       "899  1688               Was Wilson an automobile enthusiast?   \n",
       "900  1690                    Did Wilson's father own slaves?   \n",
       "901  1692                            Where is Wilson buried?   \n",
       "902  1694                Where did Wilson attend law school?   \n",
       "\n",
       "                                             reference  \\\n",
       "0                                                  yes   \n",
       "1                                                  yes   \n",
       "2                                                   no   \n",
       "3                                            18 months   \n",
       "4                                                 1832   \n",
       "..                                                 ...   \n",
       "898                                                yes   \n",
       "899                                                yes   \n",
       "900                                                yes   \n",
       "901     He was buried in Washington National Cathedral   \n",
       "902  Wilson attended law school at University of Vi...   \n",
       "\n",
       "                                    retrieved_contexts  \\\n",
       "0    [Abraham Lincoln (February 12, 1809 â April ...   \n",
       "1    [Lincoln believed in the Whig theory of the pr...   \n",
       "2    [Soon thereafter, Tesla hastened from Paris to...   \n",
       "3    [Lincoln's formal education consisted of about...   \n",
       "4    [Lincoln began his political career in 1832, a...   \n",
       "..                                                 ...   \n",
       "898  [* Wilson was a member of the Phi Kappa Psi fr...   \n",
       "899  [Wilson's Pierce Arrow, which resides in his h...   \n",
       "900  [Thomas Woodrow Wilson was born in Staunton, V...   \n",
       "901  [* Wilson is the only U.S. President buried in...   \n",
       "902  [* Wilson was a member of the Phi Kappa Psi fr...   \n",
       "\n",
       "                                              response  \n",
       "0    Yes, Abraham Lincoln was the sixteenth Preside...  \n",
       "1    Yes, the provided text states that Lincoln sig...  \n",
       "2    The provided text doesn't contain information ...  \n",
       "3    The text states that Lincoln's formal educatio...  \n",
       "4    Lincoln began his political career in 1832, at...  \n",
       "..                                                 ...  \n",
       "898  Yes, Woodrow Wilson was a member of the Phi Ka...  \n",
       "899  While not an *avid* enthusiast in the sense of...  \n",
       "900  Yes, Woodrow Wilson's father, Joseph Ruggles W...  \n",
       "901  Woodrow Wilson is buried in Washington Nationa...  \n",
       "902  Woodrow Wilson attended the University of Virg...  \n",
       "\n",
       "[903 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c5af8ed-8a4f-4dc1-be1f-81f09b5b92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only dropping this column for an equal comparison run to the previous RAGAS version, whose context precision metric did not use the \"reference\" column\n",
    "# testset_pd_noref = testset_pd.drop(\"reference\", axis=1)\n",
    "# testset_pd_noref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c991eaa9-9b22-4807-a149-d7cb28523e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_input', 'reference', 'retrieved_contexts', 'response'],\n",
       "    num_rows: 903\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The EvaluationDataset format takes input in the HuggingFace Datasets format\n",
    "# Note: I am also dropping the id column here\n",
    "testset_ds = Dataset.from_pandas(testset_pd.drop(\"id\", axis=1))\n",
    "testset_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e675be85-9047-40d4-a9a3-95311a8fc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvaluationDataset.from_hf_dataset(testset_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcc219c6-499c-43e6-a3ea-d6b264f96aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=903)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a39ae55f-204a-4b42-8811-710bd26502cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I'm using the normal LLM, not the RAG context-loaded query engine\n",
    "# There is code at the bottom of the notebook for using the query engine, but it appears to just use the query engine to develop\n",
    "# new answers and contexts and then to use the non-RAG LLM for the metrics evaluation\n",
    "# That code also appears to be broken from RAGAS right now, so I was forced to use the regular Gemini LLM anyway\n",
    "\n",
    "# Note: The RAGAS evaluate function (below) may re-run the query and give new answers and contexts\n",
    "# See this issue: https://github.com/explodinggradients/ragas/issues/1211\n",
    "# In testing, the results still output the same answers and contexts as I started with, so I'm not concerned by this\n",
    "\n",
    "generator_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", timeout=300) # Other notable parameters: temperature=0.7, transport=\"rest\"\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    generator_llm,\n",
    "    is_finished_parser=custom_is_finished_parser,\n",
    ")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", request_options={\"timeout\": 300}) \n",
    "\n",
    "#embeddings_wrapper = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47977c82-14b7-4291-9b53-3c9ce0510487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the timeout settings\n",
    "run_config = RunConfig(timeout=3000, max_wait=3000, max_workers=1, max_retries=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06c41cfd-13bd-426d-be07-f51260bda5d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2456070787.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    context_precision\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# Two coding options for running evaluate:\n",
    "# 1) Bulk run with the evaluate function, as intended. \n",
    "# Unfortunately rate-limiting does not work well with this, \n",
    "# and 2/3 of my 800 example dataset received NaN results because of rate limiting issues.\n",
    "# Ex: Evaluating 1 example for 1 metric resulted in 10 API calls.\n",
    "\n",
    "# 2) Run the evaluation in small batches\n",
    "# This allowed me to finish the evaluation of the entire dataset without rate limiting errors.\n",
    "\n",
    "#cp new below\n",
    "#from ragas import evaluate\n",
    "\n",
    "#results = evaluate(eval_dataset, metrics=[metric])\n",
    "\n",
    "# Bulk evaluation of the dataset\n",
    "evalresult = evaluate(\n",
    "    dataset = eval_dataset,\n",
    "    metrics = [\n",
    "        context_precision\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall\n",
    "    ],\n",
    "    llm = ragas_llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config\n",
    ")\n",
    "\n",
    "# Optional parameter: in_ci: bool, Whether the evaluation is running in CI or not. \n",
    "# If set to True then some metrics will be run to increase the reproducability of the evaluations. \n",
    "# This will increase the runtime and cost of evaluations. Default is False.\n",
    "# In practice, setting in_ci = True resulted in a lot of timeouts / no score calculated / NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "997ebe04-8527-447e-b41b-98e8596069db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for smaller batches\n",
    "testset_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4017dc50-c94a-4b98-83ab-e6e0ce0a03e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddc1040f6fe4ec08f27161767371183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f20eca702843649defd412c9160029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9f2c6c7296487d8550d21a8bc775aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a3a503b378459283b835569fcdf591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a24eb624f8b42f1af2409b590f500d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2add78269add4f5eb12f0961d64ee232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a3187e2e2c43c583988d70932f3f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "349\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81a6fd0da5c461e8c9bacf21f4008ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250837e5a1af4800be511a910728fce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca484b8287b46cfb086d8ae29a35c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df1c8c2b02d4fc5a1518918e377ffef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9c565569dd4558acefec216d4554c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4a2b0bf8d446afa6723bdf4b31e3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b6431cfabf42dca13ab7045786606a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b586bb2d7e71413c939f224702a86335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788843a515224ed595a993656d0df8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234824e98fbd4f819e89f131916fe5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc89d9326c9746acb7939aef04cf17ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303ba0250e7041b8a9ad78728b6299fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3816f9a3313f45028c8f69b0da27c43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8b7cb4fe5647588fc4c8771e796ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c601d0944a5d4a4199077f76953def24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8121075eeb7a426aa348c6f23b0f21e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a28365c1af4870b13e5bada3a0b0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091ba11795a445ba84c37b7305c75871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf51103a68f46299f3f691644f0f6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70ea0e3f0214d3e8abee01d55b87990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8430a5ff006c456abf6ca3fce509b331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc58aa937e6483297e16a86219aa515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685e589a9e4e49cda7761591dff6adda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3b1c608a0e468788d54058e15c2209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092d4e02e12142b182170e233dc9083f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457cdf2183804c2ca44816fa8b45815d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f22328fdb7142b3b5216ab0a5ccf2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3f69edeb8446498c290f55e01023c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18190c6927554e4888db1d1b4edfc812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161e58925a0b426783c517f4dcf720a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f8e1826c3e40b485d2947c666cc674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2169b27e28cb430c807225417bd671d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c734fbc519c4fe2b44dcb6889384165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8550e5a9e34576aaacd11952e6716f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d47578690014c90a9d1be6c034e89ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f3496b2bfa4a1f87fdc6737a24fef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4634994f7ff046d098d12663493616ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7543f678fbaf41eebbdeb4bb5f78643f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda3c4eafad94298b12c6e35bb8d6109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f6112740a8439cbffeb2dbca6b629a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c18589d34042f9961fe06090126767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Exception raised in Job[0]: ResourceExhausted(429 Resource has been exhausted (e.g. check quota).)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1beb3f4537fc4de280a42025d9243f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Exception raised in Job[0]: ResourceExhausted(429 Resource has been exhausted (e.g. check quota).)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc52d2e2a8c4db681f7ef72307240d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Exception raised in Job[0]: ResourceExhausted(429 Resource has been exhausted (e.g. check quota).)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through dataset for smaller batches to be evaluated\n",
    "context_precision_ref = LLMContextPrecisionWithReference(llm=ragas_llm) # most similar to DeepEval's Context Precision\n",
    "context_precision_noref = LLMContextPrecisionWithoutReference(llm=ragas_llm)\n",
    "context_precision_ref_nollm = NonLLMContextPrecisionWithReference()\n",
    "faithfulness_score = Faithfulness(llm=ragas_llm)\n",
    "batchsize = 1\n",
    "for i in range(389,392,batchsize):   # stopped at 389, 388 was last fully executed\n",
    "    tempdataset = EvaluationDataset(samples=eval_dataset.samples[i:i+batchsize])\n",
    "    print(i)\n",
    "    evalresult = evaluate(\n",
    "        metrics = [\n",
    "            context_precision_ref\n",
    "        ],\n",
    "        dataset = tempdataset,\n",
    "        llm = ragas_llm,\n",
    "        embeddings=embeddings,\n",
    "        run_config=run_config\n",
    "    )\n",
    "    testset_results = pd.concat([testset_results, evalresult.to_pandas()])\n",
    "    time.sleep(65) # RAGAS generates ~2 API calls per example for faithfulness.\n",
    "# RAGAS generates ~10 API calls per example for context precision. Gemini free-tier limit is 15RPM, so we need to wait a minute between calls for the limit to reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4da8166b-5946-4f59-9272-e2c91c0511d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>llm_context_precision_with_reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does a large proportion of the population spea...</td>\n",
       "      <td>[Finland has a population of 5,300,362 people,...</td>\n",
       "      <td>No, while Swedish is an official language, onl...</td>\n",
       "      <td>No</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do women live longer than men?</td>\n",
       "      <td>[The life expectancy is 82 years for women and...</td>\n",
       "      <td>Yes, the provided text indicates that women in...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a country with which Finland is involv...</td>\n",
       "      <td>[Finland, officially the Republic of Finland \"...</td>\n",
       "      <td>The text states that Finland is not involved i...</td>\n",
       "      <td>Finland is not involved in international confl...</td>\n",
       "      <td>0.908532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Ford a member of the House of Representati...</td>\n",
       "      <td>[Ford was a member of the House of Representat...</td>\n",
       "      <td>Yes, Gerald Ford was a member of the House of ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For how long was Ford a member of the House of...</td>\n",
       "      <td>[Ford was a member of the House of Representat...</td>\n",
       "      <td>Gerald Ford was a member of the House of Repre...</td>\n",
       "      <td>Over eight years.</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In which years were John Monroe elected as Pre...</td>\n",
       "      <td>[Following the War of 1812, Monroe was elected...</td>\n",
       "      <td>James Monroe, not John Monroe, was elected Pre...</td>\n",
       "      <td>1817-1825</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who was John Monroe standing behind in the pai...</td>\n",
       "      <td>[The Presidentâs parents, father Spence Monr...</td>\n",
       "      <td>There's no widely known painting of \"Washingto...</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was James Monroe appointed to Secretary o...</td>\n",
       "      <td>[James Monroe (April 28, 1758 â July 4, 1831...</td>\n",
       "      <td>James Monroe was appointed Secretary of War in...</td>\n",
       "      <td>1814</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did James Monroe die?</td>\n",
       "      <td>[Upon Elizabeth's death in 1830, Monroe moved ...</td>\n",
       "      <td>James Monroe died on July 4, 1831.\\n</td>\n",
       "      <td>July 4, 1831</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did James Monroe graduate from William an...</td>\n",
       "      <td>[James Monroe (April 28, 1758 â July 4, 1831...</td>\n",
       "      <td>James Monroe graduated from the College of Wil...</td>\n",
       "      <td>1776</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   Does a large proportion of the population spea...   \n",
       "0                      Do women live longer than men?   \n",
       "0   What is a country with which Finland is involv...   \n",
       "0   Was Ford a member of the House of Representati...   \n",
       "0   For how long was Ford a member of the House of...   \n",
       "..                                                ...   \n",
       "0   In which years were John Monroe elected as Pre...   \n",
       "0   Who was John Monroe standing behind in the pai...   \n",
       "0   When was James Monroe appointed to Secretary o...   \n",
       "0                          When did James Monroe die?   \n",
       "0   When did James Monroe graduate from William an...   \n",
       "\n",
       "                                   retrieved_contexts  \\\n",
       "0   [Finland has a population of 5,300,362 people,...   \n",
       "0   [The life expectancy is 82 years for women and...   \n",
       "0   [Finland, officially the Republic of Finland \"...   \n",
       "0   [Ford was a member of the House of Representat...   \n",
       "0   [Ford was a member of the House of Representat...   \n",
       "..                                                ...   \n",
       "0   [Following the War of 1812, Monroe was elected...   \n",
       "0   [The Presidentâs parents, father Spence Monr...   \n",
       "0   [James Monroe (April 28, 1758 â July 4, 1831...   \n",
       "0   [Upon Elizabeth's death in 1830, Monroe moved ...   \n",
       "0   [James Monroe (April 28, 1758 â July 4, 1831...   \n",
       "\n",
       "                                             response  \\\n",
       "0   No, while Swedish is an official language, onl...   \n",
       "0   Yes, the provided text indicates that women in...   \n",
       "0   The text states that Finland is not involved i...   \n",
       "0   Yes, Gerald Ford was a member of the House of ...   \n",
       "0   Gerald Ford was a member of the House of Repre...   \n",
       "..                                                ...   \n",
       "0   James Monroe, not John Monroe, was elected Pre...   \n",
       "0   There's no widely known painting of \"Washingto...   \n",
       "0   James Monroe was appointed Secretary of War in...   \n",
       "0                James Monroe died on July 4, 1831.\\n   \n",
       "0   James Monroe graduated from the College of Wil...   \n",
       "\n",
       "                                            reference  \\\n",
       "0                                                  No   \n",
       "0                                                 Yes   \n",
       "0   Finland is not involved in international confl...   \n",
       "0                                                 Yes   \n",
       "0                                   Over eight years.   \n",
       "..                                                ...   \n",
       "0                                           1817-1825   \n",
       "0                                   George Washington   \n",
       "0                                                1814   \n",
       "0                                        July 4, 1831   \n",
       "0                                                1776   \n",
       "\n",
       "    llm_context_precision_with_reference  \n",
       "0                               0.833333  \n",
       "0                               1.000000  \n",
       "0                               0.908532  \n",
       "0                               0.966667  \n",
       "0                               0.500000  \n",
       "..                                   ...  \n",
       "0                               0.500000  \n",
       "0                               0.000000  \n",
       "0                                    NaN  \n",
       "0                                    NaN  \n",
       "0                                    NaN  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e78b5653-bab0-4f83-9a72-e2aa39c5c647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_results[:147]['llm_context_precision_with_reference'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb6b1a-2df4-44cc-958d-b40331cf111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83793eb7-2820-4d92-9e57-63cdb1159dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_697/751023299.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testset_results.iloc[21]['llm_context_precision_with_reference'] = testset_results.iloc[80]['llm_context_precision_with_reference']\n"
     ]
    }
   ],
   "source": [
    "# to do later, 1\n",
    "testset_results.iloc[21]['llm_context_precision_with_reference'] = testset_results.iloc[80]['llm_context_precision_with_reference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40b81e31-b776-47e9-ab8f-4aaa46918584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_input                                                       What is named after him?\n",
       "retrieved_contexts                      [The Celsius crater on the Moon is named after...\n",
       "response                                The Celsius temperature scale is named after A...\n",
       "reference                                                  The Celsius crater on the Moon\n",
       "llm_context_precision_with_reference                                                  NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_results.iloc[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ccf8ce4-4b79-459a-b055-1f51af55518e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_results['llm_context_precision_with_reference'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "445e7fbd-9b27-4584-9ec6-b7a6ea8caeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                   41\n",
       "user_input            Was Lincoln chosen as a presidential candidate...\n",
       "reference                                                           Yes\n",
       "retrieved_contexts    [On November 6, 1860, Lincoln was elected as t...\n",
       "response              Yes, Abraham Lincoln was chosen as the Republi...\n",
       "Name: 20, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_pd.iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0798ecd8-9e0f-43d7-b6e3-43bb6e383c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend checking contexts column to be sure context nodes are separated from each other after saving\n",
    "testset_results[:147].to_csv(\"results/results_ragas_2_12_gemini_1_5_rag_mini_wiki_complete_chat_cpwithref_242_388.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "642f08c7-6a8e-4ca5-9eba-49c942a6322a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4913204b-721e-4e21-921e-c60f31f672da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing new method\n",
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import LLMContextPrecisionWithoutReference\n",
    "from ragas.metrics import Faithfulness\n",
    "\n",
    "context_precision_ref = LLMContextPrecisionWithReference(llm=ragas_llm)\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    reference=\"Paris\",\n",
    "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n",
    "    response=\"The Eiffel Tower is in Paris.\"\n",
    ")\n",
    "\n",
    "await context_precision_ref.single_turn_ascore(sample, timeout=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc52666-3b5b-4f72-9a14-46c879307441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import Faithfulness\n",
    "faithfulness_score = Faithfulness(llm=ragas_llm)\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    reference=\"Paris\",\n",
    "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n",
    "    response=\"The Eiffel Tower is in Paris.\"\n",
    ")\n",
    "\n",
    "await faithfulness_score.single_turn_ascore(sample, timeout=70)\n",
    "# stopped here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06206d2-7486-4edb-9d90-75d1dd11234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "faithfulnesshhem_score = FaithfulnesswithHHEM(llm=ragas_llm)\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"Where is the Eiffel Tower located?\",\n",
    "    reference=\"Paris\",\n",
    "    retrieved_contexts=[\"The Eiffel Tower is located in Paris.\"], \n",
    "    response=\"The Eiffel Tower is in Paris.\"\n",
    ")\n",
    "await faithfulnesshhem_score.single_turn_ascore(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75a03c5c-b6a2-4a15-9d62-b9a6890dcd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.6944, 'faithfulness': 1.0000, 'answer_relevancy': 0.6770, 'context_recall': 0.6455}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5ecd8cf-8642-4ea0-ac1a-c6bcbb05a621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.5979, 'faithfulness': 1.0000, 'answer_relevancy': 0.6533, 'context_recall': 0.8000}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example result:\n",
    "# {'context_precision': 0.4676, 'faithfulness': 1.0000, 'answer_relevancy': 0.6515, 'context_recall': 0.8000}\n",
    "# Reran\n",
    "# {'context_precision': 0.5979, 'faithfulness': 1.0000, 'answer_relevancy': 0.6533, 'context_recall': 0.8000}\n",
    "evalresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0a5aa-f1b4-4945-957b-9e30a0dd43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: received warning for the answer where there was no response from the llm, definitely reduced faithfulness score\n",
    "\n",
    "# Results:\n",
    "# Using contexts generated when produced answers from LLM:\n",
    "# new testset_answer_newcontext_flash_pro15.csv result, with contexts_gt (aka contexts generated with ground truth generation) column removed\n",
    "# {'context_precision': 0.4171, 'faithfulness': 0.9167, 'answer_relevancy': 0.6509, 'context_recall': 0.8000}\n",
    "# reran\n",
    "# {'context_precision': 0.4676, 'faithfulness': 1.0000, 'answer_relevancy': 0.6515, 'context_recall': 0.8000}\n",
    "# reran\n",
    "# {'context_precision': 0.5979, 'faithfulness': 1.0000, 'answer_relevancy': 0.6533, 'context_recall': 0.8000}\n",
    "\n",
    "# Compared to using contexts generated for ground truth (probably not correct):\n",
    "# new testset_answer_newcontext_flash_pro15.csv result, using old contexts\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.7392, 'answer_relevancy': 0.6041, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.8500, 'faithfulness': 0.7123, 'answer_relevancy': 0.5934, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.6556, 'answer_relevancy': 0.5638, 'context_recall': 1.0000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a027af-7dff-483f-97ff-9072e71d7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results on metrics:\n",
    "\n",
    "# RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics\n",
    "# I don't have example ranges to compare anything to, so below is my best guess.\n",
    "\n",
    "# Faithfulness - Measures the factual consistency of the answer to the context based on the question.\n",
    "# 0.9167 - 1.0000 indicates that the LLM is staying true to the facts provided in the context for answering the question.\n",
    "# There is another Faithfulness metric: from ragas.metrics import FaithulnesswithHHEM\n",
    "# This uses a huggingface model to help detect hallucination : https://huggingface.co/vectara/hallucination_evaluation_model\n",
    "# See below for code : {'faithfulness_with_hhem': 0.6319} \n",
    "# This doesn't really agree with the RAGAS faithfulness score... may need to dive in further another time.\n",
    "# Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.\n",
    "# At 0.4171 - 0.5979, suggests that the context isn't particularly relevant to the question.\n",
    "# Answer_relevancy - Measures how relevant the answer is to the question.\n",
    "# 0.6509 - 0.6533 seems moderately low, just going off of the number.\n",
    "# Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.\n",
    "# 0.8 indicates that the llm context is decently good and can typically answer the question or most of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "04f4d426-e499-475e-8459-a497368b8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run, just compare to using contexts_gt column instead of the newer context generated with the answer\n",
    "testset_ds_oldcontext = Dataset.from_pandas(testset_pd.drop(\"contexts\", axis=1).rename(columns={'contexts_old':'contexts'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7b251-9833-4931-baf2-c575bf900721",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalresult_old2 = evaluate(\n",
    "    testset_ds_oldcontext,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm = ragas_llm,\n",
    "    embeddings=embeddings, \n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7170d4-ca68-46b1-a962-3c71093374c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new testset_answer_newcontext_flash_pro15.csv result, using old contexts\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.7392, 'answer_relevancy': 0.6041, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.8500, 'faithfulness': 0.7123, 'answer_relevancy': 0.5934, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.6556, 'answer_relevancy': 0.5638, 'context_recall': 1.0000}\n",
    "evalresult_old2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79fac0-ea9d-473f-b33c-02ee96df9559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "02abe059-1129-4f60-81b7-55cef6b958ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra non-working code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "38993aa7-31ed-438e-96c6-63de9c608016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to use the query_engine in the evaluation \n",
    "# Modeled after this tutorial: https://docs.ragas.io/en/latest/howtos/applications/compare_llms.html\n",
    "\n",
    "# Does not currently work: for some metrics, it is not finding the 'ground_truth' column in the dataset\n",
    "# For other metrics, appears to run but returns the below errors and returns 'nan' for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b983f067-90e5-465a-9014-26fc5593f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of testing to try and get rag query engine for evaluate\n",
    "def generate_responses(query_engine, test_questions, test_answers):\n",
    "  responses = [query_engine.query(q) for q in test_questions]\n",
    "\n",
    "  answers = []\n",
    "  contexts = []\n",
    "  for r in responses:\n",
    "    answers.append(r.response)\n",
    "    contexts.append([c.node.get_content() for c in r.source_nodes])\n",
    "  dataset_dict = {\n",
    "        \"question\": test_questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "  }\n",
    "  if test_answers is not None:\n",
    "    dataset_dict[\"ground_truth\"] = test_answers\n",
    "  ds = Dataset.from_dict(dataset_dict)\n",
    "  return ds\n",
    "\n",
    "test_questions = testset_pd['question'].values.tolist()\n",
    "test_answers = [[item] for item in testset_pd['answer'].values.tolist()]\n",
    "\n",
    "result_ds = generate_responses(query_engine, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f75a3-602b-49b8-b269-dab9f3ce817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This evaluate function that uses the query_engine does not return results (nan for all metrics)\n",
    "# Errors (below are repeated many times):\n",
    "# WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
    "# n values greater than 1 not support for LlamaIndex LLMs\n",
    "# INFO:ragas.llms.base:callbacks not supported for LlamaIndex LLMs, ignoring callbacks\n",
    "# callbacks not supported for LlamaIndex LLMs, ignoring callbacks\n",
    "# ERROR:ragas.executor:Exception raised in Job[5]: TimeoutError()\n",
    "# Exception raised in Job[5]: TimeoutError()\n",
    "# ERROR:ragas.executor:Exception raised in Job[19]: AttributeError('ChatGoogleGenerativeAI' object has no attribute 'acomplete')\n",
    "# Exception raised in Job[19]: AttributeError('ChatGoogleGenerativeAI' object has no attribute 'acomplete')\n",
    "\n",
    "from ragas.integrations.llama_index import evaluate\n",
    "\n",
    "eval_qe2 = evaluate(\n",
    "    query_engine=query_engine,\n",
    "    dataset=result_ds,\n",
    "    metrics=[faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_utilization],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=embeddings, \n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "296c8c6d-737d-4233-ae5a-4114597a76f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': nan, 'answer_relevancy': nan, 'context_utilization': nan}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_qe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c19d1-71a0-40ba-ad28-f689cba75277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini_ragas",
   "language": "python",
   "name": "gemini_ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
