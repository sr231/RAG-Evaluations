{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd552471-bc05-4749-b207-9be5e7e2d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook uses the Google Gemini API (free tier, local API key) \n",
    "# and the RAGAS evaluation library to evaluate several metrics for a RAG pipeline\n",
    "# Google Gemini: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# RAGAS: https://docs.ragas.io/en/stable/\n",
    "# Note: This notebook is with RAGAS 2-6 and Python 3.11.8\n",
    "\n",
    "# Note: I had to edit underlying RAGAS library (cloned locally, edited files, then pip -e installed locally) for this issue re: temperature with Gemini:\n",
    "# https://github.com/explodinggradients/ragas/pull/657/files\n",
    "# https://github.com/explodinggradients/ragas/issues/678\n",
    "# Edits simply remove the temperature variable, see notes.txt for more specific info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44922bc8-8feb-44cd-9541-84618200beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics\n",
    "\n",
    "# Faithfulness - Measures the factual consistency of the answer to the context based on the question.\n",
    "# Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.\n",
    "# Answer_relevancy - Measures how relevant the answer is to the question.\n",
    "# Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.\n",
    "\n",
    "# Faithfulness with HHEM - Similar to Faithfulness but uses a HuggingFace model (Vectara's HHEM 2.1 classifier) to detect hallucinations\n",
    "# https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html#faithfullness-with-hhem-2-1-model\n",
    "# https://huggingface.co/vectara/hallucination_evaluation_model\n",
    "\n",
    "# RAGAS has other metrics as well : https://docs.ragas.io/en/latest/concepts/metrics/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f13f959-5e0b-4e76-b116-da5da6a04ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set do not track variable to RAGAS\n",
    "# more info: https://github.com/explodinggradients/ragas/issues/49\n",
    "import os\n",
    "os.environ[\"RAGAS_DO_NOT_TRACK\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30fd81d-ce93-4e03-bbec-862ece105b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import google.generativeai as genai\n",
    "import textwrap\n",
    "import ast\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import ragas\n",
    "from ragas.testset import TestsetGenerator\n",
    "#from ragas.testset.evolution import simple, reasoning, multi_context\n",
    "from ragas.run_config import RunConfig\n",
    "#from ragas.metrics import (\n",
    "#    answer_relevancy,\n",
    "#    faithfulness,\n",
    "#    context_recall,\n",
    "#    context_precision,\n",
    "#)\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    LLMContextPrecisionWithoutReference,\n",
    "    LLMContextPrecisionWithReference, \n",
    "    NonLLMContextPrecisionWithReference\n",
    ")\n",
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from datasets import Dataset\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56373c08-f6b4-452a-8eba-63a20455d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca08633-0877-4773-93ab-d4a78b8f5662",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b65e5d-d679-49c4-ade6-2719fecb55d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas._analytics.do_not_track()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f5c3780-22bc-42d7-8653-107c0e0c26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5248aab9-6fcd-46bf-b260-773c23b8845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document database\n",
    "# using 4 State of the Union speeches, all text from whitehouse.gov briefing room speeches posted online\n",
    "# Example from 2024:\n",
    "# https://www.whitehouse.gov/briefing-room/speeches-remarks/2024/03/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery-2/\n",
    "sotu = []\n",
    "#files = [\"./Speeches/state_of_the_union_042921.txt\", \"./Speeches/state_of_the_union_030122.txt\", \"./Speeches/state_of_the_union_020723.txt\", \"./Speeches/state_of_the_union_030724.txt\"]\n",
    "newfiles = [\"./Speeches/titleedits/state_of_the_union_042921.txt\", \"./Speeches/titleedits/state_of_the_union_030122.txt\", \"./Speeches/titleedits/state_of_the_union_020723.txt\", \"./Speeches/titleedits/state_of_the_union_030724.txt\"]\n",
    "\n",
    "for i in newfiles:\n",
    "    with open(i) as file:\n",
    "        for line in file:\n",
    "            nl = line.rstrip()\n",
    "            if nl != '':\n",
    "                sotu.append(nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd48335-0c8c-4f86-ae1b-d1832ff6a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [Document(text=line) for line in sotu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb9bf47-2a72-41a7-af8e-ae663bf95e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='235d1f3b-a216-412c-8459-51d27c73c8d0', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='May God protect our troops.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a loaded Document line\n",
    "documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ed3997-3a20-4a99-a9c1-7cc6d6a217e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Set up the faiss index\n",
    "d = 768 # dimensions of the input vector of the embedding model that we're going to use; in this case, the google embedding model\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "print(faiss_index.is_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "490599ac-6499-4174-999d-4ddc0609fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # optional: task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "Settings.embed_model = doc_embeddings\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24d9337-5832-47d6-90df-6e73b3de8a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3171f73-723f-40f5-91d3-14e0f26173e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment for when you need to re-embed and vectorize documents\n",
    "## otherwise, doing local loading below\n",
    "#vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "#storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#index = VectorStoreIndex.from_documents(\n",
    "#    documents, storage_context=storage_context, show_progress=True\n",
    "#)\n",
    "## save index to disk\n",
    "#index.storage_context.persist()\n",
    "#index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c8a330-0b12-434d-bc1c-4d8a7a1e32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading llama_index.vector_stores.faiss.base from ./storage/default__vector_store.json.\n",
      "Loading llama_index.vector_stores.faiss.base from ./storage/default__vector_store.json.\n",
      "INFO:llama_index.core.indices.loading:Loading indices with ids: ['3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5']\n",
      "Loading indices with ids: ['3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5']\n"
     ]
    }
   ],
   "source": [
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "#index = load_index_from_storage(storage_context=storage_context)\n",
    "# index id '3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5' uses the speeches with a title that includes the date it was given\n",
    "index = load_index_from_storage(storage_context=storage_context, index_id='3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcff60d7-d40f-4346-af9f-cfd9ab812e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7faf185372d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ba1e938-d2bc-4ef3-a9d2-f2bc01a41e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026088c-515b-4933-9ec9-d3a616870533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query and response\n",
    "query = \"What has the President done related to healthcare?\"\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a13c4949-af9b-43da-9954-bebdd2cdbdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The President has taken several actions related to healthcare, including establishing a special sign-up period for the Affordable Care Act, enacting tax credits to reduce health care premiums, and re-igniting the Cancer Moonshot. \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e0178b0-ee47-4443-b8fa-c5cb3b4de228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7506474256515503 -> I’m pleased to say that more Americans have health insurance now than ever in history.\n",
      "0.7599508762359619 -> During these 100 days, an additional 800,000 Americans enrolled in the Affordable Care Act when I established the special sign-up period to do that — 800,000 in that period.\n",
      "0.7667317390441895 -> The Affordable Care Act has been a lifeline for millions of Americans, protecting people with preexisting conditions, protecting women’s health.  And the pandemic has demonstrated how badly — how badly it’s needed.  Let’s lower deductibles for working families on the Affordable Care — in the Affordable Care Act.  (Applause.)  And let’s lower prescription drug costs.  (Applause.)\n",
      "0.796398401260376 -> A president, my predecessor, who failed the most basic duty. Any President owes the American people the duty to care.\n",
      "0.801885724067688 -> Over one hundred million of you can no longer be denied health insurance because of pre-existing conditions.\n",
      "0.8065693974494934 -> I enacted tax credits that save $800 per person per year reducing health care premiums for millions of working families.\n",
      "0.8096300363540649 -> A record 16 million people are enrolled under the Affordable Care Act.\n",
      "0.8154945373535156 -> And fourth, last year Jill and I re-ignited the Cancer Moonshot that President Obama asked me to lead in our Administration.\n",
      "0.8243342638015747 -> We’ve talked about it long enough.  Democrats and Republicans, let’s get it done this year.  This is all about a simple premise: Healthcare should be a right, not a privilege in America.  (Applause.)\n",
      "0.8246870636940002 -> I get it. With the Inflation Reduction Act that I signed into law, we’re taking on powerful interests to bring your health care costs down so you can sleep better at night.\n"
     ]
    }
   ],
   "source": [
    "# Resulting scores \n",
    "for node in response.source_nodes:\n",
    "    print(f\"{node.get_score()} -> {node.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35114936-2d0c-4b06-99d7-16b4699d9388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to embed a sentence with Gemini\n",
    "#result = genai.embed_content(\n",
    "#    model=\"models/text-embedding-004\",\n",
    "#    content=\"What is the meaning of life?\",\n",
    "#    task_type=\"retrieval_document\",\n",
    "#    title=\"Embedding of single string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "972fd7ce-ab58-42b9-962a-e604172eb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of setting up a chat engine with our index\n",
    "chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "116cf23e-1423-477a-9bd3-ea0272ee5083",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What does the President say about Congress during these 4 years?\"\n",
    "response = chat_engine.chat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2661c93d-c075-41ee-a5e8-ef5ae67e0265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided excerpt doesn't give us a clear picture of the President's overall view of Congress during his 4-year term.  However, it does highlight a few key points:\n",
      "\n",
      "* **He expresses a desire for bipartisanship:**  He states that if they could work together in the last Congress, there's no reason they can't work together in the new one. This suggests a willingness to collaborate with both Republicans and Democrats.\n",
      "* **He acknowledges their role in the government:** He addresses them directly and respectfully, using titles like \"Madam Speaker\" and \"Mr. Speaker,\" highlighting their importance in the political process. \n",
      "* **He doesn't explicitly criticize Congress:** While he doesn't explicitly praise them, he also doesn't criticize their performance. \n",
      "\n",
      "To understand the President's full perspective on Congress during his term, we'd need to look at more of his speeches, actions, and interactions with Congress throughout those four years. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa970253-2559-4803-b390-c1f81fdc97d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7417372465133667 -> Throughout our history, Presidents have come to this chamber to speak to Congress, to the nation, and to the world to declare war, to celebrate peace, to announce new plans and possibilities.\n",
      "0.7774401307106018 -> To my Republican friends, if we could work together in the last Congress, there is no reason we can’t work together in this new Congress.\n",
      "0.7898776531219482 -> So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report.\n",
      "0.7941027879714966 -> I promised to be the president for all Americans.\n",
      "0.8024358153343201 -> Madam Speaker, Madam Vice President — (applause) — no President has ever said those words from this podium.  No President has ever said those words, and it’s about time.  (Applause.)\n",
      "0.8032354712486267 -> Mr. Speaker. Madam Vice President. Members of Congress. My Fellow Americans.\n",
      "0.8043006658554077 -> And my report is this: the State of the Union is strong—because you, the American people, are strong.\n",
      "0.8044711351394653 -> Madam Speaker, Madam Vice President, our First Lady and Second Gentleman. Members of Congress and the Cabinet. Justices of the Supreme Court. My fellow Americans.\n",
      "0.8044825792312622 -> THE PRESIDENT:  Thank you.  (Applause.)  Thank you.  Thank you.  Good to be back.  And Mitch and Chuck will understand it’s good to be almost home, down the hall.  Anyway, thank you all.\n",
      "0.805178165435791 -> So on this night, in our 245th year as a nation, I have come to report on the State of the Union.\n"
     ]
    }
   ],
   "source": [
    "# Resulting scores\n",
    "for node in response.source_nodes:\n",
    "    print(f\"{node.get_score()} -> {node.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97057f9c-ffbb-493c-9ee6-819fb1fdf2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, content='What does the President say about Congress during these 4 years?', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The excerpt you provided focuses on the President\\'s opening remarks to Congress, not on a broader assessment of their performance over four years. \\n\\nHowever, we can glean some insights:\\n\\n* **He emphasizes the need for bipartisanship:** He explicitly states, \"To my Republican friends, if we could work together in the last Congress, there is no reason we can’t work together in this new Congress.\" This suggests a desire for collaboration across party lines.\\n* **He acknowledges the presence of both Republican and Democratic leaders:** He addresses \"Mitch and Chuck,\" likely referring to Mitch McConnell and Chuck Schumer, the leaders of the Republican and Democratic parties in the Senate, respectively. This implies a recognition of the different political forces at play.\\n\\nHowever, without further context, we can\\'t determine the President\\'s overall view of Congress over his four years in office. We\\'d need additional information, like speeches, interviews, or policy decisions, to understand his assessment of their performance. \\n', additional_kwargs={})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf198ee7-45f6-4573-8bcb-60b25e2b7af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1f8b91f-afc2-4916-a65b-4a4bcb088d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of RAGAS implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af3f8028-582d-49b5-9939-45eb0d810501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic test data\n",
    "# Note: When generating a synthetic test dataset, the columns generated are \n",
    "# 'question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', 'episode_done'\n",
    "# Ground truth is supposed to be the 'human' level answer vs the RAG answer\n",
    "# We have to generate the answer separately with our RAG, which then (obviously) generates new context used.\n",
    "# My best guess is to use the context that was used to generate the answer for the metrics calculation.\n",
    "# Thus, in the below example, I keep the old contexts column, but for evaluating RAG, I use the new context to calculate the metrics.\n",
    "# I can't find good documentation to confirm this; the following issues are close:\n",
    "# https://github.com/explodinggradients/ragas/issues/1145\n",
    "# https://github.com/explodinggradients/ragas/issues/1084\n",
    "\n",
    "# Possible future edit:\n",
    "# The best thing to do would be to generate the answer when creating the synthetic test dataset, but this is no longer done (perhaps done in RAGAS 1.0?)\n",
    "\n",
    "# Also, a relevant warning from the same issue 1084:\n",
    "# \"Since you use the same LLM to generate your synthetic dataset ground_truth and your answer, \n",
    "# I think the results of this evaluation might be biased. I haven't realized a comparative study \n",
    "# but it might be an issue which could have an impact on your interpretation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2002e239-4a5d-491d-b02b-611ae0f6afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents for use in generating testset with RAGAS\n",
    "loader = DirectoryLoader(\"./Speeches/titleedits\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f71778-7d81-4510-972d-c63496ade675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Speeches/titleedits/state_of_the_union_042921.txt'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[3].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83aac065-04c4-46fc-baed-cdb9bf2d9825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add 'filename' metadata for RAGAS\n",
    "for document in documents:\n",
    "    document.metadata['filename'] = document.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893a9a9-ced5-4355-9155-6d6ca7d9fedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b846e0-b319-4e00-ae3a-8796cb4be771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator with Gemini models\n",
    "generator_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", timeout=240, max_output_tokens=8192) #, temperature=0.7, timeout=180, transport=\"rest\"\n",
    "critic_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", timeout=240) #timeout=180\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\", timeout=240) #transport=\"rest\" #, request_options={\"timeout\": 10} #, request_options={\"maxConcurrency\": 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06863fa3-f603-4a30-b2ba-44130f3d310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_wrapper = LangchainLLMWrapper(generator_llm)\n",
    "embeddings_wrapper = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "139d2c27-e0c8-4b88-8188-dbf40ba7b6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n",
      "unable to apply transformation: The LLM generation was not completed. Please increase try increasing the max_tokens and try again.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n",
      "unable to apply transformation: node.property('summary') must be a string, found '<class 'NoneType'>'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.testset.transforms.engine:unable to apply transformation: Node 06e5b239-3c97-4ab3-84a4-6d819df54e27 has no summary_embedding\n",
      "unable to apply transformation: Node 06e5b239-3c97-4ab3-84a4-6d819df54e27 has no summary_embedding\n",
      "INFO:ragas.testset.synthesizers.multi_hop.abstract:found 0 clusters\n",
      "found 0 clusters\n",
      "INFO:ragas.testset.synthesizers.multi_hop.specific:found 0 clusters\n",
      "found 0 clusters\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No nodes that satisfied the given filer. Try changing the filter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Increase the timeout settings\u001b[39;00m\n\u001b[1;32m      3\u001b[0m my_run_config \u001b[38;5;241m=\u001b[39m RunConfig(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m, max_wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m, max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Increase timeout to 120 seconds\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_run_config\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# stopped here\u001b[39;00m\n",
      "File \u001b[0;32m~/newRAGAS/ragas-0.2.6/src/ragas/testset/synthesizers/generate.py:188\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[0;34m(self, documents, testset_size, transforms, transforms_llm, transforms_embedding_model, query_distribution, run_config, callbacks, with_debugging_logs, raise_exceptions)\u001b[0m\n\u001b[1;32m    185\u001b[0m apply_transforms(kg, transforms)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknowledge_graph \u001b[38;5;241m=\u001b[39m kg\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtestset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestset_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_distribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_distribution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newRAGAS/ragas-0.2.6/src/ragas/testset/synthesizers/generate.py:369\u001b[0m, in \u001b[0;36mTestsetGenerator.generate\u001b[0;34m(self, testset_size, query_distribution, num_personas, run_config, batch_size, callbacks, token_usage_parser, with_debugging_logs, raise_exceptions)\u001b[0m\n\u001b[1;32m    366\u001b[0m     patch_logger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mragas.experimental.testset.transforms\u001b[39m\u001b[38;5;124m\"\u001b[39m, logging\u001b[38;5;241m.\u001b[39mDEBUG)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersona_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersona_list \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_personas_from_kg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknowledge_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_personas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_personas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersona_list)\n",
      "File \u001b[0;32m~/newRAGAS/ragas-0.2.6/src/ragas/testset/persona.py:95\u001b[0m, in \u001b[0;36mgenerate_personas_from_kg\u001b[0;34m(kg, llm, persona_generation_prompt, num_personas, filter_fn, callbacks)\u001b[0m\n\u001b[1;32m     93\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [node \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m kg\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;28;01mif\u001b[39;00m filter_fn(node)]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nodes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo nodes that satisfied the given filer. Try changing the filter.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m summaries \u001b[38;5;241m=\u001b[39m [node\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes]\n\u001b[1;32m    100\u001b[0m summaries \u001b[38;5;241m=\u001b[39m [summary \u001b[38;5;28;01mfor\u001b[39;00m summary \u001b[38;5;129;01min\u001b[39;00m summaries \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(summary, \u001b[38;5;28mstr\u001b[39m)]\n",
      "\u001b[0;31mValueError\u001b[0m: No nodes that satisfied the given filer. Try changing the filter."
     ]
    }
   ],
   "source": [
    "# Not working with RAGAS 2.6\n",
    "generator = TestsetGenerator(llm=generator_wrapper, embedding_model=embeddings_wrapper)\n",
    "# Increase the timeout settings\n",
    "my_run_config = RunConfig(timeout=120, max_wait=70, max_workers=1)  # Increase timeout to 120 seconds\n",
    "dataset = generator.generate_with_langchain_docs(documents, testset_size=10, run_config=my_run_config) # stopped here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd22ec1-0bb5-4e12-8401-3866b2900d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea76249e-a95d-421f-bc02-a290bbc94e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate testset\n",
    "def generate_testset_rate(docs):\n",
    "    \"\"\"\n",
    "    Calls the model and embeddings with rate limit run_config\n",
    "    \"\"\"\n",
    "    testset = generator.generate_with_langchain_docs(docs, test_size=50, distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}, raise_exceptions=True, run_config=run_config, is_async=False)\n",
    "    return testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6006a-7672-4bc2-80ee-af126e85d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RAGAS internal RunConfig settings do a decent job at limiting the 429 resource exhausted warnings when max_workers=1\n",
    "# Still very difficult to have this run successfully\n",
    "# Tried ratelimit and backoff libraries in Python... didn't affect it enough, got so many warnings that it wouldn't finish\n",
    "# Occasionally even the 1 max worker will not finish, but it will finish sometimes\n",
    "testset = generate_testset_rate(documents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baaa2b4b-8008-49bb-8387-b4dc901b993d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtestset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testset' is not defined"
     ]
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87fd9f5e-145f-4f79-a430-07c32e5b7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_pd = testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9bb052d-d942-4db4-a848-1a4e733c840a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does the author argue is under attack reg...</td>\n",
       "      <td>[ workers they need and families don’t wait de...</td>\n",
       "      <td>The author argues that the constitutional righ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_03012...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the speech from President Biden, why has \"t...</td>\n",
       "      <td>[ more than a million dollars a year and pay a...</td>\n",
       "      <td>President Biden states that \"trickle-down econ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_04292...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the speaker's stance on the \"Buy Ameri...</td>\n",
       "      <td>[ right here in America where they belong!\\n\\n...</td>\n",
       "      <td>The speaker believes in the \"Buy American\" pol...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_03072...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What measures does the speaker propose to lowe...</td>\n",
       "      <td>[- a parent, a spouse, or child.\\n\\nAnd fourth...</td>\n",
       "      <td>The speaker proposes that Medicare should be g...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_04292...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are President Biden's proposals for addre...</td>\n",
       "      <td>[omes destroyed, neighborhoods in rubble, citi...</td>\n",
       "      <td>President Biden proposes a six-week ceasefire ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_03072...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does Biden compare a threat to American de...</td>\n",
       "      <td>[’s argue over it, let’s debate it, but let’s ...</td>\n",
       "      <td>The context discusses the threat to American d...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_04292...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What gun violence prevention actions did Presi...</td>\n",
       "      <td>[ job at another burger place to make a couple...</td>\n",
       "      <td>President Biden mentioned the need to ban assa...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_02072...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What economic policy did President Biden rejec...</td>\n",
       "      <td>[- a parent, a spouse, or child.\\n\\nAnd fourth...</td>\n",
       "      <td>President Biden rejected trickle-down economic...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_04292...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How does the President's plan address societal...</td>\n",
       "      <td>[ workers they need and families don’t wait de...</td>\n",
       "      <td>The President's plan addresses societal issues...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_03012...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How will Biden address China's trade practices...</td>\n",
       "      <td>[ more than a million dollars a year and pay a...</td>\n",
       "      <td>Biden stated that he will defend America's int...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'Speeches/state_of_the_union_04292...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What does the author argue is under attack reg...   \n",
       "1  In the speech from President Biden, why has \"t...   \n",
       "2  What is the speaker's stance on the \"Buy Ameri...   \n",
       "3  What measures does the speaker propose to lowe...   \n",
       "4  What are President Biden's proposals for addre...   \n",
       "5  How does Biden compare a threat to American de...   \n",
       "6  What gun violence prevention actions did Presi...   \n",
       "7  What economic policy did President Biden rejec...   \n",
       "8  How does the President's plan address societal...   \n",
       "9  How will Biden address China's trade practices...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [ workers they need and families don’t wait de...   \n",
       "1  [ more than a million dollars a year and pay a...   \n",
       "2  [ right here in America where they belong!\\n\\n...   \n",
       "3  [- a parent, a spouse, or child.\\n\\nAnd fourth...   \n",
       "4  [omes destroyed, neighborhoods in rubble, citi...   \n",
       "5  [’s argue over it, let’s debate it, but let’s ...   \n",
       "6  [ job at another burger place to make a couple...   \n",
       "7  [- a parent, a spouse, or child.\\n\\nAnd fourth...   \n",
       "8  [ workers they need and families don’t wait de...   \n",
       "9  [ more than a million dollars a year and pay a...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  The author argues that the constitutional righ...         simple   \n",
       "1  President Biden states that \"trickle-down econ...         simple   \n",
       "2  The speaker believes in the \"Buy American\" pol...         simple   \n",
       "3  The speaker proposes that Medicare should be g...         simple   \n",
       "4  President Biden proposes a six-week ceasefire ...         simple   \n",
       "5  The context discusses the threat to American d...      reasoning   \n",
       "6  President Biden mentioned the need to ban assa...      reasoning   \n",
       "7  President Biden rejected trickle-down economic...  multi_context   \n",
       "8  The President's plan addresses societal issues...  multi_context   \n",
       "9  Biden stated that he will defend America's int...      reasoning   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'Speeches/state_of_the_union_03012...          True  \n",
       "1  [{'source': 'Speeches/state_of_the_union_04292...          True  \n",
       "2  [{'source': 'Speeches/state_of_the_union_03072...          True  \n",
       "3  [{'source': 'Speeches/state_of_the_union_04292...          True  \n",
       "4  [{'source': 'Speeches/state_of_the_union_03072...          True  \n",
       "5  [{'source': 'Speeches/state_of_the_union_04292...          True  \n",
       "6  [{'source': 'Speeches/state_of_the_union_02072...          True  \n",
       "7  [{'source': 'Speeches/state_of_the_union_04292...          True  \n",
       "8  [{'source': 'Speeches/state_of_the_union_03012...          True  \n",
       "9  [{'source': 'Speeches/state_of_the_union_04292...          True  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbca3ac8-56b1-4af6-8ba1-a8ec25549a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset_pd.to_csv('datasets/testset_flash_pro15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e411a9d-7aa3-4783-aa48-d6467a2b25d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code uses the resulting testset without generated answers to generate new answers\n",
    "\n",
    "#  use if imported testset_pd from csv without answers\n",
    "# this is a fix for 'contexts' column being saved as a string; needs to be a list\n",
    "testset_pd = pd.read_csv(\"datasets/testset_flash_pro15.csv\", index_col = None)\n",
    "testset_pd['contexts'] = testset_pd['contexts'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dfd0ca-b9cd-4003-b6e3-addfc065a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate answer column, per these two issues\n",
    "# https://github.com/explodinggradients/ragas/issues/1145\n",
    "# https://github.com/explodinggradients/ragas/issues/1084#issuecomment-2248219601\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "answers = [query_engine.query(q) for q in testset_pd['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33893af4-a200-4df9-9843-2e21a27d6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out new 'answer' and 'contexts' columns\n",
    "answers_r = []\n",
    "context_n = []\n",
    "for i in answers:\n",
    "    answers_r.append(i.response)\n",
    "    context_n.append([c.node.get_content() for c in i.source_nodes])\n",
    "\n",
    "testset_pd = testset_pd.rename(columns={\"contexts\":\"contexts_gt\"})\n",
    "testset_pd['contexts'] = context_n\n",
    "testset_pd['answer'] = answers_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fbfc8109-0336-43b4-bf49-1ed6142b5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_pd.to_csv('datasets/testset_answer_newcontext_flash_pro15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7866f0b2-d32b-45af-8a91-7d77fbb0a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_pd = pd.read_csv(\"datasets/unlabeled_dataset/unlabeled_dataset.csv\", index_col = None) # datasets/manual_dataset_complete.csv # testset_answer_newcontext_flash_pro15.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceaa4148-aba2-4dab-8c04-d263de3c82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_pd = testset_pd.rename(columns={\"Query\": \"user_input\", \"Answer\": \"response\", \"Expected_Output\": \"reference\", \"Contexts\": \"orig_contexts\", \"Source_File\":\"source_file\", \"Document\": \"retrieved_contexts\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2492f767-6a59-472d-966c-4992b4cbcbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>orig_contexts</th>\n",
       "      <th>source_file</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Identify specific examples of government inves...</td>\n",
       "      <td>The transcontinental railroad and the intersta...</td>\n",
       "      <td>The speech highlights several examples: the tr...</td>\n",
       "      <td>; discovering vaccines; gave us the Internet a...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>['Throughout our history, if you think about i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does the American Jobs Plan, a large-scale inv...</td>\n",
       "      <td>The plan seeks to create jobs by modernizing i...</td>\n",
       "      <td>The American Jobs Plan aims to create jobs by ...</td>\n",
       "      <td>; discovering vaccines; gave us the Internet a...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>['The American Jobs Plan creates jobs replacin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Considering the significant impact of cancer o...</td>\n",
       "      <td>Investing in cancer research is a priority bec...</td>\n",
       "      <td>Investing in cancer research is a priority bec...</td>\n",
       "      <td>But so many of us have deceased sons, daughter...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>['But so many of us have deceased sons, daught...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the President's viewpoint on infrastr...</td>\n",
       "      <td>The President believes that infrastructure inv...</td>\n",
       "      <td>The President emphasizes that infrastructure i...</td>\n",
       "      <td>But so many of us have deceased sons, daughter...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>['Investments in jobs and infrastructure, like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analyze the potential economic consequences of...</td>\n",
       "      <td>A progressive tax structure, where higher earn...</td>\n",
       "      <td>The President advocates for raising taxes on c...</td>\n",
       "      <td>you should be able to become a billionaire an...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>['When you hear someone say that they don’t wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Explain how the president's call for aid to Ga...</td>\n",
       "      <td>The president emphasizes the importance of inc...</td>\n",
       "      <td>The president's call for aid to Gaza is direct...</td>\n",
       "      <td>I say we must stop it.  \\n\\nI’m proud we beat ...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>['Tonight, I’m directing the U.S. military to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>If a ceasefire were to fail, how would the Pre...</td>\n",
       "      <td>The President's proposed humanitarian efforts ...</td>\n",
       "      <td>The President's proposal for a temporary pier ...</td>\n",
       "      <td>I say we must stop it.  \\n\\nI’m proud we beat ...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>['Tonight, I’m directing the U.S. military to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>What steps is the US President taking to achie...</td>\n",
       "      <td>The US President is directing the military to ...</td>\n",
       "      <td>The US President is working to achieve a cease...</td>\n",
       "      <td>I say we must stop it.  \\n\\nI’m proud we beat ...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>['Tonight, I’m directing the U.S. military to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Identify the central theme of the President's ...</td>\n",
       "      <td>The President strongly advocates for reproduct...</td>\n",
       "      <td>The President's opening remarks regarding repr...</td>\n",
       "      <td>no place in America! \\n\\nHistory is watching....</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>['Like most Americans, I believe Roe v. Wade g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Analyze the speech's rhetorical strategies in ...</td>\n",
       "      <td>The speech employs several rhetorical strategi...</td>\n",
       "      <td>The speech uses several rhetorical strategies ...</td>\n",
       "      <td>no place in America! \\n\\nHistory is watching....</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>['In its decision to overturn Roe v. Wade the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            user_input  \\\n",
       "0    Identify specific examples of government inves...   \n",
       "1    Does the American Jobs Plan, a large-scale inv...   \n",
       "2    Considering the significant impact of cancer o...   \n",
       "3    How does the President's viewpoint on infrastr...   \n",
       "4    Analyze the potential economic consequences of...   \n",
       "..                                                 ...   \n",
       "795  Explain how the president's call for aid to Ga...   \n",
       "796  If a ceasefire were to fail, how would the Pre...   \n",
       "797  What steps is the US President taking to achie...   \n",
       "798  Identify the central theme of the President's ...   \n",
       "799  Analyze the speech's rhetorical strategies in ...   \n",
       "\n",
       "                                              response  \\\n",
       "0    The transcontinental railroad and the intersta...   \n",
       "1    The plan seeks to create jobs by modernizing i...   \n",
       "2    Investing in cancer research is a priority bec...   \n",
       "3    The President believes that infrastructure inv...   \n",
       "4    A progressive tax structure, where higher earn...   \n",
       "..                                                 ...   \n",
       "795  The president emphasizes the importance of inc...   \n",
       "796  The President's proposed humanitarian efforts ...   \n",
       "797  The US President is directing the military to ...   \n",
       "798  The President strongly advocates for reproduct...   \n",
       "799  The speech employs several rhetorical strategi...   \n",
       "\n",
       "                                             reference  \\\n",
       "0    The speech highlights several examples: the tr...   \n",
       "1    The American Jobs Plan aims to create jobs by ...   \n",
       "2    Investing in cancer research is a priority bec...   \n",
       "3    The President emphasizes that infrastructure i...   \n",
       "4    The President advocates for raising taxes on c...   \n",
       "..                                                 ...   \n",
       "795  The president's call for aid to Gaza is direct...   \n",
       "796  The President's proposal for a temporary pier ...   \n",
       "797  The US President is working to achieve a cease...   \n",
       "798  The President's opening remarks regarding repr...   \n",
       "799  The speech uses several rhetorical strategies ...   \n",
       "\n",
       "                                         orig_contexts  \\\n",
       "0    ; discovering vaccines; gave us the Internet a...   \n",
       "1    ; discovering vaccines; gave us the Internet a...   \n",
       "2    But so many of us have deceased sons, daughter...   \n",
       "3    But so many of us have deceased sons, daughter...   \n",
       "4     you should be able to become a billionaire an...   \n",
       "..                                                 ...   \n",
       "795  I say we must stop it.  \\n\\nI’m proud we beat ...   \n",
       "796  I say we must stop it.  \\n\\nI’m proud we beat ...   \n",
       "797  I say we must stop it.  \\n\\nI’m proud we beat ...   \n",
       "798   no place in America! \\n\\nHistory is watching....   \n",
       "799   no place in America! \\n\\nHistory is watching....   \n",
       "\n",
       "                                           source_file  \\\n",
       "0    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "1    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "2    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "3    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "4    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "..                                                 ...   \n",
       "795  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "796  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "797  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "798  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "799  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "\n",
       "                                    retrieved_contexts  \n",
       "0    ['Throughout our history, if you think about i...  \n",
       "1    ['The American Jobs Plan creates jobs replacin...  \n",
       "2    ['But so many of us have deceased sons, daught...  \n",
       "3    ['Investments in jobs and infrastructure, like...  \n",
       "4    ['When you hear someone say that they don’t wa...  \n",
       "..                                                 ...  \n",
       "795  ['Tonight, I’m directing the U.S. military to ...  \n",
       "796  ['Tonight, I’m directing the U.S. military to ...  \n",
       "797  ['Tonight, I’m directing the U.S. military to ...  \n",
       "798  ['Like most Americans, I believe Roe v. Wade g...  \n",
       "799  ['In its decision to overturn Roe v. Wade the ...  \n",
       "\n",
       "[800 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c991eaa9-9b22-4807-a149-d7cb28523e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if imported testset_pd from csv\n",
    "# this is a fix for 'contexts' column being saved as a string; needs to be a list\n",
    "\n",
    "testset_pd['retrieved_contexts'] = testset_pd['retrieved_contexts'].apply(ast.literal_eval)\n",
    "#testset_pd['contexts_gt'] = testset_pd['contexts_gt'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd8adaf0-d20c-4d55-b25e-73b84da5b063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>orig_contexts</th>\n",
       "      <th>source_file</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Identify specific examples of government inves...</td>\n",
       "      <td>The transcontinental railroad and the intersta...</td>\n",
       "      <td>The speech highlights several examples: the tr...</td>\n",
       "      <td>; discovering vaccines; gave us the Internet a...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>[Throughout our history, if you think about it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does the American Jobs Plan, a large-scale inv...</td>\n",
       "      <td>The plan seeks to create jobs by modernizing i...</td>\n",
       "      <td>The American Jobs Plan aims to create jobs by ...</td>\n",
       "      <td>; discovering vaccines; gave us the Internet a...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>[The American Jobs Plan creates jobs replacing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Considering the significant impact of cancer o...</td>\n",
       "      <td>Investing in cancer research is a priority bec...</td>\n",
       "      <td>Investing in cancer research is a priority bec...</td>\n",
       "      <td>But so many of us have deceased sons, daughter...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>[But so many of us have deceased sons, daughte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does the President's viewpoint on infrastr...</td>\n",
       "      <td>The President believes that infrastructure inv...</td>\n",
       "      <td>The President emphasizes that infrastructure i...</td>\n",
       "      <td>But so many of us have deceased sons, daughter...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>[Investments in jobs and infrastructure, like ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analyze the potential economic consequences of...</td>\n",
       "      <td>A progressive tax structure, where higher earn...</td>\n",
       "      <td>The President advocates for raising taxes on c...</td>\n",
       "      <td>you should be able to become a billionaire an...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_042921.txt</td>\n",
       "      <td>[When you hear someone say that they don’t wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Explain how the president's call for aid to Ga...</td>\n",
       "      <td>The president emphasizes the importance of inc...</td>\n",
       "      <td>The president's call for aid to Gaza is direct...</td>\n",
       "      <td>I say we must stop it.  \\n\\nI’m proud we beat ...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>[Tonight, I’m directing the U.S. military to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>If a ceasefire were to fail, how would the Pre...</td>\n",
       "      <td>The President's proposed humanitarian efforts ...</td>\n",
       "      <td>The President's proposal for a temporary pier ...</td>\n",
       "      <td>I say we must stop it.  \\n\\nI’m proud we beat ...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>[Tonight, I’m directing the U.S. military to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>What steps is the US President taking to achie...</td>\n",
       "      <td>The US President is directing the military to ...</td>\n",
       "      <td>The US President is working to achieve a cease...</td>\n",
       "      <td>I say we must stop it.  \\n\\nI’m proud we beat ...</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>[Tonight, I’m directing the U.S. military to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Identify the central theme of the President's ...</td>\n",
       "      <td>The President strongly advocates for reproduct...</td>\n",
       "      <td>The President's opening remarks regarding repr...</td>\n",
       "      <td>no place in America! \\n\\nHistory is watching....</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>[Like most Americans, I believe Roe v. Wade go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Analyze the speech's rhetorical strategies in ...</td>\n",
       "      <td>The speech employs several rhetorical strategi...</td>\n",
       "      <td>The speech uses several rhetorical strategies ...</td>\n",
       "      <td>no place in America! \\n\\nHistory is watching....</td>\n",
       "      <td>Speeches/titleedits/state_of_the_union_030724.txt</td>\n",
       "      <td>[In its decision to overturn Roe v. Wade the S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            user_input  \\\n",
       "0    Identify specific examples of government inves...   \n",
       "1    Does the American Jobs Plan, a large-scale inv...   \n",
       "2    Considering the significant impact of cancer o...   \n",
       "3    How does the President's viewpoint on infrastr...   \n",
       "4    Analyze the potential economic consequences of...   \n",
       "..                                                 ...   \n",
       "795  Explain how the president's call for aid to Ga...   \n",
       "796  If a ceasefire were to fail, how would the Pre...   \n",
       "797  What steps is the US President taking to achie...   \n",
       "798  Identify the central theme of the President's ...   \n",
       "799  Analyze the speech's rhetorical strategies in ...   \n",
       "\n",
       "                                              response  \\\n",
       "0    The transcontinental railroad and the intersta...   \n",
       "1    The plan seeks to create jobs by modernizing i...   \n",
       "2    Investing in cancer research is a priority bec...   \n",
       "3    The President believes that infrastructure inv...   \n",
       "4    A progressive tax structure, where higher earn...   \n",
       "..                                                 ...   \n",
       "795  The president emphasizes the importance of inc...   \n",
       "796  The President's proposed humanitarian efforts ...   \n",
       "797  The US President is directing the military to ...   \n",
       "798  The President strongly advocates for reproduct...   \n",
       "799  The speech employs several rhetorical strategi...   \n",
       "\n",
       "                                             reference  \\\n",
       "0    The speech highlights several examples: the tr...   \n",
       "1    The American Jobs Plan aims to create jobs by ...   \n",
       "2    Investing in cancer research is a priority bec...   \n",
       "3    The President emphasizes that infrastructure i...   \n",
       "4    The President advocates for raising taxes on c...   \n",
       "..                                                 ...   \n",
       "795  The president's call for aid to Gaza is direct...   \n",
       "796  The President's proposal for a temporary pier ...   \n",
       "797  The US President is working to achieve a cease...   \n",
       "798  The President's opening remarks regarding repr...   \n",
       "799  The speech uses several rhetorical strategies ...   \n",
       "\n",
       "                                         orig_contexts  \\\n",
       "0    ; discovering vaccines; gave us the Internet a...   \n",
       "1    ; discovering vaccines; gave us the Internet a...   \n",
       "2    But so many of us have deceased sons, daughter...   \n",
       "3    But so many of us have deceased sons, daughter...   \n",
       "4     you should be able to become a billionaire an...   \n",
       "..                                                 ...   \n",
       "795  I say we must stop it.  \\n\\nI’m proud we beat ...   \n",
       "796  I say we must stop it.  \\n\\nI’m proud we beat ...   \n",
       "797  I say we must stop it.  \\n\\nI’m proud we beat ...   \n",
       "798   no place in America! \\n\\nHistory is watching....   \n",
       "799   no place in America! \\n\\nHistory is watching....   \n",
       "\n",
       "                                           source_file  \\\n",
       "0    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "1    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "2    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "3    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "4    Speeches/titleedits/state_of_the_union_042921.txt   \n",
       "..                                                 ...   \n",
       "795  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "796  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "797  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "798  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "799  Speeches/titleedits/state_of_the_union_030724.txt   \n",
       "\n",
       "                                    retrieved_contexts  \n",
       "0    [Throughout our history, if you think about it...  \n",
       "1    [The American Jobs Plan creates jobs replacing...  \n",
       "2    [But so many of us have deceased sons, daughte...  \n",
       "3    [Investments in jobs and infrastructure, like ...  \n",
       "4    [When you hear someone say that they don’t wan...  \n",
       "..                                                 ...  \n",
       "795  [Tonight, I’m directing the U.S. military to l...  \n",
       "796  [Tonight, I’m directing the U.S. military to l...  \n",
       "797  [Tonight, I’m directing the U.S. military to l...  \n",
       "798  [Like most Americans, I believe Roe v. Wade go...  \n",
       "799  [In its decision to overturn Roe v. Wade the S...  \n",
       "\n",
       "[800 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d735bf4-b628-4b08-b504-61c67b137d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per this ragas thread, need to convert pandas testset to Dataset format for evaluate to work\n",
    "# https://github.com/explodinggradients/ragas/issues/803\n",
    "#testset_ds = Dataset.from_pandas(testset_pd.drop(\"contexts_gt\", axis=1))\n",
    "testset_ds = Dataset.from_pandas(testset_pd.drop(\"orig_contexts\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "104fced3-164c-44df-a0b2-55e953d33d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_input', 'response', 'reference', 'source_file', 'retrieved_contexts'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e675be85-9047-40d4-a9a3-95311a8fc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = EvaluationDataset.from_hf_dataset(testset_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcc219c6-499c-43e6-a3ea-d6b264f96aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=800)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a39ae55f-204a-4b42-8811-710bd26502cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: I'm using the normal LLM, not the RAG context-loaded query engine\n",
    "# There is code at the bottom of the notebook for using the query engine, which should be the way to go\n",
    "# However, that code appears to be broken from RAGAS right now, so I was forced to use the regular Gemini LLM\n",
    "# It appears that evaluate (below) may re-run the query and gives new answers and contexts anyway...\n",
    "# Watch this issue: https://github.com/explodinggradients/ragas/issues/1211\n",
    "\n",
    "ragas_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", timeout=120) # try request_timeout=120\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47977c82-14b7-4291-9b53-3c9ce0510487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the timeout settings\n",
    "my_run_config = RunConfig(timeout=120, max_wait=70, max_workers=1)  # Increase timeout to 120 seconds\n",
    "#run_config = RunConfig(timeout=600, max_wait=600, max_workers=1, max_retries=10)  # Increase timeout to 180 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2696df06-4744-4e0d-838a-1f2342454f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_precision = LLMContextPrecisionWithoutReference(llm=generator_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d845d9f-6d08-4fde-8ded-361b7d12e3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b289896951844756aa66c9bf8ef13395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.executor:Exception raised in Job[0]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[0]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[1]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[1]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[2]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[2]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[3]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[3]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[4]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[4]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[5]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[5]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[6]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[6]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[7]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[7]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# old method\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Optional parameter: \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# in_ci: bool, Whether the evaluation is running in CI or not. \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# If set to True then some metrics will be run to increase the reproducability of the evaluations. \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This will increase the runtime and cost of evaluations. Default is False.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m evalresult \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext_precision\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#faithfulness,\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#answer_relevancy,\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#context_recall\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtestset_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgenerator_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_run_config\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newRAGAS/ragas-0.2.6/src/ragas/_analytics.py:130\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m    129\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 130\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/newRAGAS/ragas-0.2.6/src/ragas/evaluation.py:303\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size)\u001b[0m\n\u001b[1;32m    300\u001b[0m scores: t\u001b[38;5;241m.\u001b[39mList[t\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, t\u001b[38;5;241m.\u001b[39mAny]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[0;32m~/newRAGAS/ragas-0.2.6/src/ragas/executor.py:200\u001b[0m, in \u001b[0;36mExecutor.results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m             nest_asyncio\u001b[38;5;241m.\u001b[39mapply()\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nest_asyncio_applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "File \u001b[0;32m~/anaconda3/envs/gemini_ragas/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/anaconda3/envs/gemini_ragas/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gemini_ragas/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/anaconda3/envs/gemini_ragas/lib/python3.11/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.executor:Exception raised in Job[8]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[8]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[9]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[9]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[10]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[10]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[11]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[11]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[12]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[12]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[13]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[13]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[14]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[14]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[15]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[15]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[16]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[16]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "ERROR:ragas.executor:Exception raised in Job[17]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[17]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[18]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[18]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[19]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[19]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[20]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[20]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[21]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[21]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[22]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[22]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[23]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[23]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[24]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[24]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[25]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[25]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[26]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[26]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[27]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[27]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[28]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[28]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[29]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[29]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[30]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[30]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[31]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[31]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[32]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[32]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[33]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[33]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "ERROR:ragas.executor:Exception raised in Job[34]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[34]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[35]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[35]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[36]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[36]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[37]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[37]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[38]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[38]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[39]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[39]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[40]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[40]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[41]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[41]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[42]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[42]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[43]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[43]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[44]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[44]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[45]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[45]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[46]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[46]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[47]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[47]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[48]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[48]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "ERROR:ragas.executor:Exception raised in Job[49]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[49]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "ERROR:ragas.executor:Exception raised in Job[50]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "Exception raised in Job[50]: LLMDidNotFinishException(The LLM generation was not completed. Please increase try increasing the max_tokens and try again.)\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    }
   ],
   "source": [
    "# old method\n",
    "# Optional parameter: \n",
    "# in_ci: bool, Whether the evaluation is running in CI or not. \n",
    "# If set to True then some metrics will be run to increase the reproducability of the evaluations. \n",
    "# This will increase the runtime and cost of evaluations. Default is False.\n",
    "evalresult = evaluate(\n",
    "    metrics = [\n",
    "        context_precision\n",
    "        #faithfulness,\n",
    "        #answer_relevancy,\n",
    "        #context_recall\n",
    "    ],\n",
    "    dataset = testset_ds,\n",
    "    llm = generator_wrapper,\n",
    "    embeddings=embeddings_wrapper,\n",
    "    run_config=my_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75a03c5c-b6a2-4a15-9d62-b9a6890dcd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.6944, 'faithfulness': 1.0000, 'answer_relevancy': 0.6770, 'context_recall': 0.6455}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5ecd8cf-8642-4ea0-ac1a-c6bcbb05a621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.5979, 'faithfulness': 1.0000, 'answer_relevancy': 0.6533, 'context_recall': 0.8000}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example result:\n",
    "# {'context_precision': 0.4676, 'faithfulness': 1.0000, 'answer_relevancy': 0.6515, 'context_recall': 0.8000}\n",
    "# Reran\n",
    "# {'context_precision': 0.5979, 'faithfulness': 1.0000, 'answer_relevancy': 0.6533, 'context_recall': 0.8000}\n",
    "evalresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb0a5aa-f1b4-4945-957b-9e30a0dd43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: received warning for the answer where there was no response from the llm, definitely reduced faithfulness score\n",
    "\n",
    "# Results:\n",
    "# Using contexts generated when produced answers from LLM:\n",
    "# new testset_answer_newcontext_flash_pro15.csv result, with contexts_gt (aka contexts generated with ground truth generation) column removed\n",
    "# {'context_precision': 0.4171, 'faithfulness': 0.9167, 'answer_relevancy': 0.6509, 'context_recall': 0.8000}\n",
    "# reran\n",
    "# {'context_precision': 0.4676, 'faithfulness': 1.0000, 'answer_relevancy': 0.6515, 'context_recall': 0.8000}\n",
    "# reran\n",
    "# {'context_precision': 0.5979, 'faithfulness': 1.0000, 'answer_relevancy': 0.6533, 'context_recall': 0.8000}\n",
    "\n",
    "# Compared to using contexts generated for ground truth (probably not correct):\n",
    "# new testset_answer_newcontext_flash_pro15.csv result, using old contexts\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.7392, 'answer_relevancy': 0.6041, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.8500, 'faithfulness': 0.7123, 'answer_relevancy': 0.5934, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.6556, 'answer_relevancy': 0.5638, 'context_recall': 1.0000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a027af-7dff-483f-97ff-9072e71d7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation results on metrics:\n",
    "\n",
    "# RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics\n",
    "# I don't have example ranges to compare anything to, so below is my best guess.\n",
    "\n",
    "# Faithfulness - Measures the factual consistency of the answer to the context based on the question.\n",
    "# 0.9167 - 1.0000 indicates that the LLM is staying true to the facts provided in the context for answering the question.\n",
    "# There is another Faithfulness metric: from ragas.metrics import FaithulnesswithHHEM\n",
    "# This uses a huggingface model to help detect hallucination : https://huggingface.co/vectara/hallucination_evaluation_model\n",
    "# See below for code : {'faithfulness_with_hhem': 0.6319} \n",
    "# This doesn't really agree with the RAGAS faithfulness score... may need to dive in further another time.\n",
    "# Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.\n",
    "# At 0.4171 - 0.5979, suggests that the context isn't particularly relevant to the question.\n",
    "# Answer_relevancy - Measures how relevant the answer is to the question.\n",
    "# 0.6509 - 0.6533 seems moderately low, just going off of the number.\n",
    "# Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.\n",
    "# 0.8 indicates that the llm context is decently good and can typically answer the question or most of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "04f4d426-e499-475e-8459-a497368b8b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run, just compare to using contexts_gt column instead of the newer context generated with the answer\n",
    "testset_ds_oldcontext = Dataset.from_pandas(testset_pd.drop(\"contexts\", axis=1).rename(columns={'contexts_old':'contexts'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7b251-9833-4931-baf2-c575bf900721",
   "metadata": {},
   "outputs": [],
   "source": [
    "evalresult_old2 = evaluate(\n",
    "    testset_ds_oldcontext,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm = ragas_llm,\n",
    "    embeddings=embeddings, \n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7170d4-ca68-46b1-a962-3c71093374c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new testset_answer_newcontext_flash_pro15.csv result, using old contexts\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.7392, 'answer_relevancy': 0.6041, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.8500, 'faithfulness': 0.7123, 'answer_relevancy': 0.5934, 'context_recall': 1.0000}\n",
    "# reran:\n",
    "# {'context_precision': 0.7500, 'faithfulness': 0.6556, 'answer_relevancy': 0.5638, 'context_recall': 1.0000}\n",
    "evalresult_old2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79fac0-ea9d-473f-b33c-02ee96df9559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a0ae2-f6ae-47e4-bda8-1c66c0104ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS also has an additional Faithfulness with HHEM metric (yes- it is misspelled in their documentation) \n",
    "# that uses a HuggingFace model to detect hallucinations\n",
    "# Note: There's a message on HuggingFace about the token indices sequence length error being normal and an artifact; thus, ignoring the below error\n",
    "# https://huggingface.co/vectara/hallucination_evaluation_model\n",
    "from ragas.metrics import FaithulnesswithHHEM\n",
    "faithfulness_with_hhem = FaithulnesswithHHEM()\n",
    "result_faithfulness_hhem = evaluate(\n",
    "    testset_ds,\n",
    "    metrics=[faithfulness_with_hhem],\n",
    "    llm = ragas_llm,\n",
    "    embeddings=embeddings, \n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7f158bea-0167-43ab-b4fb-0b5c0bc145c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness_with_hhem': 0.6319}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with context from answer generation:\n",
    "# {'faithfulness_with_hhem': 0.6319}\n",
    "# testing: with context from ground truth/synthetic testset generation\n",
    "# {'faithfulness_with_hhem': 0.5241}\n",
    "# this seems to agree with the RAGAS faithfulness score in that answers seem to be partially made up.\n",
    "result_faithfulness_hhem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cbe4c-3937-4d48-829a-f0b262698e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "02abe059-1129-4f60-81b7-55cef6b958ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra non-working code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "38993aa7-31ed-438e-96c6-63de9c608016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to use the query_engine in the evaluation \n",
    "# Modeled after this tutorial: https://docs.ragas.io/en/latest/howtos/applications/compare_llms.html\n",
    "\n",
    "# Does not currently work: for some metrics, it is not finding the 'ground_truth' column in the dataset\n",
    "# For other metrics, appears to run but returns the below errors and returns 'nan' for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b983f067-90e5-465a-9014-26fc5593f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of testing to try and get rag query engine for evaluate\n",
    "def generate_responses(query_engine, test_questions, test_answers):\n",
    "  responses = [query_engine.query(q) for q in test_questions]\n",
    "\n",
    "  answers = []\n",
    "  contexts = []\n",
    "  for r in responses:\n",
    "    answers.append(r.response)\n",
    "    contexts.append([c.node.get_content() for c in r.source_nodes])\n",
    "  dataset_dict = {\n",
    "        \"question\": test_questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "  }\n",
    "  if test_answers is not None:\n",
    "    dataset_dict[\"ground_truth\"] = test_answers\n",
    "  ds = Dataset.from_dict(dataset_dict)\n",
    "  return ds\n",
    "\n",
    "test_questions = testset_pd['question'].values.tolist()\n",
    "test_answers = [[item] for item in testset_pd['answer'].values.tolist()]\n",
    "\n",
    "result_ds = generate_responses(query_engine, test_questions, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f75a3-602b-49b8-b269-dab9f3ce817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This evaluate function that uses the query_engine does not return results (nan for all metrics)\n",
    "# Errors (below are repeated many times):\n",
    "# WARNING:ragas.llms.base:n values greater than 1 not support for LlamaIndex LLMs\n",
    "# n values greater than 1 not support for LlamaIndex LLMs\n",
    "# INFO:ragas.llms.base:callbacks not supported for LlamaIndex LLMs, ignoring callbacks\n",
    "# callbacks not supported for LlamaIndex LLMs, ignoring callbacks\n",
    "# ERROR:ragas.executor:Exception raised in Job[5]: TimeoutError()\n",
    "# Exception raised in Job[5]: TimeoutError()\n",
    "# ERROR:ragas.executor:Exception raised in Job[19]: AttributeError('ChatGoogleGenerativeAI' object has no attribute 'acomplete')\n",
    "# Exception raised in Job[19]: AttributeError('ChatGoogleGenerativeAI' object has no attribute 'acomplete')\n",
    "\n",
    "from ragas.integrations.llama_index import evaluate\n",
    "\n",
    "eval_qe2 = evaluate(\n",
    "    query_engine=query_engine,\n",
    "    dataset=result_ds,\n",
    "    metrics=[faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_utilization],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=embeddings, \n",
    "    run_config=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "296c8c6d-737d-4233-ae5a-4114597a76f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': nan, 'answer_relevancy': nan, 'context_utilization': nan}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_qe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c19d1-71a0-40ba-ad28-f689cba75277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini_ragas",
   "language": "python",
   "name": "gemini_ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
