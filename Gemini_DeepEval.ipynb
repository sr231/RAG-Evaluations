{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e651c99a-fc6c-4b3e-927e-5b7dae9813fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial for setting up a small RAG system using Faiss \n",
    "# and evaluating it using the Gemini Flash 1.5 LLM and the DeepEval library\n",
    "# Google Gemini: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# DeepEval: https://docs.confident-ai.com/docs/guides-rag-evaluation\n",
    "\n",
    "# DeepEval v1.1.6 was fairly compatible with Google Gemini by creating a new LLM class that inherited from DeepEvalBaseLLM\n",
    "# and adding methods that called Gemini's generation functions; it was a similar setup for the Embeddings, inheriting from DeepEvalBaseEmbeddingModel\n",
    "# The only trick is that the LLM output needs to be in JSON format\n",
    "# I used the pydantic and instructor libraries for this; the following gives good examples of how to use them\n",
    "# Tutorial on using custom LLMs with DeepEval: https://docs.confident-ai.com/docs/guides-using-custom-llms\n",
    "\n",
    "# Metrics available in DeepEval:\n",
    "# - Contextual Precision: Evaluates whether the reranker in your retriever ranks more relevant nodes in your retrieval context higher than irrelevant ones.\n",
    "# - Contextual Recall: Evaluates whether the embedding model in your retriever is able to accurately capture and retrieve relevant information based on the context of the input.\n",
    "# - Contextual Relevance: Evaluates whether the text chunk size and top-K of your retriever is able to retrieve information without much irrelevancies.\n",
    "# - Answer Relevancy: Evaluates whether the prompt template in your generator is able to instruct your LLM to output relevant and helpful outputs based on the retrieval_context.\n",
    "# - Faithfulness: Evaluates whether the LLM used in your generator can output information that does not hallucinate AND contradict any factual information presented in the retrieval_context.\n",
    "# - Other metrics are available for non-RAG systems; custom metrics can also be created (I did not test this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dafebe7-76cf-41d8-8734-0f06d15b9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdda733-b425-42f1-831a-6d71952ac5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pydantic import BaseModel, ConfigDict # for JSON output from DeepEval\n",
    "import instructor # for JSON output from DeepEval\n",
    "\n",
    "# Replace these two Google Gemini imports with imports for your LLM\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss\n",
    "\n",
    "import deepeval\n",
    "from deepeval.models import DeepEvalBaseLLM, DeepEvalBaseEmbeddingModel\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from deepeval.synthesizer import Synthesizer\n",
    "from deepeval import evaluate\n",
    "from deepeval.evaluate import TestResult, print_test_result\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    FaithfulnessMetric\n",
    ")\n",
    "from deepeval.metrics.ragas import (\n",
    "    RagasMetric,\n",
    "    RAGASAnswerRelevancyMetric,\n",
    "    RAGASFaithfulnessMetric, \n",
    "    RAGASContextualRecallMetric,\n",
    "    RAGASContextualPrecisionMetric,\n",
    "    RAGASContextualRelevancyMetric\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8297c3a4-1366-4e38-9a4c-10f22905e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental variable to opt out of DeepEval tracking telemetry data\n",
    "os.environ[\"DEEPEVAL_TELEMETRY_OPT_OUT\"] = \"YES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03796ba-74cc-4bad-99a2-a594b3898399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepeval.telemetry_opt_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90ecc13-1002-4dad-82e5-cf42b656718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc3144c-2ac2-4448-a86a-a514ee26b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up local API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06fb133-0f27-4e72-8713-b8df5cda2f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c3cd9fd-5949-4b1e-9894-c26618049ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish RAG pipeline with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eab73af-ed30-4840-be7b-20eb1c66807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Faiss vector store for RAG\n",
    "# # If you already have an index created, skip a few coding cells to the LLM / embeddings setup\n",
    "\n",
    "# # Example of creating a small vector store\n",
    "# # Using 4 State of the Union speeches, all text from whitehouse.gov briefing room speeches posted online, edited to include a title with the date of the speech\n",
    "# # Example from 2024:\n",
    "# # https://www.whitehouse.gov/briefing-room/speeches-remarks/2024/03/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery-2/\n",
    "\n",
    "# # load and parse files\n",
    "# sotu = []\n",
    "# newfiles = [\"./Speeches/titleedits/state_of_the_union_042921.txt\", \"./Speeches/titleedits/state_of_the_union_030122.txt\", \"./Speeches/titleedits/state_of_the_union_020723.txt\", \"./Speeches/titleedits/state_of_the_union_030724.txt\"]\n",
    "# for i in newfiles:\n",
    "#     with open(i) as file:\n",
    "#         for line in file:\n",
    "#             nl = line.rstrip()\n",
    "#             if nl != '':\n",
    "#                 sotu.append(nl)\n",
    "\n",
    "# # convert into Document format\n",
    "# documents = [Document(text=line) for line in sotu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab816039-1c34-4073-84a6-e57ad84832e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='833ea164-b547-46ec-8854-cefdc83fbb10', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='May God protect our troops.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Example of a loaded Document line\n",
    "# documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41cce1c9-8df4-492b-9d15-18dd67cf3444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# # Set up the faiss index\n",
    "# d = 768 # dimensions of the input vector of the embedding model that we're going to use; in this case, the google embedding model\n",
    "# faiss_index = faiss.IndexFlatL2(d)\n",
    "# print(faiss_index.is_trained) # double check that the training worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48c5f84-8fc4-4970-9505-3c67705b151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the llm, embeddings, and Settings for Faiss \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\") # Can substitute any LangChain Chat Model\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # Can substitute any LangChain embedding model \n",
    "Settings.embed_model = doc_embeddings # used for LlamaIndex FaissVectorStore\n",
    "Settings.llm = llm # used for LlamaIndex FaissVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82ce1780-061d-4d6d-bf3c-6853db1eb4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment for when you need to re-embed and vectorize documents\n",
    "\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, show_progress=True\n",
    "# )\n",
    "\n",
    "# # Save index to disk\n",
    "# index.storage_context.persist()\n",
    "\n",
    "# # Save/remember index id for loading next time\n",
    "# index.index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d046f3-293f-4bac-be0c-bedc5d88012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have a saved index, load that index for RAG answer generation:\n",
    "\n",
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "# My local index id '3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5' uses the 4 speeches including a title that includes the date it was given\n",
    "index = load_index_from_storage(storage_context=storage_context, index_id='3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a1ba96f-37e3-4160-bf13-5536c2cbaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional- if you'd like to query your index\n",
    "# # Set up query and chat engines with the index\n",
    "# query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "# chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48698c67-efba-4fe3-a2e1-864a3f43131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query and response\n",
    "# query = \"In detail, what has the President done to improve the economy over the four years of his speeches?\"\n",
    "# response = query_engine.query(query) \n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff5c6ba9-c23c-4adb-ad14-f612ce6ed45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of setting up a chat engine with our index\n",
    "# chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')\n",
    "# query = \"You are an expert speech analyst and specialize in analyzing Presidential State of the Union speeches. Could you please analyze the speeches and generate 2 questions and answers from each speech, providing the document filename of each speech that relates to each question?\"\n",
    "# response = chat_engine.chat(query) \n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce50c0-fb99-421f-a198-e397f5411d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05980859-d7e3-44e8-bc4f-10a41fdd111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for DeepEval RAG evaluation library to work with Gemini and our local RAG setup\n",
    "\n",
    "# In DeepEval v1.1.6, an example Input/Output/Context is a 'LLMTestCase'\n",
    "# You can evaluate LLMTestCases individually or in a large batch with the evaluate function\n",
    "# Metric scores are produced as well as an LLM generated explanation for a given score \n",
    "\n",
    "# DeepEval can also synthetically generate data from documents - see Synthesizer and generate_goldens_from_docs, below. \n",
    "# Of the 3 libraries I tested, this function produced the most human-realistic queries.\n",
    "# DeepEval also offers other functionality besides RAG evaluation, including \"red teaming LLM applications for security vulnerabilities\"\n",
    "\n",
    "# https://docs.confident-ai.com/docs/guides-rag-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "002bf206-84b1-4bae-ad0c-d23dad165360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval requires a json response. In practice, this has led to malformed json returned from the llm, even with as simple of a schema as this, \n",
    "# but this and the LLM class can likely be refined to improve responses\n",
    "class Response(BaseModel):\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad054ba9-9fb0-4e3d-9f48-7e60eb379ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Open-AI usage requires a custom LLM class for using DeepEval\n",
    "# Tutorial with example code on using custom LLMs with DeepEval: https://docs.confident-ai.com/docs/guides-using-custom-llms\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "        model_config  = ConfigDict(protected_namespaces=())\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel: \n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Gemini 1.5 Flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b903c42-fc2d-4fe2-bb1e-faaa7de1f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, a custom embedding model class is required for non Open-AI embeddings\n",
    "# Tutorial on using custom embeddings with DeepEval: https://docs.confident-ai.com/docs/guides-using-custom-embedding-models \n",
    "class CustomGeminiEmbeddingModel(DeepEvalBaseEmbeddingModel):\n",
    "    def __init__(self):\n",
    "        model_config  = ConfigDict(protected_namespaces=())\n",
    "\n",
    "    def load_model(self):\n",
    "        return GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/text-embedding-004\"\n",
    "        )\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_query(text)\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    async def a_embed_text(self, text: str) -> List[float]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_query(text)\n",
    "\n",
    "    async def a_embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        embedding_model = self.load_model()\n",
    "        return await embedding_model.aembed_documents(texts)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        \"Custom Gemini Embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e11dcd46-4410-4ae9-bb68-3a386d6484c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom llm and embeddings\n",
    "custom_geminiflash = CustomGeminiFlash()\n",
    "custom_geminiembeddings = CustomGeminiEmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654da851-9125-4c34-b1e9-19add3a09b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset of \"Goldens\" (AKA a dataset with 'input', 'context', 'source_file' columns -- not 'Retrieval_Context') with DeepEval\n",
    "dataset = EvaluationDataset()\n",
    "synthesizer = Synthesizer(model=custom_geminiflash, embedder=custom_geminiembeddings)\n",
    "dataset.generate_goldens_from_docs(\n",
    "    synthesizer=synthesizer,\n",
    "    document_paths=['Speeches/titleedits/state_of_the_union_042921.txt', 'Speeches/titleedits/state_of_the_union_030122.txt', \n",
    "                    'Speeches/titleedits/state_of_the_union_020723.txt', 'Speeches/titleedits/state_of_the_union_030724.txt'],\n",
    "    max_goldens_per_document=3, # maximum number of questions to generate per document\n",
    "    include_expected_output=True\n",
    ")\n",
    "\n",
    "dataset.save_as(file_type=\"csv\", directory=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea3e3d-f1aa-406c-8128-3bbe6ce45872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1da71503-d604-4936-a243-c3d30b6461f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of evaluating one example/test case\n",
    "\n",
    "# Parameters in a DeepEval LLMTestCase:\n",
    "# Input: Question/query for the LLM\n",
    "# Actual Output: Answer returned from the LLM\n",
    "# Expected Output: The ideal output for the input/question\n",
    "# Retrieval Context (optional): LLM's actual retrieval results from the RAG system\n",
    "# Context (optional): Additional ground truth context besides RAG\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric(model=custom_geminiflash)\n",
    "contextual_recall = ContextualRecallMetric(model=custom_geminiflash)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=custom_geminiflash)\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What measures does the speaker propose to lower prescription drug costs in America?\",\n",
    "    actual_output=\"The speaker proposes giving Medicare the power to negotiate lower prescription drug prices, capping prescription drug costs at $2,000 a year for everyone, and allowing Medicare to negotiate lower prices for 500 drugs over the next decade.s\",\n",
    "    expected_output=\"The speaker proposes that Medicare should be given the power to negotiate lower drug prescription prices. They argue that this would save hundreds of billions of dollars and lower prescription drug costs for everyone. The speaker also states that the money saved could be used to strengthen the Affordable Care Act and expand Medicare coverage benefits without costing taxpayers an additional penny.\",\n",
    "    retrieval_context=['Letâ€™s do what weâ€™ve always talked about for all the years I was down here in this â€” in this body â€” in Congress.  Letâ€™s give Medicare the power to save hundreds of billions of dollars by negotiating lower drug prescription prices.  (Applause.)', 'In fact, we pay the highest prescription drug prices of anywhere in the world right here in America â€” nearly three times â€” for the same drug, nearly three times what other countries pay.  We have to change that, and we can.', 'And weâ€™re finally giving Medicare the power to negotiate drug prices. Bringing down prescription drug costs doesnâ€™t just save seniors money.', 'For years people have talked about it but I finally got it done and gave Medicare the power to negotiate lower prices for prescription drugs just like the VA does for our veterans.', 'And, by the way, that wonâ€™t just â€” that wonâ€™t just help people on Medicare; it will lower prescription drug costs for everyone.', 'Now I want to cap prescription drug costs at $2,000 a year for everyone!', 'We know how to do this.  The last President had that as an objective.  We all know how outrageously expensive drugs are in America.', 'Make no mistake, if you try to do anything to raise the cost of prescription drugs, I will veto it.', 'Now itâ€™s time to go further and give Medicare the power to negotiate lower prices for 500 drugs over the next decade.', 'It will cut the federal deficit, saving tax payers hundreds of billions of dollars on the prescription drugs the government buys for Medicare.']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ef1ea0f-8367-4747-84eb-f16f79895d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Precision Score:  0.9095238095238096\n",
      "Contextual Precision Reason:  The score is 0.91 because the first five nodes are relevant and directly address the speaker's proposal to lower drug prices through Medicare negotiation.  However, the sixth node, focusing on capping prescription drug costs,  is a separate proposal and doesn't directly align with the initial proposal, making it a relevant node ranked lower. The seventh and eighth nodes don't mention specific measures, making them less relevant compared to the first five nodes that directly discuss the speaker's proposed measure.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Recall Score:  1.0\n",
      "Contextual Recall Reason:  The score is 1.00 because the speaker proposes that Medicare should be given the power to negotiate lower drug prescription prices, and the node(s) in retrieval context) support this with phrases like 'Letâ€™s give Medicare the power to save hundreds of billions of dollars by negotiating lower drug prescription prices.'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Relevancy Score:  0.5\n",
      "Contextual Relevancy Reason:  The score is 0.50 because the reasons for irrelevancy indicate that the context doesn't specifically outline measures to lower prescription drug costs, but it does discuss the speaker's position and impact on costs. \"The context discusses the high prescription drug prices in America and suggests changing them, but it doesn't mention any specific measures the speaker proposes to lower costs.\" and \"The context only states the speaker's position on raising the cost of prescription drugs, not any measures to lower it.\" highlight this, suggesting a partial relevance as the context doesn't ignore the topic entirely.\n"
     ]
    }
   ],
   "source": [
    "# For RAG systems, DeepEval recommends the following Retrieval and Generation metrics:\n",
    "# Retrieval metrics:\n",
    "contextual_precision.measure(test_case)\n",
    "print(\"Contextual Precision Score: \", contextual_precision.score)\n",
    "print(\"Contextual Precision Reason: \", contextual_precision.reason)\n",
    "\n",
    "contextual_recall.measure(test_case)\n",
    "print(\"Contextual Recall Score: \", contextual_recall.score)\n",
    "print(\"Contextual Recall Reason: \", contextual_recall.reason)\n",
    "\n",
    "contextual_relevancy.measure(test_case)\n",
    "print(\"Contextual Relevancy Score: \", contextual_relevancy.score)\n",
    "print(\"Contextual Relevancy Reason: \", contextual_relevancy.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cd1ac8d-1f6f-47bb-87df-87528474486a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Relevancy Score:  1.0\n",
      "Answer Relevancy Reason:  The score is 1.00 because the input asks for measures to lower prescription drug costs and the provided JSON correctly represents that information. Great job!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score:  1.0\n",
      "Faithfulness Reason:  The score is 1.00 because there are no contradictions, this is great!\n"
     ]
    }
   ],
   "source": [
    "# Generation metrics:\n",
    "answer_relevancy = AnswerRelevancyMetric(model=custom_geminiflash)\n",
    "faithfulness = FaithfulnessMetric(model=custom_geminiflash)\n",
    "                                 \n",
    "answer_relevancy.measure(test_case)\n",
    "print(\"Answer Relevancy Score: \", answer_relevancy.score)\n",
    "print(\"Answer Relevancy Reason: \", answer_relevancy.reason)\n",
    "\n",
    "faithfulness.measure(test_case)\n",
    "print(\"Faithfulness Score: \", faithfulness.score)\n",
    "print(\"Faithfulness Reason: \", faithfulness.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c32b6c3f-c08a-439f-9996-d33d20ca6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of measuring metrics for multiple test cases / a full dataset\n",
    "\n",
    "# Load manually curated dataset\n",
    "evaldataset = EvaluationDataset()\n",
    "evaldataset.add_test_cases_from_csv_file(\n",
    "    file_path=\"datasets/unlabeled_dataset/unlabeled_dataset_deepeval.csv\",\n",
    "    input_col_name=\"Input\",\n",
    "    actual_output_col_name=\"Actual_Output\",\n",
    "    expected_output_col_name=\"Expected_Output\",\n",
    "#    context_col_name=\"context\",\n",
    "#    context_col_delimiter= \",\",\n",
    "    retrieval_context_col_name=\"Retrieval_Context\", # Context that the LLM produced when it answered the Input\n",
    "    retrieval_context_col_delimiter= \",\"\n",
    "#    additional_metadata_col_name=\"source_file\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7d54857-001b-403c-9838-01082c365a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever metrics:\n",
    "contextual_precision = ContextualPrecisionMetric(model=custom_geminiflash)\n",
    "contextual_recall = ContextualRecallMetric(model=custom_geminiflash)\n",
    "contextual_relevancy = ContextualRelevancyMetric(model=custom_geminiflash) # Note: this was the only metric that would not finish execution for the manually curated dataset (too many 429 errors)\n",
    "\n",
    "# Generation metrics:\n",
    "answer_relevancy = AnswerRelevancyMetric(model=custom_geminiflash)\n",
    "faithfulness = FaithfulnessMetric(model=custom_geminiflash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84960d-d52d-4178-8ef0-088e818b71db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 Options for Metrics Evaluation: \n",
    "\n",
    "# 1) Iterating through test cases seems to work better than bulk evaluation with evaluate,\n",
    "# as errors encountered with evaluate(...) cause no results to be returned\n",
    "# Looping at least saves partial results until an error occurs\n",
    "# I encountered this sometimes with contextual_relevancy and contextual_precision on the test dataset (429 errors or Invalid JSON errors),\n",
    "# yet typically I got results if I iterated through individual test_cases\n",
    "# For future: https://github.com/confident-ai/deepeval/issues/964 may assist with incorrect json errors like what was being returned\n",
    "\n",
    "# Example for evaluating one test case at a time\n",
    "contextprecision_results = []\n",
    "for i in range(0, len(evaldataset.test_cases)):\n",
    "    eval_contextprecision = evaluate(test_cases=[evaldataset.test_cases[i]], metrics=[contextual_precision], throttle_value=10) #throttle_value is for rate limiting, in seconds between queries\n",
    "    contextprecision_results.append(eval_contextprecision[0])\n",
    "\n",
    "# 2) Evaluate through test_cases in bulk\n",
    "# In testing, at least faithfulness, contextual_precision metrics worked this way with a small, manually curated dataset\n",
    "\n",
    "# Throttle_value is for rate limiting- in seconds between queries\n",
    "# test_precision = evaluate(test_cases=evaldataset.test_cases, metrics=[contextual_precision], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd280f4f-5ec1-405b-9e3f-062f1908a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick print for results after evaluation of single test cases, as needed\n",
    "# Get scores to calculate average score\n",
    "scores = []\n",
    "for i in [contextprecision_results]:\n",
    "    for j in i:\n",
    "        if type(j) == TestResult:\n",
    "            scores.append(j.metrics_data[0].score)\n",
    "            print_test_result(j)\n",
    "        else:\n",
    "            print_test_result(j[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b35bb-0064-4fac-acac-eb8d7c5134e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average for the metric\n",
    "scoredata = pd.DataFrame(scores, index=None)\n",
    "scoredata.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8a6e436d-fb76-4609-b4fc-ed7b6c992b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scores\n",
    "scoredata.to_csv(\"results/deepeval_contextprecision_unlabeled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0128ad6f-1eac-4d26-80b2-c3df8ace688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepEval also has the RAGAS metrics available for evaluation\n",
    "\n",
    "# Unforuntately the RAGAS metrics in DeepEval only accept langChain chat models, so the Gemini DeepEvalBaseLLM class will not work with these metrics\n",
    "# Need to use our LangChain LLM created earlier:  \n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "# doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") \n",
    "\n",
    "ragasmetric = RagasMetric(model=llm, embeddings=doc_embeddings)\n",
    "ragas_ar = RAGASAnswerRelevancyMetric(model=llm, embeddings=doc_embeddings)\n",
    "ragas_f = RAGASFaithfulnessMetric(model=llm)\n",
    "ragas_crecall = RAGASContextualRecallMetric(model=llm)\n",
    "ragas_cp = RAGASContextualPrecisionMetric(model=llm)\n",
    "ragas_crel = RAGASContextualRelevancyMetric(model=llm) # Note: This metric did not work in testing; returned errors related to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f9317-d292-48a2-9e77-72ff0671c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of evaluation for RagasMetric, an average of RAGAS's Answer Relevancy, Faithfulness, Contextual Recall, and Contextual Precision metrics\n",
    "eval_ragas = evaluate(test_cases=[evaldataset.test_cases[0]], metrics=[ragasmetric], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3feaa-39c5-4a55-bf28-54ca1a5a5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of evaluation for each ragas metric individually\n",
    "eval_ragas_f = evaluate(test_cases=evaldataset.test_cases, metrics=[ragas_f], throttle_value=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ccaeb-6131-49a3-bc48-4167d5546ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "63c2fc3a-e543-4abe-9a17-5bf68f2933c0",
   "metadata": {},
   "source": [
    "Results/Notes on DeepEval metrics on a small test dataset (also saved in results_deepeval_manual_dataset.txt):\n",
    "\n",
    "Dataset: manual_dataset_complete.csv, n=11, I created the questions, data used all 4 speeches\n",
    "\n",
    "- Contextual Precision: Average: 0.776\n",
    "  - Calculated fairly similarly (if not the same) as RAGAS's contextual precision; would need to look into code further\n",
    "  - Manual review: Decent overall, but some definitely had the single answer needed higher ranked in the nodes list, yet received a low score (0.25), likely for giving extra context that was not as relevant.\n",
    "  - Compared with RAGAS in DeepEval Contextual Precision: 0.493 (not close at all)\n",
    "  - Compared with RAGAS package Contextual Precision: 0.6944 (closer to result here, but not to the above, which should be the same)\n",
    "\n",
    "- Contextual Recall: Average: 0.723\n",
    "  - Calculated approximately the same way as RAGAS, according to their brief algorithm description\n",
    "  - Manual review: Good overall, not great. The scores and reasonings are definitely dependent on the LLM.. For example, for the first test case, it said that a specific fact was not mentioned in the context when it actually was in the context.\n",
    "    - Another speaks for itself: \"The score is 1.00 because you provided an input, and asked for a JSON output. That's perfect! Let's keep going.\"\n",
    "    - In another reason given for a perfect score, it gave additional context that is not found in the retrieval context; it seemed to more try to answer the prompt vs doing the evaluation of the metric.\n",
    "  - Compared with RAGAS in DeepEval Contextual Recall: 0.627\n",
    "  - Compared with RAGAS package Context Recall: 0.6455 (pretty close)\n",
    "\n",
    "- Answer Relevancy: Average: 0.989\n",
    "  - Calculated differently than RAGAS's answer relevancy\n",
    "  - Manual review: Not great overall; some responses are reasonable, but some are not relevant, giving scores based off of whether or not a JSON output was returned. Additionally, in other examples, DeepEval provided the same score for both receiving and not receiving a response:\n",
    "    - Example 1: \"Reason: The score is 1.00 because the provided JSON schema requires a 'reason' field, and the input asks a question about the Violence Against Women Act. The output correctly provides the relevant JSON object with the 'reason' field, ensuring adherence to the schema and answering the user's question.\"\n",
    "    - Example 2: \"Reason: The score is 1.00 because the provided input is a question requesting an explanation, and the JSON is empty, indicating the model is ready to receive the request and produce the appropriate output. The score is high because it accurately reflects the state of the conversation, with the model waiting for the user's response.\"\n",
    "    - Example 3: \"Reason: The score is 1.00 because the input asks for a specific reason and expects a JSON object with that reason. The provided JSON does not contain any object to address the request, therefore the score is 1.00, implying that the output does not address the input.\"\n",
    "  - Compared with RAGAS in DeepEval Answer Relevancy: 0.578 (not close at all to DeepEval, but makes sense with different algorithmic calculation)\n",
    "  - Compared with RAGAS package Answer Relevancy: 0.6770 (pretty close to above, not to DeepEval)\n",
    "\n",
    "- Faithfulness: Average: 1.0\n",
    "  - Calculated approximately the same way as RAGAS, according to their brief algorithm description\n",
    "  - Manual review: This metric only returned perfect 1.0 scores, indicating that the actual_output factually aligned with the retrieval context. This seems mostly accurate; note that in examples where I observed the actual_output having outside knowledge, I believe this technically doesn't factually contradict the retrieval_context, and this falls under a \"hallucination\" instead. \n",
    "  - Compared with RAGAS in DeepEval Faithfulness: 0.984\n",
    "  - Compared with RAGAS package Faithfulness': 1.0000 (close)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42b0e5fc-c77a-4b5f-9ea5-7aa44ac60e28",
   "metadata": {},
   "source": [
    "Example output from DeepEval\n",
    "\n",
    "======================================================================\n",
    "\n",
    "Metrics Summary\n",
    "\n",
    "  - âœ… Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: Gemini 1.5 Flash, reason: The score is 1.00 because all relevant nodes are ranked higher than the irrelevant node, which is the last node. The last node is irrelevant because it's a general statement about presidential speeches, not about economic growth. All other nodes are relevant because they either explicitly state or imply the President's plans to grow the economy, and provide examples of economic successes. For instance, the first node states that the economy created more than 1,300,000 new jobs in 100 days, which indicates growth, and the second node quotes the President saying 'trickle-down economics has never worked and itâ€™s time to grow the economy from the bottom and the middle out,' which directly supports the input., error: None)\n",
    "\n",
    "For test case:\n",
    "\n",
    "  - input: How does the President plan to grow the economy in his first speech, and what are examples of his successes in this area as shared in his next three speeches?\n",
    "  - actual output: The President plans to grow the economy by focusing on the bottom and middle class, rather than trickle-down economics.  His success in this area includes creating more than 1,300,000 new jobs in his first 100 days, a record 12 million new jobs in his first two years, and a 5.7% growth in the economy last year, which is the strongest growth in nearly 40 years. \n",
    "\n",
    "  - expected output: In his first speech, the President proposes that his American Jobs Plan will create millions of new jobs and trillions of dollars of economic growth to the economy by bringing Americans back to the workforce. He also says America should grow its economy from the bottom and middle out but does not say how to grow the economy that way. In his following speeches, he says that the American Rescue Plan has created over 6.5 million jobs in one year and 12 million jobs in two years and 15 million jobs in three years, and America s economy increased at a rate of 5.7% the first year. American companies are also investing in increasing manufacturing capabilities in America that would help reduce costs to Americans and created 800,000 manufacturing jobs in 2 years. America s GDP also increased over the three years covered in the speeches.\n",
    "  - context: []\n",
    "  - retrieval context: [\"['And in the process\", ' while this was all going on', ' the economy created more than 1', '300', \"000 new jobs in 100 days â€” more jobs in the first â€” (applause) â€” more jobs in the first 100 days than any President on record.'\", \" 'My fellow Americans\", \" trickle-down â€” trickle-down economics has never worked and itâ€™s time to grow the economy from the bottom and the middle out. (Applause.)'\", \" 'Our economy grew at a rate of 5.7% last year\", ' the strongest growth in nearly 40 years', \" the first step in bringing fundamental change to an economy that hasnâ€™t worked for the working people of this nation for too long.'\", \" 'The International Monetary Fund â€” (applause) â€” the International Monetary Fund is now estimating our economy will grow at a rate of more than 6 percent this year.  That will be the fastest pace of economic growth in this country in nearly four decades.'\", \" 'I inherited an economy that was on the brink. Now our economy is the envy of the world!'\", \" 'As I stand here tonight\", ' we have created a record 12 million new jobs', \" more jobs created in two years than any president has ever created in four years.'\", \" 'And by the way\", ' when we do all of these things', \" we increase productivity. We increase economic growth.'\", \" 'Since Iâ€™ve come to office\", \" our GDP is up.'\", \" 'You know\", ' thereâ€™s a broad consensus of economists â€” left', ' right', \" center â€” and they agree what Iâ€™m proposing will help create millions of jobs and generate historic economic growth.  These are among the highest-value investments we can make as a nation.'\", \" 'Throughout our history\", ' Presidents have come to this chamber to speak to Congress', ' to the nation', ' and to the world to declare war', ' to celebrate peace', \" to announce new plans and possibilities.']\"]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b6f39ca-8d15-4f9f-9d36-d2378b9b0655",
   "metadata": {},
   "source": [
    "Results/Notes on DeepEval's RAGAS metrics on a small test dataset:\n",
    "\n",
    "Dataset: manual_dataset_complete.csv, 11 examples (Input/Actual Output/Expected Output/Retrieval Context/Source File/Context). I created the questions and used all 4 of the SOTU speeches. Retrieval context is what was returned from the LLM when it gave the Actual Output.\n",
    "\n",
    "Individual metric results from RAGAS package:\n",
    "{'context_precision': 0.6944, 'faithfulness': 1.0000, 'answer_relevancy': 0.6770, 'context_recall': 0.6455}\n",
    "\n",
    "Overall summary results from DeepEval's RAGAS metrics:\n",
    "context_precision: 90.91% pass rate, faithfulness: 81.82% pass rate, answer_relevancy: 81.82% pass rate, context_recall: 81.82% pass rate, RAGAS: 54.55% pass rate\n",
    "\n",
    "Overall: Similar but not the same scores between DeepEval's RAGAS metrics (which call RAGAS) and using RAGAS directly. \n",
    "I think the variance is probably fairly normal, given that I got different results each time I ran RAGAS as well.\n",
    "\n",
    "Average scores from DeepEval's RAGAS individual metrics:\n",
    "context precision: 0.493268741183316\n",
    "faithfulness: 0.984126984126984\n",
    "answer_relevancy: 0.577747493385347\n",
    "context_recall: 0.627272727272727\n",
    "\n",
    "Notes from manual review of results for individual RAGAS metrics in DeepEval: \n",
    "- Faithfulness scoring seemed fairly reliable. \n",
    "- Answer relevancy also seemed fairly accurate. Some of the scoring for answer relevancy was difficult to determine why one received a better score than another, but for those in question, the score was only off by 0.1 from what I expected based off of the other scoring.\n",
    "- Some of the context_recall results seemed inaccurate. For example, one returned a score of 0 but in close examination, the context did contain parts of the ground truth result. For two very similar examples, scores varied from 0 to 0.5, which seemed like a dramatic variance.\n",
    "- Contextual precision seemed a bit off, as some of the 1st and 2nd context nodes contained the exact answer, yet the score was 0.6 or 0.5.\n",
    "\n",
    "Notes about output from RAGAS metric (average of all 4 RAGAS individual metrics above):\n",
    "- Output seemed inconsistent, as RAGAS metric could not be generated for 3 examples that we had results from the other 4 metrics on. Also unexpectedly, it was generated for one example that we didn't get all 4 individual results for.\n",
    "- For those examples that the metric was generated on, the RAGAS metric was within .12 of the manually calculated averages. \n",
    "- This metric (when it gives output) seems like a general approximation for a given test case of how well the RAG pipeline answered it.\n",
    "- The overall pass rate seems useful, but given that we got 7/11 results (including 1 questionable one) on such a small test set, I don't think I would use this metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8049b6-0c32-45b9-abf2-597e509bab66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3_gemini_deepeval",
   "language": "python",
   "name": "python3_gemini_deepeval"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
