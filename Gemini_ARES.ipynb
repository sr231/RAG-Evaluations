{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf756bfd-87bf-4b11-a819-e205176d9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial for setting up a small RAG system using Faiss \n",
    "# and evaluating it using the Gemini Flash 1.5 LLM and the ARES library\n",
    "# Google Gemini: https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "# ARES: https://github.com/stanford-futuredata/ARES\n",
    "\n",
    "# ARES should work with with several common LLMs (current support for API keys from OpenAI, Anthropic, and Together) as well as vllm\n",
    "# I was able to get it working with Google Gemini (not currently supported by the library) with moderate effort \n",
    "# in adapting the source code for Gemini and installing locally \n",
    "# See ARES_files folder for full files changed; main one was RAG_Automatic_Evaluation/Evaluation_Functions.py with adapted API specific functions\n",
    "\n",
    "# Metrics available in ARES:   \n",
    "# Context Relevance: Determines if the retrieved information is pertinent to the query.\n",
    "# Answer Faithfulness: Checks if the response generated by the language model is properly grounded in the retrieved context and does not include hallucinated or extraneous information.\n",
    "# Answer Relevance: Evaluates whether the generated response is relevant to the query, addressing all aspects of the question appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7693b8-cf90-4d82-b1ba-89e6d55abcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65781f17-9358-4d53-9305-939013e2a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from llama_index.core import Document, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeabfa92-2145-4086-9ba0-d1327a8e2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('â€¢', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66957101-56ea-4dbb-86f9-347a3380ccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up local API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ea96d-6d83-4354-86e3-4e38f25b2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish RAG pipeline with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dcd6cf4-f33c-47a0-aa5b-9c236b84822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a Faiss vector store for RAG\n",
    "# # If you already have an index created, skip a few coding cells to the LLM / embeddings setup\n",
    "\n",
    "# # Example of creating a small vector store\n",
    "# # Using 4 State of the Union speeches, all text from whitehouse.gov briefing room speeches posted online, edited to include a title with the date of the speech\n",
    "# # Example from 2024:\n",
    "# # https://www.whitehouse.gov/briefing-room/speeches-remarks/2024/03/07/remarks-of-president-joe-biden-state-of-the-union-address-as-prepared-for-delivery-2/\n",
    "\n",
    "# # load and parse files\n",
    "# sotu = []\n",
    "# newfiles = [\"./Speeches/titleedits/state_of_the_union_042921.txt\", \"./Speeches/titleedits/state_of_the_union_030122.txt\", \"./Speeches/titleedits/state_of_the_union_020723.txt\", \"./Speeches/titleedits/state_of_the_union_030724.txt\"]\n",
    "# for i in newfiles:\n",
    "#     with open(i) as file:\n",
    "#         for line in file:\n",
    "#             nl = line.rstrip()\n",
    "#             if nl != '':\n",
    "#                 sotu.append(nl)\n",
    "\n",
    "# # convert into Document format\n",
    "# documents = [Document(text=line) for line in sotu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4cadba0-1efd-47a7-849c-586713f3fc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='4a26868c-5225-4612-9338-005a366b96e5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='May God protect our troops.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Example of a loaded Document line\n",
    "# documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8534e02-a129-42c1-871f-31ab6919ddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# # Set up the faiss index\n",
    "# d = 768 # dimensions of the input vector of the embedding model that we're going to use; in this case, the google embedding model\n",
    "# faiss_index = faiss.IndexFlatL2(d)\n",
    "# print(faiss_index.is_trained) # double check that the training worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6ce235-d506-4151-9970-d4a34b52a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the llm, embeddings, and Settings for Faiss \n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\") # Can substitute any LangChain Chat Model\n",
    "doc_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\") # Can substitute any LangChain embedding model \n",
    "Settings.embed_model = doc_embeddings # used for LlamaIndex FaissVectorStore\n",
    "Settings.llm = llm # used for LlamaIndex FaissVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe61b42-b268-4f15-a8cc-6ee6130aff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment for when you need to re-embed and vectorize documents\n",
    "\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, show_progress=True\n",
    "# )\n",
    "\n",
    "# # Save index to disk\n",
    "# index.storage_context.persist()\n",
    "\n",
    "# # Save/remember index id for loading next time\n",
    "# index.index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c032469-b39e-47bf-a328-d884a9e717f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have a saved index, load that index for RAG answer generation:\n",
    "\n",
    "# load index from disk\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "# My local index id '3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5' uses the 4 speeches including a title that includes the date it was given\n",
    "index = load_index_from_storage(storage_context=storage_context, index_id='3d3c99c5-aa1c-42d7-a9ce-c4bb12fbc6d5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833ae491-6e3e-497e-af35-f2823c7da9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional- if you'd like to query your index\n",
    "# # Set up query and chat engines with the index\n",
    "# query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "# chat_engine = index.as_chat_engine(similarity_top_k=10, chat_mode='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3b0844-f59f-4d12-856e-d2f28fa05857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query and response with Gemini and query_engine\n",
    "# query = \"What does the President say about his administration's first 100 days and covid-19?\"\n",
    "# response = query_engine.query(query) \n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24abe170-6959-4895-a536-33bdc4ca5577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34897696-e3d2-4a40-b741-d839cc308931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for ARES RAG evaluation library to work with Gemini and our local RAG setup\n",
    "# ARES has options to run a traditional metrics evaluation (their UES/IDP function for context relevance, answer relevance, and answer faithfulness)\n",
    "# or a Prediction Powered Inference evaluation to generate a confidence interval for a given metric\n",
    "# ARES can also synthetically generate data (Queries/Answers/Context) from specified documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2984ff52-f184-49ec-a3d8-8124c8667a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM not imported.\n",
      "vLLM not imported.\n",
      "vLLM not imported.\n",
      "vLLM not imported.\n"
     ]
    }
   ],
   "source": [
    "from ares import ARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4ce3cc2-a413-4ce2-8891-593347deddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This configuration runs an evaluation on an unlabeled evaluation set (UES) in conjunction with in-domain prompts (IDP) \n",
    "ues_idp_config = {\n",
    "    \"in_domain_prompts_dataset\": \"ARES_files/nq_few_shot_prompt_for_judge_scoring.tsv\", # Small set of labeled few-shot examples for scoring context relevance, answer faithfulness, and/or answer relevance in your RAG system\n",
    "    \"unlabeled_evaluation_set\": \"ARES_files/nq_unlabeled_output.tsv\", # Larger set of unlabeled query-document-answer triples output by your RAG system for scoring\n",
    "    \"model_choice\" : \"models/gemini-1.5-flash\",\n",
    "    \"request_delay\" : 60, # Delay (in seconds) between requests to the API\n",
    "    \"documents\" : 0 # Number of documents to be evaluated. Default is 0, which means all documents in the evaluation set will be evaluated\n",
    "} \n",
    "\n",
    "ares = ARES(ues_idp=ues_idp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0957b957-09c2-4b0d-bda1-a87945225f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acaa8039dc24c0084baa3868849af19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating large subset with models/gemini-1.5-flash:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configured gemini for context relevance\n",
      "Testing candidates\n",
      "test_response\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for answer relevance\n",
      "Testing response.text\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for answer faithfulness\n",
      "Testing response.text\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for context relevance\n",
      "Testing candidates\n",
      "test_response\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for answer relevance\n",
      "Testing response.text\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for answer faithfulness\n",
      "Testing response.text\n",
      "[[Yes]]\n",
      "\n",
      "configured gemini for context relevance\n",
      "Testing candidates\n",
      "Attempt 1 failed with error: list index out of range\n",
      "Testing candidates\n",
      "Attempt 2 failed with error: list index out of range\n",
      "Testing candidates\n",
      "Attempt 3 failed with error: list index out of range\n",
      "Testing candidates\n",
      "Attempt 4 failed with error: list index out of range\n",
      "Testing candidates\n",
      "All attempts failed. Last error was: list index out of range\n",
      "Number of times did not extract Yes or No: 0\n",
      "{'Context Relevance Scores': 0.667, 'Answer Faithfulness Scores': 0.667, 'Answer Relevance Scores': 0.667}\n"
     ]
    }
   ],
   "source": [
    "results = ares.ues_idp()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c815504d-3470-4047-9e4f-35332844fa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Context Relevance Scores': 0.667,\n",
       " 'Answer Faithfulness Scores': 0.667,\n",
       " 'Answer Relevance Scores': 0.667}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca8ee8b-e105-436f-8f52-692c736bbeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to ARES, 'document' refers to a line in a document_filepath_file\n",
    "# I never tested it with the speeches broken up line by line/by paragraph/by sentence\n",
    "# This setup of 1 large document per line in the document file did not work well; recommend trying documents being only a few sentences (or so) each\n",
    "synth_config = { \n",
    "    \"document_filepaths\": [\"Speeches/titleedits/Speeches_Docs_ARES.tsv\"], # Source documents for generating synthetic queries; requires tsv file\n",
    "    \"few_shot_prompt_filename\": \"datasets/manual_dataset_complete_ares_synthetic.tsv\", # Few shot labeled training data with queries and answers\n",
    "    \"synthetic_queries_filenames\": [\"results/synthetic_dataset_output_ARES.tsv\"], # Where to save resulting synthetic data file\n",
    "    \"model_choice\": \"models/gemini-1.5-flash\", \n",
    "    \"documents_sampled\": 1000, # How many documents to sample when generating synthetic queries; ARES filters documents < 50 words\n",
    "    \"api_model\": True\n",
    "}\n",
    "\n",
    "ares_synth = ARES(synthetic_query_generator=synth_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f5c628-84b7-4222-bd8d-5499dd1b585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ares_synth.generate_synthetic_data()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027370c9-e2af-48a3-96e2-edc9755cda26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e24faf0-1298-426e-93cd-a235144fdec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Powered Inference - enhances the evaluation accuracy and estimates a confidence interval for the quality of each RAG system\n",
    "ppi_config = { \n",
    "    \"evaluation_datasets\": ['datasets/unlabeled_dataset/unlabeled_dataset.tsv'], # Unlabeled data for evaluation \n",
    "    \"few_shot_examples_filepath\": \"datasets/manual_dataset_complete_ares.tsv\", # A few full labeled examples to show the labeling schema and guide the evaluation \n",
    "    #\"num_trials\" : 10, # Number of iterations used to estimate confidence intervals and other statistics utilized in PPI; Example given was 1000\n",
    "    \"llm_judge\": \"models/gemini-1.5-flash\", # LLM to use for evaluation; specify only if there is no checkpoint\n",
    "    \"labels\": [\"Context_Relevance_Label\"], # Which metric(s) in labeled dataset to evaluate\n",
    "    \"gold_label_paths\": [\"datasets/labeled_dataset/labeled_dataset_sotu.tsv\"] # A fully labeled validation dataset with 50+ examples different than few shot dataset; used to measure performance of the classifier\n",
    "    #\"checkpoints\": [\"None\"], # If created, file is a saved state of the trained classifier used for evalution; Use either checkpoint or llm_judge\n",
    "    #\"alpha\": 0.05, # Significance level for hypothesis testing and confidence interval; Default is 0.05\n",
    "    #\"request_delay\": 0 # Optional, not tested; Specifies the delay in seconds between each request to the LLM API\n",
    "}\n",
    "\n",
    "ares_ppi = ARES(ppi=ppi_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c167015-d9fe-4437-a41b-1d5f6901a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ares_ppi.evaluate_RAG()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c24b62d-c33c-4cf6-b86f-ee8b46a8a014",
   "metadata": {},
   "source": [
    "Example output for my unlabeled dataset: \n",
    "---------------------------------------\n",
    "Calculated Accuracy: 0.6949152542372882\n",
    "---------------------------\n",
    "Context_Relevance_Label Scoring\n",
    "ARES Ranking\n",
    "Evaluation_Set: datasets/unlabeled_dataset/unlabeled_dataset.tsv\n",
    "Checkpoint: None\n",
    "ARES Prediction: 0.7324999999999967\n",
    "ARES Confidence Interval: [0.588, 0.877]\n",
    "Number of Examples in Evaluation Set: 800\n",
    "ARES LLM Judge Accuracy on Ground Truth Labels: 0.695\n",
    "Annotated Examples used for PPI: 59\n",
    "\n",
    "[{'Label_Column': 'Context_Relevance_Label', 'Evaluation_Set': 'datasets/unlabeled_dataset/unlabeled_dataset.tsv', 'ARES_Prediction': 0.7324999999999967, 'ARES_Confidence_Interval': [0.588, 0.877], 'Number_of_Examples_in_Evaluation_Set': 800, 'Ground_Truth_Performance': None, 'ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels': 0.695, 'Annotated_Examples_used_for_PPI': 59}]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14fc27f1-6258-446d-a9b6-4929eab83e00",
   "metadata": {},
   "source": [
    "Example output for test run with 100 examples from nq dataset\n",
    "--------------------------------------------------\n",
    "Context_Relevance_Label Scoring\n",
    "ARES Ranking\n",
    "Evaluation_Set:ARES_files/nq_unlabeled_output_100examples.tsv\n",
    "Checkpoint:None\n",
    "ARES Prediction: [0.6395238095238103]\n",
    "ARES Confidence Interval: [[0.513, 0.766]]\n",
    "Number of Examples in Evaluation Set: [70]\n",
    "Ground Truth Performance: [0.686]\n",
    "ARES LLM Judge Accuracy on Ground Truth Labels: [0.857]\n",
    "Annotated Examples used for PPI: 300\n",
    "--------------------------------------------------\n",
    "\n",
    "[[{'Label_Column': 'Context_Relevance_Label', 'Evaluation_Set': 'ARES_files/nq_unlabeled_output_100examples.tsv', 'ARES_Prediction': 0.6395238095238103, 'ARES_Confidence_Interval': [0.513, 0.766], 'Number_of_Examples_in_Evaluation_Set': 70, 'Ground_Truth_Performance': 0.686, 'ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels': 0.857, 'Annotated_Examples_used_for_PPI': 300}]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03de8cf5-4b0e-4d4e-837a-6975f02b0216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "researchcopy",
   "language": "python",
   "name": "researchcopy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
