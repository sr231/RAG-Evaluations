RAGAS:

- Version 0.1.15 - edited locally to resolve a compatibility issue with Gemini

- Overview (as of August 2024):
  - Metrics seem well designed and are the most common metrics across RAG evaluation libraries. No explanations are given for the scores.
  - Synthetic testset generation (question/context/ground truth answer) function worked pretty well, assuming you could get it to work without being rate-limited (didn't seem to work well with traditional rate limiting libraries). Questions generated for the documents were the most "human-like" of all libraries tested.  
  - Documentation was significantly lacking when tested but has since been updated and may have been improved.
  - Internal rate limiting functions didn't work well, and RAGAS didn't work well (with limited testing) with external rate limiting libraries.

- Metrics:

  - Note: RAGAS documentation/functions were updated 10/10/24, so there are a few new variations of current metrics I haven't tried.
RAGAS metrics guide: https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics

  - Faithfulness - Measures the factual consistency of the answer to the context based on the question.
  - Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.
  - Answer_relevancy - Measures how relevant the answer is to the question.
  - Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.
  - Faithfulness with HHEM - Similar to Faithfulness but uses a HuggingFace model (Vectara's HHEM 2.1 classifier) to detect hallucinations
    - https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html#faithfullness-with-hhem-2-1-model
    - https://huggingface.co/vectara/hallucination_evaluation_model

- Data generation: RAGAS allows you to generate a synthetic testset of any size with Question/Context/Ground Truth columns, using the docs/model in your RAG pipeline.
  - On a very small generation (n=10), results seemed pretty good; saves a lot of time vs creating this manually.
  - Generate synthetic test data : When generating a synthetic test dataset, the columns generated are 'question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', 'episode_done'.
  - You specify the distribution of the kinds of questions generated: 'simple', 'reasoning', or 'multi_context' questions, as well as the percentage of multi-context questions
  - Ground truth is supposed to be the 'human' level answer vs the RAG answer. 
  - We then have to generate the answer separately with our RAG, which then (obviously) generates new context used. My best guess is to use the context that was used to generate the answer for the metrics calculation. The original 'contexts' column was pretty good overall and typically could answer the question. Occasionally the generated context was not relevant/specific enough to answer the question.
  - Example: testset_answer_newcontext_flash_pro15.csv : Synthetically generated testset with RAGAS, with the addition of answers generated by Gemini 1.5 Flash along with the context used to create those answers. The 'context_gt' column contains the original contexts used to generate the 'ground_truth' column, all of which was created with the RAGAS TestsetGenerator.
    - Generator LLM: Gemini 1.5 Flash
    - Critic LLM: Gemini 1.5 Pro
    - Embeddings: models/text-embedding-004

- Required datasets:
  - Dataset for evaluation needs to be in Dataset format, not pandas dataframe (per issues raised on GitHub repo)
  - Dataset columns: ['question', 'answer', 'ground_truth', 'contexts', 'source_file']

- Custom/local model integration:
  - Works with LangchainLLM and LangchainEmbeddings : I used ChatGoogleGenerativeAI and GoogleGenerativeAIEmbeddings. Since Langchain integrates with VLLM, in theory RAGAS should work with VLLM or other local models with a Langchain wrapper.
  - There is also a BaseRagasLLM and BaseRagasEmbeddings class you can inherit from, but I did not test this.

- Performance results:
  - Below results were on my manually created dataset: manual_dataset_complete.csv, n=11, using all 4 speeches
  - RAGAS metrics simply output a score for the chosen metric without an additional explanation.
  - RAGAS metrics guide does not have example ranges to compare anything to, so below is my best guess.

  - Faithfulness - Measures the factual consistency of the answer to the context based on the question.
  - 0.9167 - 1.0000 indicates that the LLM is staying true to the facts provided in the context for answering the question.
  - Faithfulness with HHEM - This uses a huggingface model to help detect hallucination : https://huggingface.co/vectara/hallucination_evaluation_model
  - 0.6319: This doesn't really agree with the RAGAS faithfulness score... may need to dive in further another time.
  - Context_precision - Measures how relevant the retrieved context is to the question, conveying the quality of the retrieval pipeline.
  - At 0.4171 - 0.5979, suggests that the context isn't particularly relevant to the question. After reviewing the results, I think this should probably be  a little higher, but the score makes sense.
  - Answer_relevancy - Measures how relevant the answer is to the question.
  - 0.6509 - 0.6533 seems moderately low, just going off of a manual review of the answer. This may be due to questions asked that do not have an answer in the context.
  - Context_recall - Measures the retriever’s ability to retrieve all necessary information required to answer the question.
  - 0.8 indicates that the llm context is decently good and can typically answer the question or most of it. I concur with this after manual review of the dataset.

- Other notes:
  - The RAGAS internal RunConfig settings do a decent job at limiting the 429 resource exhausted warnings when max_workers=1. Ratelimit and backoff Python libraries didn't seem to affect performance, but I didn't spend a lot of time with that.
  - There's a flag you have to set initially for RAGAS not to track your usage.


DeepEval:

- Version 1.1.6

- Overview (as of September 2024):
  - Metrics are well designed and have commonalities across all 3 libraries evaluated. DeepEval offered 6 different metrics for RAG systems, the most of the three libraries.
  - Numerical results for the metrics were good overall/somewhat comparable to RAGAS, but the reasoning returned for why a particular score was given for a metric was not consistently reliable. It probably depends on the RAG system & LLM, but I wouldn't rely on the LLM's reason for a given score when using DeepEval.
  - Documentation was more prevalent than other libraries but did not always reflect the exact code.

- Metrics:  
  - DeepEval provides a score for each metric and a reason (output by the LLM) for each metric's score

  - For RAG systems, DeepEval recommends running the following metrics:

  - Retrieval metrics:
    - Contextual Precision: evaluates whether the reranker in your retriever ranks more relevant nodes in your retrieval context higher than irrelevant ones.
    - Contextual Recall: evaluates whether the embedding model in your retriever is able to accurately capture and retrieve relevant information based on the context of the input.
    - Contextual Relevance: evaluates whether the text chunk size and top-K of your retriever is able to retrieve information without much irrelevancies.

  - Generation metrics:
    - Answer Relevancy: evaluates whether the prompt template in your generator is able to instruct your LLM to output relevant and helpful outputs based on the retrieval_context.
    - Faithfulness: evaluates whether the LLM used in your generator can output information that does not hallucinate AND contradict any factual information presented in the retrieval_context.
  
- Data generation:
  - The synthetic data generation was the overall best working of the three libraries, as even though I hit rate-limiting errors, I was able to overcome them and generate large size (n=800) datasets by appending several small ones together. Questions and answers generated were the second most "human-like" of the libraries, and after manual review to remove a few non-relevant queries, they were good enough to stand in for a manually created dataset with some very minor edits (specifying years/dates of documents), especially when considering it took a lot less time to use the LLM to generate the dataset vs creating it manually.
  - Generate synthetic data : When generating a synthetic dataset, the columns generated are 'input', 'actual_output', 'expected_output', 'context', and 'source_file
    - The 'actual_output' column is blank; you need to re-run the input and context through your LLM to get the actual output. 'expected_output' acts as that answer otherwise.
  - Example: labeled_dataset_sotu.csv : Synthetically generated testset with DeepEval, with the addition of answers generated by Gemini 1.5 Flash, along with the new context used to create those answers, and manually labeled columns for the metrics (ex: context_relevance). The 'Contexts_QueryGen' column contains the original contexts, and the 'Contexts_1file' column contains the new context retrieved when I re-ran the queries through the RAG system (each time indexed with only the one relevant sotu speech). Likewise, 'Expected_Output' contains the originally projected output with Contexts_QueryGen, and 'Answer' contains the newly generated answer with 'Contexts_1file'.
    - Generator LLM: Gemini 1.5 Flash
    - Critic LLM: Gemini 1.5 Pro
    - Embeddings: models/text-embedding-004

- Required datasets:
  - To get results for metrics in DeepEval, you can either use a single "LLMTestCase"- one 'input', one 'actual_output', one 'expected_output', and one 'retrieval_context', or you can create an "EvaluationDataset" of multiple sets of input, actual_output, expected_output, and retrieval_context.
    - I was able to input a csv file and create an EvaluationDataset fairly easily.

- Custom/local model integration:
  - Non Open-AI models require a custom LLM class and a custom embeddings class for using DeepEval, inheriting from DeepEvalBaseLLM and DeepEvalBaseEmbeddingModel, respectively. This wasn't too difficult to set up.

- Performance results:
  - To get results, you can either:
    1) Iterate through LLMTestCases (evaldataset.test_cases) - Looping through test cases seems to work a bit better than option #2, as errors encountered with evaluate() cause no results to be returned, and you can save partial results with looping.
    2) Try to use the evaluate() function to iterate through test cases in bulk. Despite this not returning partial results if you hit an error, it did work for my testing of my small manually curated dataset and a high throttle_value for rate limiting.

  - DeepEval metrics do not have example ranges to compare anything to, so below is my best guess.

  - Retrieval metrics:
    - Contextual Precision: Average: 0.776
      - Calculated fairly similarly (if not the same) as RAGAS's contextual precision; would need to look into code further
      - Manual review: Decent overall, but some definitely had the single answer needed higher ranked in the nodes list, yet received a low score (0.25), likely for giving extra context that was not as relevant.
      - Compared with RAGAS in DeepEval Contextual Precision: 0.493 (not close at all)
      - Compared with RAGAS package Contextual Precision: 0.6944 (closer to result here, but not to the above, which should be the same)

    - Contextual Recall: Average: 0.723
      - Calculated approximately the same way as RAGAS, according to their brief algorithm description
      - Manual review: Good overall, not great. For example, for the first test case, it said that a specific fact was not mentioned in the context when it actually was in the context. The output reasoning left a lot to be desired.
        - Example of poor output reason: "and the JSON output is formatted correctly to match the given schema" has nothing to do with the metric scoring for this metric
        - In another reason given for a perfect score, it gave additional context that is not found in the retrieval context; it seemed to more try to answer the prompt vs doing the evaluation of the metric.
      - Compared with RAGAS in DeepEval Contextual Recall: 0.627
      - Compared with RAGAS package Context Recall: 0.6455 (pretty close)

    - Contextual Relevance: "Measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval_context for a given input."
      - Unfortunately, I could not test this metric due to 429 Resource Exhausted Errors, regardless of rate limiting

  - Generation metrics:
    - Answer Relevancy: Average: 0.989
      - Calculated differently than RAGAS's answer relevancy
      - Manual review: Not great. If it's working, it seems to work fine generally. However some scores/reasons were VERY off, as not-relevant reasons for the scores were given and were inconsistent with the scoring.
        - Example 1: "The score is 1.00 because you provided an input, and asked for a JSON output. That's perfect! Let's keep going." 
        - Example 2: "Reason: The score is 1.00 because the input asks for a specific reason and expects a JSON object with that reason. The provided JSON does not contain any object to address the request, therefore the score is 1.00, implying that the output does not address the input."
      - Compared with RAGAS in DeepEval Answer Relevancy: 0.578 (not close at all to DeepEval, but makes sense with different algorithmic calculation)
      - Compared with RAGAS package Answer Relevancy: 0.6770 (pretty close to above, not to DeepEval)

    - Faithfulness: Average: 1.0
      - Calculated approximately the same way as RAGAS, according to their brief algorithm description
      - Manual review: This metric only returned perfect 1.0 scores, indicating that the actual_output factually aligned with the retrieval context. This seems mostly accurate except for the example where the actual_output definitely brought in outside knowledge. I suppose that technically doesn't factually contradict the retrieval_context, and that would fall under a "hallucination" instead. 
      - Compared with RAGAS in DeepEval Faithfulness: 0.984
      - Compared with RAGAS package Faithfulness': 1.0000 (close)

- Other notes:
  - DeepEval requires a json response. In practice, this has led to malformed json returned from the llm for metric evaluation, even with as simple of a schema as possible (i.e., return a string).
  - There's a flag you have to set initially for DeepEval not to track your usage.

ARES:

- Overview:

- Metrics: 

  - Context Relevance: Determines if the retrieved information is pertinent to the query.
  - Answer Faithfulness: Checks if the response generated by the language model is properly grounded in the retrieved context and does not include hallucinated or extraneous information.
  - Answer Relevance: Evaluates whether the generated response is relevant to the query, addressing all aspects of the question appropriately.

  - Of note, ARES also offers Prediction Powered Inference (PPI) to significantly enhance evaluation accuracy when evaluating RAG systems. See the Performance results section for more details.

- Data generation:

  - Dataset generated included Query, Document, Answer, along with Context_Relevance, Answer_Faithfulness, and Answer_Relevance labels. Questions generated included positive, negative, and duplicate queries. 
  - Resulting questions were very, very basic (What was the date of the address? Who was mentioned at the beginning?) and only addressed the first part of the document, despite confirming that the entire document was provided.
  - After reviewing other sample datasets from ARES, ARES seems to perform better with shorter texts; would probably need to chuck these speeches to have more relevant/specific questions generated. ARES does not confirm how long a "document" should be, but in some of its example datasets, documents were only a couple sentences long.

- Required datasets:

  - A human preference validation set of annotated query, document, and answer triples for the evaluation criteria (e.g. context relevance, answer faithfulness, and/or answer relevance). ARES requires >50 examples but prefers several hundred. I used 58 examples for my labeled dataset.
  - A set of labeled few-shot examples for scoring context relevance, answer faithfulness, and/or answer relevance in your system. In my testing, I used 11 examples for this.
  - A much larger set of unlabeled query-document-answer triples outputted by your RAG system for scoring. I created this by generating a synthetic dataset of ~800 examples with my RAG system & LLM (using DeepEval, see above).

- Performance results:

  - ARES offers two different RAG evaluation functions:
    1) ares.ues_idp() : After specifying the LLM, the unlabeled dataset, and the few-shot examples, ARES will return the Context Relevance, Answer Faithfulness, and Answer Relevance scores for the RAG system.
      - UES/IDP means evaluating an Unlabeled Evaluation Set in conjunction with In-Domain Prompts (UES/IDP)
    2) ares.evaluate_RAG() : By specifying an LLM judge, the unlabeled and labeled datasets, and the few-shot examples for the metrics of interest, ARES will use Prediction Powered Inference (PPI) to significantly enhance evaluation accuracy.

- Other notes:
